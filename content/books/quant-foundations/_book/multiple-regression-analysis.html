<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Multiple Regression Analysis | Quantitative Foundations in Educational Research</title>
  <meta name="description" content="This is for course ESC_PS 8850 at the University of Missouri. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Multiple Regression Analysis | Quantitative Foundations in Educational Research" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is for course ESC_PS 8850 at the University of Missouri. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="ze-wang/quant-foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Multiple Regression Analysis | Quantitative Foundations in Educational Research" />
  
  <meta name="twitter:description" content="This is for course ESC_PS 8850 at the University of Missouri. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Ze Wang" />


<meta name="date" content="2021-07-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-linear-regression.html"/>
<link rel="next" href="binary-logistic-regression.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contents"><i class="fa fa-check"></i>Contents</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html"><i class="fa fa-check"></i>About the Author</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to R and RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#the-r-language"><i class="fa fa-check"></i><b>1.1</b> The R Language</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#why-r"><i class="fa fa-check"></i><b>1.1.1</b> Why R?</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#statistical-analysis-software-for-data-modeling"><i class="fa fa-check"></i><b>1.1.2</b> Statistical analysis software for data modeling</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#install-r"><i class="fa fa-check"></i><b>1.2</b> Install R</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#install-rstudio"><i class="fa fa-check"></i><b>1.3</b> Install RStudio</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#use-rstudio"><i class="fa fa-check"></i><b>1.4</b> Use RStudio</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#basic-operations"><i class="fa fa-check"></i><b>1.4.1</b> Basic operations</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#data-types-in-r"><i class="fa fa-check"></i><b>1.4.2</b> Data types in R</a></li>
<li class="chapter" data-level="1.4.3" data-path="intro.html"><a href="intro.html#data-structures-in-r"><i class="fa fa-check"></i><b>1.4.3</b> Data structures in R</a></li>
<li class="chapter" data-level="1.4.4" data-path="intro.html"><a href="intro.html#packages"><i class="fa fa-check"></i><b>1.4.4</b> R packages</a></li>
<li class="chapter" data-level="1.4.5" data-path="intro.html"><a href="intro.html#rstudio-projects"><i class="fa fa-check"></i><b>1.4.5</b> RStudio projects</a></li>
<li class="chapter" data-level="1.4.6" data-path="intro.html"><a href="intro.html#import-and-export-data"><i class="fa fa-check"></i><b>1.4.6</b> Import and export Data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html"><i class="fa fa-check"></i><b>2</b> Review of Basic Statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#calculating-variance-and-covariance"><i class="fa fa-check"></i><b>2.1</b> Calculating Variance and Covariance</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#covariance"><i class="fa fa-check"></i><b>2.1.1</b> Covariance</a></li>
<li class="chapter" data-level="2.1.2" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#correlation"><i class="fa fa-check"></i><b>2.1.2</b> Correlation</a></li>
<li class="chapter" data-level="2.1.3" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#using-matrix-algebra"><i class="fa fa-check"></i><b>2.1.3</b> Using Matrix Algebra</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#basic"><i class="fa fa-check"></i><b>2.2</b> Use R for Basic Statistics</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#import-and-export-data-1"><i class="fa fa-check"></i><b>2.2.1</b> Import and export data</a></li>
<li class="chapter" data-level="2.2.2" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#several-functions-for-basic-data-management"><i class="fa fa-check"></i><b>2.2.2</b> Several functions for basic data management</a></li>
<li class="chapter" data-level="2.2.3" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#some-descriptive-statistics"><i class="fa fa-check"></i><b>2.2.3</b> Some descriptive statistics</a></li>
<li class="chapter" data-level="2.2.4" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#check-normality"><i class="fa fa-check"></i><b>2.2.4</b> Check normality</a></li>
<li class="chapter" data-level="2.2.5" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#check-linearity"><i class="fa fa-check"></i><b>2.2.5</b> Check linearity</a></li>
<li class="chapter" data-level="2.2.6" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#pearsons-product-moment-correlation"><i class="fa fa-check"></i><b>2.2.6</b> Pearson’s product moment correlation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#use-r-for-graphing-data"><i class="fa fa-check"></i><b>2.3</b> Use R for Graphing Data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#use-plot"><i class="fa fa-check"></i><b>2.3.1</b> Use <code>plot()</code></a></li>
<li class="chapter" data-level="2.3.2" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#use-ggplot2-package"><i class="fa fa-check"></i><b>2.3.2</b> Use <code>ggplot2</code> package</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#use-r-to-graph-a-correlation-matrix"><i class="fa fa-check"></i><b>2.4</b> Use R to Graph a Correlation Matrix</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#use-plot-1"><i class="fa fa-check"></i><b>2.4.1</b> Use <code>plot()</code></a></li>
<li class="chapter" data-level="2.4.2" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#use-ggplot"><i class="fa fa-check"></i><b>2.4.2</b> Use <code>ggplot()</code></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#use-r-to-generate-random-data"><i class="fa fa-check"></i><b>2.5</b> Use R to Generate Random Data</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#sampling-distribution"><i class="fa fa-check"></i><b>2.5.1</b> Sampling distribution</a></li>
<li class="chapter" data-level="2.5.2" data-path="review-of-basic-statistics.html"><a href="review-of-basic-statistics.html#simulate-data-from-a-model"><i class="fa fa-check"></i><b>2.5.2</b> Simulate data from a model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="correlation-1.html"><a href="correlation-1.html"><i class="fa fa-check"></i><b>3</b> Correlation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="correlation-1.html"><a href="correlation-1.html#variance-and-covariance"><i class="fa fa-check"></i><b>3.1</b> Variance and Covariance</a></li>
<li class="chapter" data-level="3.2" data-path="correlation-1.html"><a href="correlation-1.html#pearson-product-moment-correlation-pearson-correlation"><i class="fa fa-check"></i><b>3.2</b> Pearson Product-Moment Correlation (Pearson Correlation)</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="correlation-1.html"><a href="correlation-1.html#test-that-the-population-correlation-equals-zero"><i class="fa fa-check"></i><b>3.2.1</b> Test that the population correlation equals zero</a></li>
<li class="chapter" data-level="3.2.2" data-path="correlation-1.html"><a href="correlation-1.html#check-normality-1"><i class="fa fa-check"></i><b>3.2.2</b> Check normality</a></li>
<li class="chapter" data-level="3.2.3" data-path="correlation-1.html"><a href="correlation-1.html#check-for-linearity-with-scatterplot"><i class="fa fa-check"></i><b>3.2.3</b> Check for linearity with scatterplot</a></li>
<li class="chapter" data-level="3.2.4" data-path="correlation-1.html"><a href="correlation-1.html#factors-that-affect-the-pearson-correlation"><i class="fa fa-check"></i><b>3.2.4</b> Factors that affect the Pearson Correlation</a></li>
<li class="chapter" data-level="3.2.5" data-path="correlation-1.html"><a href="correlation-1.html#test-that-the-population-correlation-equals-a-certain-value"><i class="fa fa-check"></i><b>3.2.5</b> Test that the population correlation equals a certain value</a></li>
<li class="chapter" data-level="3.2.6" data-path="correlation-1.html"><a href="correlation-1.html#test-that-two-population-correlations-are-equal"><i class="fa fa-check"></i><b>3.2.6</b> Test that two population correlations are equal</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="correlation-1.html"><a href="correlation-1.html#variations-of-the-pearson-product-moment-correlation"><i class="fa fa-check"></i><b>3.3</b> Variations of the Pearson Product-Moment Correlation</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="correlation-1.html"><a href="correlation-1.html#point-biserial-correlation"><i class="fa fa-check"></i><b>3.3.1</b> Point Biserial Correlation</a></li>
<li class="chapter" data-level="3.3.2" data-path="correlation-1.html"><a href="correlation-1.html#biserial-correlation"><i class="fa fa-check"></i><b>3.3.2</b> Biserial Correlation</a></li>
<li class="chapter" data-level="3.3.3" data-path="correlation-1.html"><a href="correlation-1.html#spearmans-rankspearmans-correlationspearmans-rho"><i class="fa fa-check"></i><b>3.3.3</b> Spearman’s Rank/Spearman’s Correlation/Spearman’s rho</a></li>
<li class="chapter" data-level="3.3.4" data-path="correlation-1.html"><a href="correlation-1.html#phi-coefficient"><i class="fa fa-check"></i><b>3.3.4</b> Phi Coefficient</a></li>
<li class="chapter" data-level="3.3.5" data-path="correlation-1.html"><a href="correlation-1.html#tetrachoric-correlation"><i class="fa fa-check"></i><b>3.3.5</b> Tetrachoric Correlation</a></li>
<li class="chapter" data-level="3.3.6" data-path="correlation-1.html"><a href="correlation-1.html#polychoric-correlation"><i class="fa fa-check"></i><b>3.3.6</b> Polychoric Correlation</a></li>
<li class="chapter" data-level="3.3.7" data-path="correlation-1.html"><a href="correlation-1.html#chi-square"><i class="fa fa-check"></i><b>3.3.7</b> Chi-Square</a></li>
<li class="chapter" data-level="3.3.8" data-path="correlation-1.html"><a href="correlation-1.html#summary-of-measures-of-association"><i class="fa fa-check"></i><b>3.3.8</b> Summary of measures of association</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="correlation-1.html"><a href="correlation-1.html#an-example"><i class="fa fa-check"></i><b>3.4</b> An Example</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#plot-the-data"><i class="fa fa-check"></i><b>4.1</b> Plot the Data</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#using-plot"><i class="fa fa-check"></i><b>4.1.1</b> Using <code>plot()</code></a></li>
<li class="chapter" data-level="4.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#using-ggplot"><i class="fa fa-check"></i><b>4.1.2</b> Using <code>ggplot()</code></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-regression-equation"><i class="fa fa-check"></i><b>4.2</b> The Regression Equation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#variance-explained"><i class="fa fa-check"></i><b>4.2.1</b> Variance explained</a></li>
<li class="chapter" data-level="4.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#relationship-with-pearson-correlation"><i class="fa fa-check"></i><b>4.2.2</b> Relationship with Pearson correlation</a></li>
<li class="chapter" data-level="4.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#coefficient-of-determination-r2"><i class="fa fa-check"></i><b>4.2.3</b> Coefficient of Determination (<span class="math inline">\({{R}^{2}}\)</span>)</a></li>
<li class="chapter" data-level="4.2.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#r-syntax"><i class="fa fa-check"></i><b>4.2.4</b> R Syntax</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#test-of-significance-and-confidence-interval"><i class="fa fa-check"></i><b>4.3</b> Test of Significance and Confidence Interval</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#testing-the-regression-of-y-on-x"><i class="fa fa-check"></i><b>4.3.1</b> Testing the regression of Y on X</a></li>
<li class="chapter" data-level="4.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#testing-the-regression-coefficient-slope-and-the-confidence-interval"><i class="fa fa-check"></i><b>4.3.2</b> Testing the regression coefficient (slope) and the confidence interval</a></li>
<li class="chapter" data-level="4.3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#r-syntax-1"><i class="fa fa-check"></i><b>4.3.3</b> R Syntax</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary-of-important-statistics"><i class="fa fa-check"></i><b>4.4</b> Summary of Important Statistics</a></li>
<li class="chapter" data-level="4.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#confidence-intervals-hypothesis-testing-and-prediction-intervals"><i class="fa fa-check"></i><b>4.5</b> Confidence Intervals, Hypothesis Testing, and Prediction Intervals</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#prediction-intervals-for-new-observations"><i class="fa fa-check"></i><b>4.5.1</b> Prediction Intervals for New Observations</a></li>
<li class="chapter" data-level="4.5.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#r-syntax-2"><i class="fa fa-check"></i><b>4.5.2</b> R Syntax</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#a-complete-example"><i class="fa fa-check"></i><b>4.6</b> A Complete Example</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html"><i class="fa fa-check"></i><b>5</b> Multiple Regression Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#f-test-and-partial-f-test"><i class="fa fa-check"></i><b>5.1</b> F Test and Partial F Test</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#overall-test-f-test"><i class="fa fa-check"></i><b>5.1.1</b> Overall test (F Test)</a></li>
<li class="chapter" data-level="5.1.2" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#partial-f-test-for-additional-variables"><i class="fa fa-check"></i><b>5.1.2</b> Partial F test for additional variable(s)</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#testing-the-partial-regression-coefficients"><i class="fa fa-check"></i><b>5.2</b> Testing the Partial Regression Coefficients</a></li>
<li class="chapter" data-level="5.3" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#partial-and-semi-paritial-part-correlations"><i class="fa fa-check"></i><b>5.3</b> Partial and Semi-paritial (Part) Correlations</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#r-syntax-3"><i class="fa fa-check"></i><b>5.3.1</b> R Syntax</a></li>
<li class="chapter" data-level="5.3.2" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#sum-up"><i class="fa fa-check"></i><b>5.3.2</b> Sum up</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#a-complete-example-1"><i class="fa fa-check"></i><b>5.4</b> A Complete Example</a></li>
<li class="chapter" data-level="5.5" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#types-of-multiple-regression"><i class="fa fa-check"></i><b>5.5</b> Types of Multiple Regression</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#regression-model-selection-based-on-statistics"><i class="fa fa-check"></i><b>5.5.1</b> Regression model selection based on statistics</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#a-few-assumptions-in-regression"><i class="fa fa-check"></i><b>5.6</b> (A Few) Assumptions in Regression</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#linearity"><i class="fa fa-check"></i><b>5.6.1</b> Linearity</a></li>
<li class="chapter" data-level="5.6.2" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#homoscedasticity-constant-variance-assumption"><i class="fa fa-check"></i><b>5.6.2</b> Homoscedasticity (Constant Variance Assumption)</a></li>
<li class="chapter" data-level="5.6.3" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#normality"><i class="fa fa-check"></i><b>5.6.3</b> Normality</a></li>
<li class="chapter" data-level="5.6.4" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#independence-of-residuals"><i class="fa fa-check"></i><b>5.6.4</b> Independence of Residuals</a></li>
<li class="chapter" data-level="5.6.5" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#multicollinearity"><i class="fa fa-check"></i><b>5.6.5</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#multiple-regression-and-patterns-of-association"><i class="fa fa-check"></i><b>5.7</b> Multiple Regression and Patterns of Association</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#complete-independence"><i class="fa fa-check"></i><b>5.7.1</b> Complete independence</a></li>
<li class="chapter" data-level="5.7.2" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#partial-redundancy"><i class="fa fa-check"></i><b>5.7.2</b> Partial redundancy</a></li>
<li class="chapter" data-level="5.7.3" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#complete-redundancy"><i class="fa fa-check"></i><b>5.7.3</b> Complete redundancy</a></li>
<li class="chapter" data-level="5.7.4" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#suppression-in-multiple-regression"><i class="fa fa-check"></i><b>5.7.4</b> Suppression in multiple regression</a></li>
<li class="chapter" data-level="5.7.5" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#summary-of-patterns-of-association"><i class="fa fa-check"></i><b>5.7.5</b> Summary of patterns of association</a></li>
<li class="chapter" data-level="5.7.6" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#statistical-paradoxes"><i class="fa fa-check"></i><b>5.7.6</b> Statistical paradoxes</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#regression-diagnostics"><i class="fa fa-check"></i><b>5.8</b> Regression Diagnostics</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#leverage"><i class="fa fa-check"></i><b>5.8.1</b> Leverage</a></li>
<li class="chapter" data-level="5.8.2" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#discrepancy"><i class="fa fa-check"></i><b>5.8.2</b> Discrepancy</a></li>
<li class="chapter" data-level="5.8.3" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#influence"><i class="fa fa-check"></i><b>5.8.3</b> Influence</a></li>
<li class="chapter" data-level="5.8.4" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#sources-of-outliers-and-remedies"><i class="fa fa-check"></i><b>5.8.4</b> Sources of Outliers and Remedies</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#curvilinear-regression"><i class="fa fa-check"></i><b>5.9</b> Curvilinear Regression</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#polynomial-regression"><i class="fa fa-check"></i><b>5.9.1</b> Polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#useful-functions-and-symbols-commonly-used-in-r-when-fitting-linear-models"><i class="fa fa-check"></i><b>5.10</b> Useful Functions and Symbols Commonly Used in R When Fitting Linear Models</a></li>
<li class="chapter" data-level="5.11" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#interactions-in-regression"><i class="fa fa-check"></i><b>5.11</b> Interactions in Regression</a>
<ul>
<li class="chapter" data-level="5.11.1" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#an-example-1"><i class="fa fa-check"></i><b>5.11.1</b> An example</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#categorical-independent-variables"><i class="fa fa-check"></i><b>5.12</b> Categorical Independent Variables</a>
<ul>
<li class="chapter" data-level="5.12.1" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#binary-independent-variables"><i class="fa fa-check"></i><b>5.12.1</b> Binary independent variables</a></li>
<li class="chapter" data-level="5.12.2" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#categorical-independent-variables-with-more-than-two-categories"><i class="fa fa-check"></i><b>5.12.2</b> Categorical independent variables with more than two categories</a></li>
<li class="chapter" data-level="5.12.3" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#an-example-2"><i class="fa fa-check"></i><b>5.12.3</b> An example</a></li>
</ul></li>
<li class="chapter" data-level="5.13" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#interactions-between-categorical-and-continuous-variables"><i class="fa fa-check"></i><b>5.13</b> Interactions Between Categorical and Continuous Variables</a></li>
<li class="chapter" data-level="5.14" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#an-open-book-on-regression"><i class="fa fa-check"></i><b>5.14</b> An Open Book on Regression</a></li>
<li class="chapter" data-level="5.15" data-path="multiple-regression-analysis.html"><a href="multiple-regression-analysis.html#create-apa-tables-with-apatables-package"><i class="fa fa-check"></i><b>5.15</b> Create APA Tables with <code>apaTables</code> Package</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html"><i class="fa fa-check"></i><b>6</b> Binary Logistic Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#some-definitions"><i class="fa fa-check"></i><b>6.1</b> Some Definitions</a></li>
<li class="chapter" data-level="6.2" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#logarithm-rules"><i class="fa fa-check"></i><b>6.2</b> Logarithm Rules</a></li>
<li class="chapter" data-level="6.3" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#logistic-regression-equation"><i class="fa fa-check"></i><b>6.3</b> Logistic Regression Equation</a></li>
<li class="chapter" data-level="6.4" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#run-logistic-regression"><i class="fa fa-check"></i><b>6.4</b> Run Logistic Regression</a></li>
<li class="chapter" data-level="6.5" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#logistic-regression-coefficients"><i class="fa fa-check"></i><b>6.5</b> Logistic Regression Coefficients</a></li>
<li class="chapter" data-level="6.6" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#similarities-and-differences-between-binary-logistic-regression-and-ols-regression"><i class="fa fa-check"></i><b>6.6</b> Similarities and Differences Between Binary Logistic Regression and OLS Regression</a></li>
<li class="chapter" data-level="6.7" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#empirical-example"><i class="fa fa-check"></i><b>6.7</b> Empirical Example</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#assess-the-model---model-chi-square"><i class="fa fa-check"></i><b>6.7.1</b> Assess the model - model chi-square</a></li>
<li class="chapter" data-level="6.7.2" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#assess-the-model---r2"><i class="fa fa-check"></i><b>6.7.2</b> Assess the model - <span class="math inline">\({R^2}\)</span></a></li>
<li class="chapter" data-level="6.7.3" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#diagnostics"><i class="fa fa-check"></i><b>6.7.3</b> Diagnostics</a></li>
<li class="chapter" data-level="6.7.4" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#assumptions---linearity"><i class="fa fa-check"></i><b>6.7.4</b> Assumptions - Linearity</a></li>
<li class="chapter" data-level="6.7.5" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#assumptions---independence-of-errors"><i class="fa fa-check"></i><b>6.7.5</b> Assumptions - Independence of errors</a></li>
<li class="chapter" data-level="6.7.6" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#use-more-predictors"><i class="fa fa-check"></i><b>6.7.6</b> Use more predictors</a></li>
<li class="chapter" data-level="6.7.7" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#check-multicollinearity-among-ivs"><i class="fa fa-check"></i><b>6.7.7</b> Check Multicollinearity Among IVs</a></li>
<li class="chapter" data-level="6.7.8" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#interprete-results"><i class="fa fa-check"></i><b>6.7.8</b> Interprete results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html"><i class="fa fa-check"></i><b>7</b> Analysis of Variance</a>
<ul>
<li class="chapter" data-level="7.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#historical-background"><i class="fa fa-check"></i><b>7.1</b> Historical Background</a></li>
<li class="chapter" data-level="7.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-general-linear-model-glm"><i class="fa fa-check"></i><b>7.2</b> The General Linear Model (GLM)</a></li>
<li class="chapter" data-level="7.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#one-way-anova-for-single-factor-designs"><i class="fa fa-check"></i><b>7.3</b> One-Way ANOVA (for Single Factor Designs)</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#statistical-hypotheses"><i class="fa fa-check"></i><b>7.3.1</b> Statistical hypotheses</a></li>
<li class="chapter" data-level="7.3.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#basic-idea-of-anova"><i class="fa fa-check"></i><b>7.3.2</b> Basic idea of ANOVA</a></li>
<li class="chapter" data-level="7.3.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#empirical-example-1"><i class="fa fa-check"></i><b>7.3.3</b> Empirical example</a></li>
<li class="chapter" data-level="7.3.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#pairwise-comparisons"><i class="fa fa-check"></i><b>7.3.4</b> Pairwise comparisons</a></li>
<li class="chapter" data-level="7.3.5" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#planned-comparisons"><i class="fa fa-check"></i><b>7.3.5</b> Planned comparisons</a></li>
<li class="chapter" data-level="7.3.6" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#post-hoc-pairwise-comparisons"><i class="fa fa-check"></i><b>7.3.6</b> Post-hoc pairwise comparisons</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#anova-assumptions"><i class="fa fa-check"></i><b>7.4</b> ANOVA Assumptions</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#robustness"><i class="fa fa-check"></i><b>7.4.1</b> Robustness</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#one-way-anova-using-glm-approach"><i class="fa fa-check"></i><b>7.5</b> One-way ANOVA Using GLM approach</a></li>
<li class="chapter" data-level="7.6" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#factorial-analysis-of-variance-factorial-anova"><i class="fa fa-check"></i><b>7.6</b> Factorial Analysis of Variance (Factorial ANOVA)</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#numerical-calcuations-for-two-way-anova"><i class="fa fa-check"></i><b>7.6.1</b> Numerical calcuations for two-way ANOVA</a></li>
<li class="chapter" data-level="7.6.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#partitioning-the-total-sum-of-squares"><i class="fa fa-check"></i><b>7.6.2</b> Partitioning the total sum of squares</a></li>
<li class="chapter" data-level="7.6.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#empirical-example---two-way-anova"><i class="fa fa-check"></i><b>7.6.3</b> Empirical example - Two-way ANOVA</a></li>
<li class="chapter" data-level="7.6.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#assumptions-for-factorial-anova-see-refanova-assumptions"><i class="fa fa-check"></i><b>7.6.4</b> Assumptions for factorial ANOVA (see @ref(anova-assumptions))</a></li>
<li class="chapter" data-level="7.6.5" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#contrasts-using-the-multcomp-package"><i class="fa fa-check"></i><b>7.6.5</b> Contrasts using the <code>multcomp</code> package</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#factorial-anova-as-multiple-regression"><i class="fa fa-check"></i><b>7.7</b> Factorial ANOVA as Multiple Regression</a></li>
<li class="chapter" data-level="7.8" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-blog-on-anova-in-r"><i class="fa fa-check"></i><b>7.8</b> A Blog on ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html"><i class="fa fa-check"></i><b>8</b> Analysis of Covariance</a>
<ul>
<li class="chapter" data-level="8.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#purposes-of-ancova"><i class="fa fa-check"></i><b>8.1</b> Purposes of ANCOVA</a></li>
<li class="chapter" data-level="8.2" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#choice-of-covariates-to-use"><i class="fa fa-check"></i><b>8.2</b> Choice of covariates to use</a></li>
<li class="chapter" data-level="8.3" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#statistical-model-for-ancova"><i class="fa fa-check"></i><b>8.3</b> Statistical Model for ANCOVA</a></li>
<li class="chapter" data-level="8.4" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#ancova-assumptions"><i class="fa fa-check"></i><b>8.4</b> ANCOVA Assumptions</a></li>
<li class="chapter" data-level="8.5" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#empirical-example-2"><i class="fa fa-check"></i><b>8.5</b> Empirical Example</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#assumptions---normality"><i class="fa fa-check"></i><b>8.5.1</b> Assumptions - Normality</a></li>
<li class="chapter" data-level="8.5.2" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#assumptions---homogeneity-of-variance"><i class="fa fa-check"></i><b>8.5.2</b> Assumptions - Homogeneity of Variance</a></li>
<li class="chapter" data-level="8.5.3" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#assumptions---independence-of-the-covariate-and-the-independent-variable"><i class="fa fa-check"></i><b>8.5.3</b> Assumptions - Independence of the covariate and the independent variable</a></li>
<li class="chapter" data-level="8.5.4" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#assumptions---homogeneity-of-regression-slopes"><i class="fa fa-check"></i><b>8.5.4</b> Assumptions - Homogeneity of regression slopes</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#post-hoc-tests-using-the-multcomp-package"><i class="fa fa-check"></i><b>8.6</b> Post-hoc Tests using the <code>multcomp</code> package</a></li>
<li class="chapter" data-level="8.7" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#ancova-as-a-glm"><i class="fa fa-check"></i><b>8.7</b> ANCOVA as a GLM</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="repeated-measures-anova-and-mixed-designs.html"><a href="repeated-measures-anova-and-mixed-designs.html"><i class="fa fa-check"></i><b>9</b> Repeated Measures ANOVA and Mixed Designs</a>
<ul>
<li class="chapter" data-level="9.1" data-path="repeated-measures-anova-and-mixed-designs.html"><a href="repeated-measures-anova-and-mixed-designs.html#repeated-measures-anova"><i class="fa fa-check"></i><b>9.1</b> Repeated-Measures ANOVA</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="repeated-measures-anova-and-mixed-designs.html"><a href="repeated-measures-anova-and-mixed-designs.html#advantages-of-repeated-measures-design"><i class="fa fa-check"></i><b>9.1.1</b> Advantages of repeated-measures design</a></li>
<li class="chapter" data-level="9.1.2" data-path="repeated-measures-anova-and-mixed-designs.html"><a href="repeated-measures-anova-and-mixed-designs.html#disadvantages-of-repeated-measures-design"><i class="fa fa-check"></i><b>9.1.2</b> Disadvantages of repeated-measures design</a></li>
<li class="chapter" data-level="9.1.3" data-path="repeated-measures-anova-and-mixed-designs.html"><a href="repeated-measures-anova-and-mixed-designs.html#variance-partitioning-in-one-way-repeated-measures-design"><i class="fa fa-check"></i><b>9.1.3</b> Variance partitioning in one-way repeated measures design</a></li>
<li class="chapter" data-level="9.1.4" data-path="repeated-measures-anova-and-mixed-designs.html"><a href="repeated-measures-anova-and-mixed-designs.html#assumptions-in-repeated-measures-analysis"><i class="fa fa-check"></i><b>9.1.4</b> Assumptions in repeated measures analysis</a></li>
<li class="chapter" data-level="9.1.5" data-path="repeated-measures-anova-and-mixed-designs.html"><a href="repeated-measures-anova-and-mixed-designs.html#alternative-specification-of-the-variance-covariance-matrix"><i class="fa fa-check"></i><b>9.1.5</b> Alternative Specification of the Variance-Covariance Matrix</a></li>
<li class="chapter" data-level="9.1.6" data-path="repeated-measures-anova-and-mixed-designs.html"><a href="repeated-measures-anova-and-mixed-designs.html#empirical-example-3"><i class="fa fa-check"></i><b>9.1.6</b> Empirical example</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="repeated-measures-anova-and-mixed-designs.html"><a href="repeated-measures-anova-and-mixed-designs.html#mixed-designs"><i class="fa fa-check"></i><b>9.2</b> Mixed Designs</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="repeated-measures-anova-and-mixed-designs.html"><a href="repeated-measures-anova-and-mixed-designs.html#one-between-and-one-within-factor-design"><i class="fa fa-check"></i><b>9.2.1</b> One between and one within factor design</a></li>
<li class="chapter" data-level="9.2.2" data-path="repeated-measures-anova-and-mixed-designs.html"><a href="repeated-measures-anova-and-mixed-designs.html#a-few-online-resources"><i class="fa fa-check"></i><b>9.2.2</b> A few online resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="power-analysis.html"><a href="power-analysis.html"><i class="fa fa-check"></i><b>10</b> Power Analysis</a>
<ul>
<li class="chapter" data-level="10.1" data-path="power-analysis.html"><a href="power-analysis.html#components-of-power-analysis"><i class="fa fa-check"></i><b>10.1</b> Components of Power Analysis</a></li>
<li class="chapter" data-level="10.2" data-path="power-analysis.html"><a href="power-analysis.html#effect-sizes"><i class="fa fa-check"></i><b>10.2</b> Effect Sizes</a></li>
<li class="chapter" data-level="10.3" data-path="power-analysis.html"><a href="power-analysis.html#types-of-power-analysis"><i class="fa fa-check"></i><b>10.3</b> Types of Power Analysis</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="power-analysis.html"><a href="power-analysis.html#a-priori-power-analysis"><i class="fa fa-check"></i><b>10.3.1</b> A priori power analysis</a></li>
<li class="chapter" data-level="10.3.2" data-path="power-analysis.html"><a href="power-analysis.html#post-hoc-power-analysis"><i class="fa fa-check"></i><b>10.3.2</b> Post-hoc power analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="power-analysis.html"><a href="power-analysis.html#a-two-tailed-t-test-for-independent-samples-power-example"><i class="fa fa-check"></i><b>10.4</b> a Two-Tailed <em>t Test</em> for Independent Samples Power Example</a></li>
<li class="chapter" data-level="10.5" data-path="power-analysis.html"><a href="power-analysis.html#software"><i class="fa fa-check"></i><b>10.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Foundations in Educational Research</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-regression-analysis" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Multiple Regression Analysis</h1>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="multiple-regression-analysis.html#cb236-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rio); <span class="fu">library</span>(ggplot2); <span class="fu">library</span>(QuantPsyc); <span class="fu">library</span>(psych); <span class="fu">library</span>(car); <span class="fu">library</span>(lmtest); <span class="fu">library</span>(MASS); <span class="fu">library</span>(interactions); <span class="fu">library</span>(memisc); <span class="fu">library</span>(apaTables)</span></code></pre></div>
<p><strong>Recall</strong>: Simple Linear Regression: <span class="math inline">\(Y=a+bX+e\)</span></p>
<p>Regression with 2 Independent Variables: <span class="math inline">\(Y=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+e\)</span>, where a is the intercept/constant (value of Y when X1 and X2 are both equal to zero), <span class="math inline">\({{b}_{1}}\ \text{and}\ {{b}_{2}}\)</span> are regression coefficients associated with X1 and X2, respectively, X1 and X2 are raw scores on the independent variables, and e is the error or residual.</p>
<p>Can be extended to incorporate more independent variables: <span class="math inline">\(Y=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+...+{{b}_{k}}{{X}_{k}}+e\)</span>, where k is the number of independent variables.</p>
<p><strong>Recall</strong>: Simple Linear Regression: <span class="math inline">\({\hat Y}=a+bX\)</span>. Solved for <em>a</em> and <em>b</em> using Least Squares solution that minimizes the sum of squared residuals ( <span class="math inline">\(SS_{res}\)</span>; <span class="math inline">\(\sum{{{(Y-{\hat Y})}^{2}}=\sum{{{e}^{2}}}}\)</span> )</p>
<ul>
<li><p>Intercept: <span class="math inline">\(a=\bar{Y}-b\bar{X}\)</span></p></li>
<li><p>Slope: <span class="math inline">\(b=\frac{\sum{xy}}{\sum{{{x}^{2}}}}=\frac{\sum{(X-\bar{X})(Y-\bar{Y})}}{\sum{{{(X-\bar{X})}^{2}}}}\)</span></p></li>
</ul>
<p>Regression with 2 Independent Variables: <span class="math inline">\({\hat Y}=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}\)</span></p>
<p>Now, <span class="math inline">\({{b}_{1}}\ \text{and}\ {{b}_{2}}\)</span> are considered partial regression coefficients associated with variables X1 and X2, respectively. X1 and X2 are used together in predicting Y. <span class="math inline">\({{b}_{1}}\)</span> is the partial regression coefficient for Y on X1 with X2 in the regression equation/model. <span class="math inline">\({{b}_{2}}\)</span> is the partial regression coefficient for Y on X2 with X1 in the regression equation/model. Still trying to minimize the difference between observed Y scores and predicted Y scores (i.e., <span class="math inline">\(\sum{(Y-{\hat Y})}^{2}\)</span> ; Least Squares solution) when solving for the intercept (<em>a</em>) and the partial regression coefficients ( <span class="math inline">\({{b}_{1}}\)</span> and <span class="math inline">\({{b}_{2}}\)</span> ).</p>
<p>Using our professors’ salary example, we want to estimate/predict Salary from Time since Ph.D. was earned (X1) and Number of citations (X2). Regress Salary on Time since Ph.D. and Number of citations.</p>
<p>To compute the partial regression coefficients, <span class="math inline">\({{b}_{1}}\ \text{and}\ {{b}_{2}}\)</span>, let’s first calculate the partial <em>standardized</em> regression coefficients or beta weights ( <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> ):</p>
<p><span class="math inline">\(b_1 = \beta_1\frac{{s_Y}}{{s_{X1}}}\)</span> ; <span class="math inline">\(b_2={\beta_2}\frac{{s_Y}}{{s_{X2}}}\)</span></p>
<p>In simple linear regression, the standardized regression coefficient (Beta) is the correlation coefficient between X and Y:</p>
<p><span class="math display">\[\beta =b\frac{{{s}_{x}}}{{{s}_{y}}}=\frac{\sum{xy}\sqrt{\sum{{{x}^{2}}}}\sqrt{n-1}}{\sum{{{x}^{2}}}\sqrt{n-1}\sqrt{\sum{{{y}^{2}}}}}=\frac{\sum{xy}}{\sqrt{\sum{{{x}^{2}}}\sqrt{\sum{{{y}^{2}}}}}}={{r}_{xy}}\]</span></p>
<ul>
<li><p>In Simple Linear Regression, the standardized regression coefficient (Beta) has fixed upper lower and upper limits a correlation coefficient (i.e., ranges from -1 to +1).</p></li>
<li><p>In Multiple Regression, the partial standardized regression coefficients (Beta weights) no longer have this fixed lower and upper limit. Generally speaking, however, the partial standardized regression coefficient should not exceed the absolute value of 1. If it does, this may be an indication of a problem among the independent variables (to be discussed later). They are called <strong><em>partial standardized regression coefficients</em></strong> because they are the partial slopes of an equation using standardized scores (i.e., z scores): <span class="math inline">\(z_{\hat y}={{\beta }_{1}}{{z}_{1}}+{{\beta }_{2}}{{z}_{2}}\)</span>, where <span class="math inline">\({{\beta }_{1}}\ and\ {{\beta }_{2}}\)</span> are standardized regression coefficients; <span class="math inline">\({{z}_{1}}\ and\ {{z}_{2}}\)</span> are standard scores on X1 and X2, respectively.</p></li>
<li><p>Unstandardized regression coefficients indicate the direction and by how many units Y will change with a 1 unit increase in X. They are expressed in their original scale of measurement of the independent variable (e.g., number of years). Recall that the mean of z scores is zero and the standard deviation is 1. Because the standard deviation of z scores is 1, a unit change in <span class="math inline">\(z_x\)</span> indicates a change of one standard deviation in X. Thus, standardized regression coefficients indicate the direction and by how many standard deviations Y will change with 1 standard deviation increase in X.</p></li>
</ul>
<p>The relationships between standardized regression coefficients and correlations with two independent variables are below:</p>
<p><span class="math display">\[{{\beta }_{1}}=\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}\]</span></p>
<p><span class="math display">\[{{\beta }_{2}}=\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{1-r_{12}^{2}}\]</span></p>
<p>When we interpret the partial regression coefficient for one independent variable, we should state that we are <em>holding the other independent variable constant</em>.</p>
<p>Using the Professor’s salary example, we would like to regress Salary (<em>Y</em>) on Time (<em>X1</em>) and Citations (<em>X2</em>). We get</p>
<p><span class="math display">\[{{\beta }_{1}}=\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}=\frac{0.608-(0.55)(0.373)}{1-{{(0.373)}^{2}}}=0.468\]</span>
<span class="math display">\[{{\beta }_{2}}=\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{1-r_{12}^{2}}=\frac{0.55-(0.608)(0.373)}{1-{{(0.373)}^{2}}}=0.375\]</span>
With every 1 standard deviation increase in Time, predicted Salary increases by about .47 standard deviations <em>while holding number of citations constant</em>. With every 1 standard deviation increase in number of citations, predicted Salary increases by about .38 standard deviations <em>while holding time since Ph.D. was earned constant</em>.</p>
<p>Why do we state that we are holding the other independent variable constant?</p>
<ul>
<li>In multiple regression, there is redundancy or shared variance between X1 and X2 which must now be taken into account. For example, if two predictors, X1 and X2, are positively correlated and both affect Y (are correlated with the criterion/dependent variable), then the change in Y that is associated with an increase in X1 is partly caused by the increase in X1 and partly caused by the corresponding increase in X2. The regression coefficient or effect of a given predictor variable should indicate the change in Y produced by a unit increase in that predictor variable alone, that is, when the other predictor variable does not change. Thus, when computing multiple regression statistics for a given predictor, we must hold the effects of the other predictor variable statistically constant to determine the extent to which changes in Y result from a change in a given predictor alone. This basically means that multiple regression statistics, such as the partial regression coefficients (standardized or unstandardized), remove the variance from X1 that it shares with X2 in the multiple regression model.</li>
</ul>
<p>Once we have calculated the partial standardized regression coefficients, convert them back to original scale (time in years; number of citations) to get the unstandardized regression coefficients and intercept.</p>
<ul>
<li>Multiple R is the correlation coefficient between the observed and predicted Y values ( <span class="math inline">\({r_{Y{\hat Y}}}\)</span>; ranges from 0 to 1). The Squared multiple correlation, or R Square (<span class="math inline">\({{R}^{2}}\)</span>), is interpreted as the proportion of variance in the criterion/dependent variable Y that can be explained by the predictor variables X1 and X2. The <em>adjusted R Square</em> is an adjustment to better reflect the fit of the model in the population.</li>
</ul>
<p><span class="math display">\[R=\sqrt{\frac{r_{Y1}^{2}+r_{Y2}^{2}-2{{r}_{Y1}}{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}}\]</span></p>
<p><span class="math display">\[R=\sqrt{{{\beta }_{1}}{{r}_{Y1}}+{{\beta }_{2}}{{r}_{Y2}}}\]</span></p>
<ul>
<li>Multiple R can never be smaller than the absolute value of the largest correlation between a predictor variable and the dependent variable.</li>
</ul>
<p><span class="math inline">\({{R}^{2}}=\frac{S{{S}_{reg}}}{\sum{{{y}^{2}}}}=\frac{S{{S}_{reg}}}{S{{S}_{total}}}\)</span> , where <span class="math inline">\(S{S_{reg}} = \sum {{{({\hat Y} - \bar Y)}^2}}\)</span> and <span class="math inline">\(S{S_{total}} = \sum {{{(Y - \bar Y)}^2}}\)</span>.</p>
<p><span class="math inline">\({{R}^{2}}=\frac{r_{Y1}^{2}+r_{Y2}^{2}-2{{r}_{Y1}}{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}\)</span></p>
<p><span class="math inline">\({{R}^{2}}={{\beta }_{1}}{{r}_{Y1}}+{{\beta }_{2}}{{r}_{Y2}}\)</span></p>
<ul>
<li><p>R Square in multiple regression is also referred to as the Coefficient of Multiple Determination (Indicating the amount of variance in the dependent variable explained by the independent variables).</p></li>
<li><p>Coefficient of Multiple Alienation: <span class="math inline">\(1-{{R}^{2}}\)</span>, indicating the amount of variance in the dependent variable not explained by the independent variables.</p></li>
</ul>
<p><span class="math inline">\(R_{adj}^{2}={{R}^{2}}-\frac{(1-{{R}^{2}})k}{n-k-1}\)</span> where <em>n</em> = sample size and <em>k</em> = the number of independent variables included in the model.</p>
<div id="f-test-and-partial-f-test" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> F Test and Partial F Test</h2>
<p>Just as in Simple Linear Regression, we can calculate an F ratio to test the regression of Y on both X1 and X2. <span class="math inline">\(F=\frac{M{{S}_{reg}}}{M{{S}_{res}}}\)</span> with <span class="math inline">\(d{{f}_{reg}}\)</span> in the numerator and <span class="math inline">\(d{{f}_{res}}\)</span> in the denominator (<em>df</em> for the Regression Sum of Squares = <em>k</em>; <em>df</em> for the Residual Sum of Squares = <em>n – k – 1</em>). If the <em>F</em> ratio exceeds the critical value with ( <span class="math inline">\(d{{f}_{reg}}\)</span>, <span class="math inline">\(d{{f}_{res}}\)</span> ) at alpha = .05, we reject the null that the regression of Y on both X1 and X2 is equal to zero. That is, we are testing whether R Square is significantly different from zero:</p>
<p><span class="math inline">\(F=\frac{{{R}^{2}}(n-k-1)}{(1-{{R}^{2}})k}\tilde{\ }df=k\ \text{and}\ n-k-1\)</span></p>
<p>The null hypothesis of the F test is that all partial regression coefficients are equal to zero.</p>
<p>Three Questions That can be Answered with F tests:</p>
<ul>
<li><p><strong>Overall test</strong>. Taken collectively, does the entire set of IVs (or equivalently, the fitted model itself) contribute significantly to the prediction of Y?</p></li>
<li><p><strong>Test for addition of a single variable</strong>. Does the addition of one particular IV of interest add significantly to the prediction of Y achieved by other IVs already present in the model?</p></li>
<li><p><strong>Test for addition of a group of variables</strong>. Does the addition of some group of IVs of interest add significantly to the prediction of Y obtained through other IVs already present in the model?</p></li>
</ul>
<div id="overall-test-f-test" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Overall test (F Test)</h3>
<p><span class="math display">\[F = \frac{{M{S_{reg}}}}{{M{S_{res}}}} = \frac{{S{S_{reg}}/k}}{{S{S_{res}}/(n - k - 1)}}\]</span></p>
<ul>
<li><p>How many degrees of freedom for the numerator?</p></li>
<li><p>How many degrees of freedom for the denominator?</p></li>
</ul>
</div>
<div id="partial-f-test-for-additional-variables" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Partial F test for additional variable(s)</h3>
<p><strong>Add k IVs to the model with p IVs already in the model</strong>:</p>
<p><span class="math display">\[F = \frac{{[S{S_{reg}}(full) - S{S_{reg}}(reduced)]/k}}{{S{S_{res}}(full)/[n - (p + k) - 1]}} = \frac{{[S{S_{reg}}(full) - S{S_{reg}}(reduced)]/k}}{{M{S_{res}}(full)}}\]</span></p>
<ul>
<li><p>This partial F test compares two models: one with p IVs and the other with (p+k) IVs.</p></li>
<li><p>df for the numerator is k;</p></li>
<li><p>df for the denominator is [n-(p+k)-1]</p></li>
<li><p>Estimate both models</p></li>
</ul>
<p>In R,</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="multiple-regression-analysis.html#cb237-1" aria-hidden="true" tabindex="-1"></a>mydata <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="st">&quot;data/profs.sav&quot;</span>)</span>
<span id="cb237-2"><a href="multiple-regression-analysis.html#cb237-2" aria-hidden="true" tabindex="-1"></a>model_full <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> time <span class="sc">+</span> cits, <span class="at">data =</span> mydata)</span>
<span id="cb237-3"><a href="multiple-regression-analysis.html#cb237-3" aria-hidden="true" tabindex="-1"></a>model_reduced <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> time, <span class="at">data =</span> mydata)</span></code></pre></div>
<ul>
<li>Conduct Partial F test using the <code>anova()</code> function</li>
</ul>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="multiple-regression-analysis.html#cb238-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(model_full, model_reduced)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: salary ~ time + cits
## Model 2: salary ~ time
##   Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    
## 1     59 2926312249                                   
## 2     60 3623032005 -1 -696719756 14.047 0.0004078 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p><strong>Variables Added in Order, and Variables Added Last</strong></p>
<ul>
<li><p><strong>Recall</strong>: A partial F test compares two models</p></li>
<li><p>Type I F test is for variables added in order</p>
<ul>
<li>The current model is compared to the previous model with one IV less</li>
<li>Order of entry matters</li>
</ul></li>
<li><p>Type III F test is for variables added last
-The full model is compared to the model with all IVs except for the one IV currently under consideration</p>
<ul>
<li>Order of entry does not matter</li>
</ul></li>
<li><p>It is more important to know which models you are comparing than which F test (Type I or Type III) you use!</p></li>
<li><p>Type I F test (using Type I Sum of Squares). This is the default in the <code>anova</code> function.</p></li>
</ul>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="multiple-regression-analysis.html#cb240-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(model_full)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: salary
##           Df     Sum Sq    Mean Sq F value    Pr(&gt;F)    
## time       1 2123587818 2123587818  42.816 1.596e-08 ***
## cits       1  696719756  696719756  14.047 0.0004078 ***
## Residuals 59 2926312249   49598513                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<ul>
<li>Type III F test (using Type III Sum of Squares). Use the <code>Anova</code> function from the <code>car</code> package.</li>
</ul>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="multiple-regression-analysis.html#cb242-1" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">Anova</span>(model_full, <span class="at">type =</span> <span class="st">&quot;III&quot;</span>)</span></code></pre></div>
<pre><code>## Anova Table (Type III tests)
## 
## Response: salary
##                 Sum Sq Df F value    Pr(&gt;F)    
## (Intercept) 1.3185e+10  1 265.842 &lt; 2.2e-16 ***
## time        1.0834e+09  1  21.844 1.761e-05 ***
## cits        6.9672e+08  1  14.047 0.0004078 ***
## Residuals   2.9263e+09 59                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
</div>
<div id="testing-the-partial-regression-coefficients" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Testing the Partial Regression Coefficients</h2>
<p>Just as in Simple Linear Regression, you can test the significance of a partial regression coefficient (slope) <em>b</em> to see if it is significantly different from zero.</p>
<ul>
<li><strong>Recall the Variance of Estimate:</strong> The variance of estimate indicates the variance of the scores about the regression line. It is the variance of the residuals.</li>
</ul>
<p><span class="math inline">\(s_{y.x}^{2}=\frac{\sum{{{(Y-{\hat Y})}^{2}}}}{n-k-1}=\frac{S{{S}_{res}}}{n-k-1}\)</span></p>
<p>Recall: <span class="math inline">\(M{{S}_{res}}=\frac{\sum{{{(Y-{\hat Y})}^{2}}}}{n-k-1}=\frac{S{{S}_{res}}}{d{{f}_{res}}}\)</span></p>
<ul>
<li><strong>Recall the Standard Error of Estimate</strong>: The standard error of estimate is the square root of the variance of estimate or the standard deviation of the residuals.</li>
</ul>
<p><span class="math display">\[{{s}_{y.x}}=\sqrt{\frac{\sum{{{(Y-{\hat Y})}^{2}}}}{n-k-1}}=\sqrt{\frac{S{{S}_{res}}}{n-k-1}}\]</span></p>
<ul>
<li>When testing whether the partial slope or partial regression coefficient is significantly different from zero, the standard error associated with the slope must be calculated. This differs slightly from the formula used in Simple Linear Regression due to the shared variance between the independent variables that must be taken into account:</li>
</ul>
<p><span class="math inline">\({{s}_{b1}}=\sqrt{\frac{s_{y.x}^{2}}{\sum{x_{1}^{2}(1-r_{12}^{2})}}}=\frac{{{s}_{y.x}}}{\sqrt{\sum{x_{1}^{2}(1-r_{12}^{2})}}}\)</span> where <span class="math inline">\(s_{b1}\)</span> is the standard error associated with <em>b1</em>, <span class="math inline">\(s_{y.x}^{2}\)</span> is the variance of estimate, <span class="math inline">\(s_{y.x}\)</span> is the standard error of estimate, and <span class="math inline">\(\sum\limits_{{}}{x_{1}^{2}}\)</span> is the sum of squares for the independent variable <span class="math inline">\(X1\)</span>.</p>
<p><span class="math inline">\({{s}_{b2}}=\sqrt{\frac{s_{y.x}^{2}}{\sum{x_{2}^{2}(1-r_{12}^{2})}}}=\frac{{{s}_{y.x}}}{\sqrt{\sum{x_{2}^{2}(1-r_{12}^{2})}}}\)</span> where <span class="math inline">\(s_{b2}\)</span> is the standard error associated with <em>b2</em>, <span class="math inline">\(s_{y.x}^{2}\)</span> is the variance of estimate, <span class="math inline">\(s_{y.x}\)</span> is the standard error of estimate, and <span class="math inline">\(\sum\limits_{{}}{x_{2}^{2}}\)</span> is the sum of squares for the independent variable <span class="math inline">\(X2\)</span>.</p>
<p><span class="math inline">\(s_b\)</span> is the standard error (standard deviation of the sampling distribution of <em>b</em>) and is used when testing the significance of the <em>b</em> using the t ratio: <span class="math inline">\(t=\frac{b}{{{s}_{b}}}=\frac{b-0}{{{s}_{b}}}\)</span> with <em>df</em> of n – k – 1.</p>
<p><span class="math display">\[{{t}_{b1}}=\frac{{{b}_{1}}}{{{s}_{b1}}}\]</span></p>
<p><span class="math display">\[{{t}_{b2}}=\frac{{{b}_{2}}}{{{s}_{b2}}}\]</span></p>
<p>With knowledge of the standard error of the partial <em>bs</em>, we can create a confidence interval around each of the partial regression coefficients: <span class="math inline">\(b\pm {{t}_{(\alpha /2,df)}}{{s}_{b}}\)</span>.</p>
</div>
<div id="partial-and-semi-paritial-part-correlations" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Partial and Semi-paritial (Part) Correlations</h2>
<p>Questions Typically Asked in Multiple Regression</p>
<ol style="list-style-type: decimal">
<li>How well does a group of independent variables together estimate Y?</li>
</ol>
<ul>
<li>R Square or Adjusted R Square may be used to answer this question. In the professor’s salary example, time since Ph.D. was earned and number of citations accounted for/explained 49% or 47% ( <span class="math inline">\(R^2 = .491\)</span>; <span class="math inline">\(R_{adj}^{2}=.474\)</span>) of the variance in Professors’ Salary. The Coefficient of Multiple Alienation = <span class="math inline">\(1-{{R}^{2}}=1-.491=.509\)</span> is the proportion of variance in Salary not explained by/associated with Time and Citations. Fifty-one percent of the variance in Salary is not associated with Time and Citations. The <em>F</em>-test indicates significance of R Square.</li>
</ul>
<p>Questions Typically Asked in Multiple Regression</p>
<ol start="2" style="list-style-type: decimal">
<li>How much does any single variable add to the estimation of Y already explained by other variables?</li>
</ol>
<ul>
<li><p>The Semipartial (or Part) Correlation may be used to answer this question.</p></li>
<li><p>A semipartial correlation is a correlation between Y and an independent variable/predictor from which the other independent variables/predictors have been partialled. It is the correlation between the dependent variable and an independent variable when the linear effects of the other independent variables in the model have been removed from the independent variable. It is related to the change in R square when a variable is added to an equation.</p></li>
<li><p>Thus, the semipartial correlation for Time (X1) is the correlation between Time (X1) and Salary (Y) with the association between Time (X1) and Citations (X2) removed, but the association between Citations (X2) and Salary (Y) is not removed ( <span class="math inline">\({{r}_{Y({{X}_{1}}|{{X}_{2}})}}\)</span> ). The semipartial correlation for Citations (X2) is the correlation between Citations (X2) and Salary (Y) with the association between Citations (X2) and Time (X1) removed, but the association between Time (X1) and Salary (Y) is not removed ( <span class="math inline">\({{r}_{Y({{X}_{2}}|{{X}_{1}})}}\)</span>).</p></li>
</ul>
<p><img src="image/partial_correlation.png" width="150%" /></p>
<p><span class="math display">\[a = sr_1^2 = {R^2} - r_{Y2}^2\]</span></p>
<p><span class="math display">\[b = sr_2^2 = {R^2} - r_{Y1}^2\]</span>
<span class="math display">\[r_{Y1}^2 = a + c\]</span>
<span class="math display">\[r_{Y2}^2 = b + c\]</span>
<span class="math display">\[{R^2} = a + b + c\]</span>
- <em>a</em> and <em>b</em> are proportions of <em>Y</em> variance uniquely accounted for by <em>X1</em> and <em>X2</em>, respectively and equal the squared semipartial correlation (the increase in <em>R</em> Square when one independent variable/predictor is added to the other independent variable/predictor).</p>
<p><span class="math display">\[s{{r}_{1}}=\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{\sqrt{1-r_{12}^{2}}}\]</span>
<span class="math display">\[s{{r}_{2}}=\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{\sqrt{1-r_{12}^{2}}}\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>When other independent variables/predictors are held constant statistically, how much of Y does a given variable account for?</li>
</ol>
<ul>
<li><p>The Partial Correlation may be used to answer this question.</p></li>
<li><p>A Partial correlation is a correlation between Y and an independent variable/predictor while controlling for the other independent variables/predictors in the model. It is the correlation between the dependent variable and an independent variable when the linear effects of the other independent variables in the model have been removed from both. Thus, the partial correlation for Time (X1) is the correlation between Time (X1) and Salary (Y) while controlling for Citations (X2) or when removing the effects of Citations ( <span class="math inline">\({{r}_{Y{{X}_{1}}|{{X}_{2}}}}\)</span>). The partial correlation for Citations (X2) is the correlation between Citations (X2) and Salary (Y) while controlling for Time (X1) or when removing the effects of Time ( <span class="math inline">\({{r}_{Y{{X}_{2}}|{{X}_{1}}}}\)</span>).</p></li>
<li><p>The squared partial correlation indicates how much of the Y variance not estimated by other predictors is estimated by this predictor. For Time (X1), it is the proportion of variance in Y not associated with Citations (X2) but is associated with Time (X1). For Citations (X2), it is the proportion of variance in Y not associated with Time (X1) but is associated with Citations (X2).</p></li>
</ul>
<p><span class="math display">\[pr_{1}^{2}=\frac{a}{a+e}=\frac{{{R}^{2}}-r_{Y2}^{2}}{1-r_{Y2}^{2}}\]</span></p>
<p><span class="math display">\[pr_{2}^{2}=\frac{b}{b+e}=\frac{{{R}^{2}}-r_{Y1}^{2}}{1-r_{Y1}^{2}}\]</span></p>
<div id="r-syntax-3" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> R Syntax</h3>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="multiple-regression-analysis.html#cb244-1" aria-hidden="true" tabindex="-1"></a>mydata <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="st">&quot;data/profs.sav&quot;</span>)</span>
<span id="cb244-2"><a href="multiple-regression-analysis.html#cb244-2" aria-hidden="true" tabindex="-1"></a><span class="fu">corr.test</span>(mydata<span class="sc">$</span>salary, mydata<span class="sc">$</span>time)</span>
<span id="cb244-3"><a href="multiple-regression-analysis.html#cb244-3" aria-hidden="true" tabindex="-1"></a><span class="fu">corr.test</span>(mydata<span class="sc">$</span>salary, mydata<span class="sc">$</span>cits)</span>
<span id="cb244-4"><a href="multiple-regression-analysis.html#cb244-4" aria-hidden="true" tabindex="-1"></a><span class="fu">corr.test</span>(mydata<span class="sc">$</span>time, mydata<span class="sc">$</span>cits)</span>
<span id="cb244-5"><a href="multiple-regression-analysis.html#cb244-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> mydata<span class="sc">$</span>salary</span>
<span id="cb244-6"><a href="multiple-regression-analysis.html#cb244-6" aria-hidden="true" tabindex="-1"></a>y.x2 <span class="ot">&lt;-</span> (<span class="fu">lm</span>(salary <span class="sc">~</span> cits, <span class="at">data =</span> mydata))<span class="sc">$</span>residuals</span>
<span id="cb244-7"><a href="multiple-regression-analysis.html#cb244-7" aria-hidden="true" tabindex="-1"></a>x1.x2 <span class="ot">&lt;-</span> (<span class="fu">lm</span>(time <span class="sc">~</span> cits, <span class="at">data =</span> mydata))<span class="sc">$</span>residuals</span>
<span id="cb244-8"><a href="multiple-regression-analysis.html#cb244-8" aria-hidden="true" tabindex="-1"></a>y.x1 <span class="ot">&lt;-</span> (<span class="fu">lm</span>(salary <span class="sc">~</span> time, <span class="at">data =</span> mydata))<span class="sc">$</span>residuals</span>
<span id="cb244-9"><a href="multiple-regression-analysis.html#cb244-9" aria-hidden="true" tabindex="-1"></a>x2.x1 <span class="ot">&lt;-</span> (<span class="fu">lm</span>(cits <span class="sc">~</span> time, <span class="at">data =</span> mydata))<span class="sc">$</span>residuals</span>
<span id="cb244-10"><a href="multiple-regression-analysis.html#cb244-10" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(y.x2, x1.x2) <span class="co"># partial correlation between y and x1 controlling for x2</span></span>
<span id="cb244-11"><a href="multiple-regression-analysis.html#cb244-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(y.x1, x2.x1) <span class="co"># partial correlation between y and x2 controlling for x1</span></span>
<span id="cb244-12"><a href="multiple-regression-analysis.html#cb244-12" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(y, x1.x2) <span class="co"># semi-partial correlation between y and x1 controlling for x2</span></span>
<span id="cb244-13"><a href="multiple-regression-analysis.html#cb244-13" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(y, x2.x1) <span class="co"># semi-partial correlation between y and x2 controlling for x1</span></span></code></pre></div>
</div>
<div id="sum-up" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Sum up</h3>
<p>Multiple regression analysis was used to examine how well time since Ph.D. was earned and the number of citations for professors would explain/predict their salary. Time since Ph.D. and number of citations together accounted for approximately 47% of the variance in Salary <span class="math inline">\((R_{adj}^{2}=.474)\)</span>, <em>F</em>(2, 59) = 28.43, <em>p</em> &lt; .001. Time since professor’s earned their Ph.D. was a statistically significant predictor of Salary, <em>t</em>(59) = 4.67, <em>p</em> &lt; .001, which accounted for 27% of the variance in Salary not accounted for by number of citations (pr = .520) and uniquely accounted for 19% of the variance in Salary (sr = .434). Holding number of citations constant, as time since professors’ earned their Ph.D. increased by 1 year, salary was estimated to increase by <code>$</code>1062 (95%: 607,19, 1516,34). Number of citations for professors was also a statistically significant predictor of salary, <em>t</em>(59) = 3.75, <em>p</em> &lt; .001, which accounted for 19% of the variance in salary not accounted for by time since Ph.D. was earned (pr = .439) and uniquely accounted for 12% of the variance in Salary (sr = .348). Holding time since Ph.D. was earned constant, as the number of citations for professors increased by 1, salary was estimated to increase by <code>$</code>212 (95% CI: 98.87, 325.36).</p>
<p><strong>Note</strong>:
- The regular pearson correlation is also called zero-order correlation.
- Section 6.6 of textbook is on partial and semi-partial correlations.</p>
</div>
</div>
<div id="a-complete-example-1" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> A Complete Example</h2>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="multiple-regression-analysis.html#cb245-1" aria-hidden="true" tabindex="-1"></a>mydata <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="st">&quot;data/profs.sav&quot;</span>)</span>
<span id="cb245-2"><a href="multiple-regression-analysis.html#cb245-2" aria-hidden="true" tabindex="-1"></a><span class="co">#View(mydata)</span></span>
<span id="cb245-3"><a href="multiple-regression-analysis.html#cb245-3" aria-hidden="true" tabindex="-1"></a>mymodel <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> time <span class="sc">+</span> cits, <span class="at">data =</span> mydata)</span>
<span id="cb245-4"><a href="multiple-regression-analysis.html#cb245-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mymodel)</span>
<span id="cb245-5"><a href="multiple-regression-analysis.html#cb245-5" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(mymodel, <span class="at">level =</span> <span class="fl">0.95</span>)</span>
<span id="cb245-6"><a href="multiple-regression-analysis.html#cb245-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lm.beta</span>(mymodel)</span>
<span id="cb245-7"><a href="multiple-regression-analysis.html#cb245-7" aria-hidden="true" tabindex="-1"></a>mydata<span class="sc">$</span>y.pred <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mymodel)</span>
<span id="cb245-8"><a href="multiple-regression-analysis.html#cb245-8" aria-hidden="true" tabindex="-1"></a>mydata<span class="sc">$</span>y.res <span class="ot">&lt;-</span> <span class="fu">resid</span>(mymodel)</span>
<span id="cb245-9"><a href="multiple-regression-analysis.html#cb245-9" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(mydata)</span>
<span id="cb245-10"><a href="multiple-regression-analysis.html#cb245-10" aria-hidden="true" tabindex="-1"></a>new.obs <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb245-11"><a href="multiple-regression-analysis.html#cb245-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">time =</span> <span class="dv">12</span>,</span>
<span id="cb245-12"><a href="multiple-regression-analysis.html#cb245-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">cits =</span> <span class="dv">80</span></span>
<span id="cb245-13"><a href="multiple-regression-analysis.html#cb245-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb245-14"><a href="multiple-regression-analysis.html#cb245-14" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(mymodel, <span class="at">newdata =</span> new.obs, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>) <span class="co"># Prediction interval for the mean</span></span>
<span id="cb245-15"><a href="multiple-regression-analysis.html#cb245-15" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(mymodel, <span class="at">newdata =</span> new.obs, <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>) <span class="co"># Prediction interval for the individual</span></span></code></pre></div>
<p><strong>Sum up</strong>: Multiple regression analysis was used to examine how well time since Ph.D. was earned and the number of citations for professors would explain/predict their salary. Time since Ph.D. and number of citations together accounted for approximately 47% of the variance in Salary <span class="math inline">\((R_{adj}^{2}=.474)\)</span>, <em>F</em>(2, 59) = 28.43, <em>p</em> &lt; .001. Time since professor’s earned their Ph.D. was a statistically significant predictor of Salary, <em>t</em>(59) = 4.67, <em>p</em> &lt; .001. Holding number of citations constant, as time since professors’ earned their Ph.D. increased by 1 year, salary was estimated to increase by <code>$</code>1062 (95%: 607,19, 1516,34). Number of citations for professors was also a statistically significant predictor of salary, <em>t</em>(59) = 3.75, <em>p</em> &lt; .001. Holding time since Ph.D. was earned constant, as the number of citations for professors increased by 1, salary was estimated to increase by <code>$</code>212 (95% CI: 98.87, 325.36).</p>
<p>For professors who earned their Ph.D. 12 years ago and who had 80 citations, it is predicted that their salary would be about <code>$</code>68783 (95% CI: <code>$</code>64185, <code>$</code>73382). For any professor who earned their Ph.D. 12 years ago and who had 80 citations, the predicted salary is about <code>$</code>68783 (95% CI: <code>$</code>53960, <code>$</code>83607).</p>
</div>
<div id="types-of-multiple-regression" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Types of Multiple Regression</h2>
<table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-103">Table 5.1: </span>Types of Multiple Regression
</caption>
<thead>
<tr>
<th style="text-align:left;">
Type of Regression
</th>
<th style="text-align:left;">
Alternative Name
</th>
<th style="text-align:left;">
IV Entered
</th>
<th style="text-align:left;">
IV Evaluated
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Stnadard
</td>
<td style="text-align:left;width: 5em; ">
simultaneous
</td>
<td style="text-align:left;width: 15em; ">
all IVs enter at once
</td>
<td style="text-align:left;width: 15em; ">
each evaluated in terms of what it contributes as though it is last and all others have already made their contribution
</td>
</tr>
<tr>
<td style="text-align:left;">
Sequential
</td>
<td style="text-align:left;width: 5em; ">
hierarchical
</td>
<td style="text-align:left;width: 15em; ">
IVs enter in the order specified by the researcher
</td>
<td style="text-align:left;width: 15em; ">
evaluated in terms of what it contributes at the time it was entered (therefore, sometimes the weaker IVs are entered first
</td>
</tr>
<tr>
<td style="text-align:left;">
Statistical
</td>
<td style="text-align:left;width: 5em; ">
</td>
<td style="text-align:left;width: 15em; ">
entry order based solely on statistical criteria
</td>
<td style="text-align:left;width: 15em; ">
IVs are not evaluted in the same sense
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;width: 5em; ">
forward selection
</td>
<td style="text-align:left;width: 15em; ">
equation begins empty and each IV is entered one at a time
</td>
<td style="text-align:left;width: 15em; ">
criterion can be a significance level
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;width: 5em; ">
backward selection
</td>
<td style="text-align:left;width: 15em; ">
equation being full (all in) &amp; each IV deleted one at a time
</td>
<td style="text-align:left;width: 15em; ">
deleteion based on whether IV contributes substantially; if not, then delete
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;width: 5em; ">
stepwise selection
</td>
<td style="text-align:left;width: 15em; ">
equation starts out empty; IVs entered if they met statistical criterion, and deleted at any time when they no longer contribute
</td>
<td style="text-align:left;width: 15em; ">
a compromise between forward &amp; backward selection
</td>
</tr>
</tbody>
</table>
<div id="regression-model-selection-based-on-statistics" class="section level3" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Regression model selection based on statistics</h3>
<p>Use the <code>step()</code> function from the <code>stats</code> package (already installed with <code>base</code>) or the <code>stepAIC</code> function from the <code>MASS</code> pacakge.</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="multiple-regression-analysis.html#cb246-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> time <span class="sc">+</span> cits <span class="sc">+</span> pubs, <span class="at">data =</span> mydata)</span>
<span id="cb246-2"><a href="multiple-regression-analysis.html#cb246-2" aria-hidden="true" tabindex="-1"></a>step1 <span class="ot">&lt;-</span> <span class="fu">step</span>(fit, <span class="at">direction =</span> <span class="st">&quot;both&quot;</span>) <span class="co"># stepwise</span></span></code></pre></div>
<pre><code>## Start:  AIC=1102.26
## salary ~ time + cits + pubs
## 
##        Df Sum of Sq        RSS    AIC
## - pubs  1  59458298 2926312249 1101.5
## &lt;none&gt;              2866853951 1102.3
## - time  1 468968436 3335822387 1109.7
## - cits  1 634124345 3500978295 1112.7
## 
## Step:  AIC=1101.53
## salary ~ time + cits
## 
##        Df  Sum of Sq        RSS    AIC
## &lt;none&gt;               2926312249 1101.5
## + pubs  1   59458298 2866853951 1102.3
## - cits  1  696719756 3623032005 1112.8
## - time  1 1083431156 4009743405 1119.1</code></pre>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="multiple-regression-analysis.html#cb248-1" aria-hidden="true" tabindex="-1"></a>step2 <span class="ot">&lt;-</span> <span class="fu">step</span>(<span class="fu">lm</span>(salary <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> mydata), <span class="at">scope =</span> <span class="fu">formula</span>(fit), <span class="at">direction =</span> <span class="st">&quot;forward&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=1139.37
## salary ~ 1
## 
##        Df  Sum of Sq        RSS    AIC
## + time  1 2123587818 3623032005 1112.8
## + cits  1 1736876419 4009743405 1119.1
## + pubs  1 1472195326 4274424497 1123.0
## &lt;none&gt;               5746619823 1139.4
## 
## Step:  AIC=1112.77
## salary ~ time
## 
##        Df Sum of Sq        RSS    AIC
## + cits  1 696719756 2926312249 1101.5
## + pubs  1 122053710 3500978295 1112.7
## &lt;none&gt;              3623032005 1112.8
## 
## Step:  AIC=1101.53
## salary ~ time + cits
## 
##        Df Sum of Sq        RSS    AIC
## &lt;none&gt;              2926312249 1101.5
## + pubs  1  59458298 2866853951 1102.3</code></pre>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="multiple-regression-analysis.html#cb250-1" aria-hidden="true" tabindex="-1"></a>step3 <span class="ot">&lt;-</span> <span class="fu">step</span>(fit, <span class="at">direction =</span> <span class="st">&quot;backward&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=1102.26
## salary ~ time + cits + pubs
## 
##        Df Sum of Sq        RSS    AIC
## - pubs  1  59458298 2926312249 1101.5
## &lt;none&gt;              2866853951 1102.3
## - time  1 468968436 3335822387 1109.7
## - cits  1 634124345 3500978295 1112.7
## 
## Step:  AIC=1101.53
## salary ~ time + cits
## 
##        Df  Sum of Sq        RSS    AIC
## &lt;none&gt;               2926312249 1101.5
## - cits  1  696719756 3623032005 1112.8
## - time  1 1083431156 4009743405 1119.1</code></pre>
</div>
</div>
<div id="a-few-assumptions-in-regression" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> (A Few) Assumptions in Regression</h2>
<ul>
<li><p>Linear Relationship Exists Between Independent Variables and Dependent Variable</p></li>
<li><p>Homoscedasticity of Residuals (Residuals Have Constant Variance)</p></li>
<li><p>Residuals are Normally Distributed</p></li>
<li><p>Residuals are Independent</p></li>
<li><p>Lack of Multicollinearity</p></li>
</ul>
<p><strong>Note</strong>: See section 7.7.2.1 of textbook for a more complete list of assumptions</p>
<div id="linearity" class="section level3" number="5.6.1">
<h3><span class="header-section-number">5.6.1</span> Linearity</h3>
<p>Plot residuals against each independent variable and against the predicted values.</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="multiple-regression-analysis.html#cb252-1" aria-hidden="true" tabindex="-1"></a>mydata <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="st">&quot;data/profs.sav&quot;</span>)</span>
<span id="cb252-2"><a href="multiple-regression-analysis.html#cb252-2" aria-hidden="true" tabindex="-1"></a>mymodel <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> time <span class="sc">+</span> cits, <span class="at">data =</span> mydata)</span>
<span id="cb252-3"><a href="multiple-regression-analysis.html#cb252-3" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">resid</span>(mymodel)</span>
<span id="cb252-4"><a href="multiple-regression-analysis.html#cb252-4" aria-hidden="true" tabindex="-1"></a>fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mymodel)</span>
<span id="cb252-5"><a href="multiple-regression-analysis.html#cb252-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mydata, <span class="fu">aes</span>(<span class="at">x =</span> time, <span class="at">y =</span> res)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm, <span class="at">se=</span><span class="cn">FALSE</span>) <span class="sc">+</span>  <span class="fu">geom_smooth</span>(<span class="at">color=</span><span class="st">&quot;red&quot;</span>) <span class="co"># Add a loess smoothed fit curve </span></span></code></pre></div>
<p><img src="book_files/figure-html/unnamed-chunk-105-1.png" width="672" /></p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="multiple-regression-analysis.html#cb253-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mydata, <span class="fu">aes</span>(<span class="at">x =</span> cits, <span class="at">y =</span> res)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm, <span class="at">se=</span><span class="cn">FALSE</span>) <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">color=</span><span class="st">&quot;red&quot;</span>) </span></code></pre></div>
<p><img src="book_files/figure-html/unnamed-chunk-105-2.png" width="672" /></p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="multiple-regression-analysis.html#cb254-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mydata, <span class="fu">aes</span>(<span class="at">x =</span> fitted, <span class="at">y =</span> res)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm, <span class="at">se=</span><span class="cn">FALSE</span>) <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">color=</span><span class="st">&quot;red&quot;</span>) </span></code></pre></div>
<p><img src="book_files/figure-html/unnamed-chunk-105-3.png" width="672" /></p>
<p>Plot residuals against each independent variable and against the predicted values.</p>
<ul>
<li><p>The straight horizontal line (0-line) indicates where the residuals are zero. The mean of the residuals should be zero. The curved line is the loess fit line which follows the general trend of the data. If the relationship is linear, the loess fit line should not demonstrate any large or systematic deviations from the 0-line.</p></li>
<li><p>Violation of linearity leads to biased regression coefficients and standard errors, resulting in incorrect significance tests and confidence intervals.</p></li>
</ul>
<p>You can also use the <code>crPlots()</code> function (Components + Residual Plots) or the <code>ceresPlots()</code> function from the <code>car</code> package</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="multiple-regression-analysis.html#cb255-1" aria-hidden="true" tabindex="-1"></a>mydata <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="st">&quot;data/profs.sav&quot;</span>)</span>
<span id="cb255-2"><a href="multiple-regression-analysis.html#cb255-2" aria-hidden="true" tabindex="-1"></a>mymodel <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> time <span class="sc">+</span> cits, <span class="at">data =</span> mydata)</span>
<span id="cb255-3"><a href="multiple-regression-analysis.html#cb255-3" aria-hidden="true" tabindex="-1"></a><span class="fu">crPlots</span>(mymodel)</span></code></pre></div>
<p><img src="book_files/figure-html/unnamed-chunk-106-1.png" width="672" /></p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="multiple-regression-analysis.html#cb256-1" aria-hidden="true" tabindex="-1"></a><span class="co">#ceresPlots(mymodel)</span></span></code></pre></div>
</div>
<div id="homoscedasticity-constant-variance-assumption" class="section level3" number="5.6.2">
<h3><span class="header-section-number">5.6.2</span> Homoscedasticity (Constant Variance Assumption)</h3>
<p>Plot residuals against each independent variable and against the predicted values.</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="multiple-regression-analysis.html#cb257-1" aria-hidden="true" tabindex="-1"></a>mydata <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="st">&quot;data/profs.sav&quot;</span>)</span>
<span id="cb257-2"><a href="multiple-regression-analysis.html#cb257-2" aria-hidden="true" tabindex="-1"></a>mymodel <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> time <span class="sc">+</span> cits, <span class="at">data =</span> mydata)</span>
<span id="cb257-3"><a href="multiple-regression-analysis.html#cb257-3" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">resid</span>(mymodel)</span>
<span id="cb257-4"><a href="multiple-regression-analysis.html#cb257-4" aria-hidden="true" tabindex="-1"></a>fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mymodel)</span>
<span id="cb257-5"><a href="multiple-regression-analysis.html#cb257-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mydata, <span class="fu">aes</span>(<span class="at">x =</span> time, <span class="at">y =</span> res)) <span class="sc">+</span>  <span class="fu">geom_point</span>() <span class="sc">+</span>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm, <span class="at">se=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="book_files/figure-html/unnamed-chunk-107-1.png" width="25%" /></p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="multiple-regression-analysis.html#cb258-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mydata, <span class="fu">aes</span>(<span class="at">x =</span> cits, <span class="at">y =</span> res)) <span class="sc">+</span>  <span class="fu">geom_point</span>() <span class="sc">+</span>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm, <span class="at">se=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="book_files/figure-html/unnamed-chunk-107-2.png" width="25%" /></p>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="multiple-regression-analysis.html#cb259-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mydata, <span class="fu">aes</span>(<span class="at">x =</span> fitted, <span class="at">y =</span> res)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm, <span class="at">se=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="book_files/figure-html/unnamed-chunk-107-3.png" width="25%" /></p>
<ul>
<li><p>When examining these plots, you don’t want to see a relationship between the variability of the residuals and either the independent variables or the predicted values.</p></li>
<li><p>If homoscedasticity is voilated, the standard errors of estimates will be incorrect. Thus, significance tests (p-value, confidence interval) are also incorrect. However, regression coefficients will still be correctly estimated.</p></li>
<li><p>Use the <code>ncvTest()</code> function from the <code>car</code> package. This function computes a score test of the hypothesis of constant error variance against the alternative that the error variance changes with the level of the response (fitted values), or with a linear combination of predictors. The <code>ncvTest()</code> function performs the original version of Breusch-Pagan test.</p></li>
</ul>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="multiple-regression-analysis.html#cb260-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ncvTest</span>(mymodel)</span></code></pre></div>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 0.09487541, Df = 1, p = 0.75807</code></pre>
<ul>
<li>Use the <code>bptest()</code> function from the <code>lmtest</code> package. This function offers studentized Breusch-Pagan test and is more robust than the orignial B-P test.</li>
</ul>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="multiple-regression-analysis.html#cb262-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bptest</span>(mymodel)</span></code></pre></div>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  mymodel
## BP = 0.1054, df = 2, p-value = 0.9487</code></pre>
<ul>
<li>The <code>spreadLevelPlot()</code> function from the <code>car</code> package creates plots for examining the possible dependence of spread on level, or an extension of these plots to the studentized residuals from linear models. It also suggests power transformation for improving homoscedasticity.</li>
</ul>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="multiple-regression-analysis.html#cb264-1" aria-hidden="true" tabindex="-1"></a><span class="fu">spreadLevelPlot</span>(mymodel) <span class="co"># plot studentized residuals vs. fitted values</span></span></code></pre></div>
<p><img src="book_files/figure-html/unnamed-chunk-110-1.png" width="35%" /></p>
<pre><code>## 
## Suggested power transformation:  0.6806045</code></pre>
<p><strong>Homoscedasticity</strong> Remedies</p>
<p>When the homoscedasticity assumption is not met, variables can be transformed to stabilize the variance.</p>
<ul>
<li><p><strong>Natural Log Transformation</strong> can be used to reduce the skewness of variables and to stabilize the variance (values must be strictly positive to apply the log-function). <code>newy = log(y)</code></p></li>
<li><p><strong>Box-Cox transformation</strong> transforms a variable using the power <span class="math inline">\(\lambda\)</span> tha tis mostly likely to normalize the variable. Use the <code>powerTransform()</code> function in the <code>car</code> package. When <span class="math inline">\(\lambda = 0\)</span>, it is the log transformation.</p></li>
</ul>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="multiple-regression-analysis.html#cb266-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">powerTransform</span>(mydata<span class="sc">$</span>salary))</span></code></pre></div>
<pre><code>## bcPower Transformation to Normality 
##               Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd
## mydata$salary   -0.2629           0      -1.4766       0.9507
## 
## Likelihood ratio test that transformation parameter is equal to 0
##  (log transformation)
##                             LRT df    pval
## LR test, lambda = (0) 0.1808747  1 0.67062
## 
## Likelihood ratio test that no transformation is needed
##                           LRT df     pval
## LR test, lambda = (1) 4.20349  1 0.040341</code></pre>
</div>
<div id="normality" class="section level3" number="5.6.3">
<h3><span class="header-section-number">5.6.3</span> Normality</h3>
<p>How to check for <strong>Normality</strong> of residuals?</p>
<p>Plot a histogram and the normal QQ plot of residuals</p>
<ul>
<li>Want the distribution of residuals to resemble the normal curve.</li>
</ul>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="multiple-regression-analysis.html#cb268-1" aria-hidden="true" tabindex="-1"></a>mydata<span class="sc">$</span>fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mymodel)</span>
<span id="cb268-2"><a href="multiple-regression-analysis.html#cb268-2" aria-hidden="true" tabindex="-1"></a>mydata<span class="sc">$</span>res <span class="ot">&lt;-</span> <span class="fu">resid</span>(mymodel)</span>
<span id="cb268-3"><a href="multiple-regression-analysis.html#cb268-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mydata, <span class="fu">aes</span>(<span class="at">x=</span>res)) <span class="sc">+</span>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span>..density..), <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;white&quot;</span>) <span class="sc">+</span> <span class="fu">geom_density</span>(<span class="at">alpha=</span>.<span class="dv">2</span>, <span class="at">fill=</span><span class="st">&quot;#FF6666&quot;</span>) </span></code></pre></div>
<p><img src="book_files/figure-html/unnamed-chunk-112-1.png" width="30%" /></p>
<p>Plot a histogram and the normal QQ plot of residuals</p>
<ul>
<li>Want the residuals (in circles) to be close to the straight line in the QQ plot.</li>
</ul>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="multiple-regression-analysis.html#cb269-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mydata, <span class="fu">aes</span>(<span class="at">sample=</span>res)) <span class="sc">+</span> <span class="fu">stat_qq</span>() <span class="sc">+</span> <span class="fu">stat_qq_line</span>(<span class="at">color =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="book_files/figure-html/unnamed-chunk-113-1.png" width="30%" /></p>
<ul>
<li><p>If normality is violated, standard errors are incorrect. Thus, significance tests (p-value, confidence interval) are also incorrect, especially for small samples (In large sample sizes, violation of this assumption is not a big problem). However, regression coefficients will still be correctly estimated.</p></li>
<li><p>The Shapiro-Wilk test can also be used to check for normaility for small to moderate samples</p></li>
<li><p>There a few other statistical tests for normaility: Kolmogorov-Smirnov (K-S), Anderson-Darling, D’Agostino, Jarque-Bera.</p></li>
</ul>
</div>
<div id="independence-of-residuals" class="section level3" number="5.6.4">
<h3><span class="header-section-number">5.6.4</span> Independence of Residuals</h3>
<p>A violation of this assumption typically occurs under 2 conditions:</p>
<ul>
<li><p>Clustering: When data are collected from groups or other clusters (e.g., people within different classes, people within different schools, people within different school districts, people who see different doctors, order of participation in a study, etc.). In this case, the residuals may be more similar within the clusters than between the clusters.</p></li>
<li><p>Serial Dependency: When data are repeatedly collected from a single individual or the same sample of individuals over time (longitudinal studies), the residuals will often show serial dependency. Measures at adjacent times will tend to have more similar values.</p></li>
</ul>
<p>For clustering, plot residuals against the cluster variable.</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="multiple-regression-analysis.html#cb270-1" aria-hidden="true" tabindex="-1"></a>mydata<span class="sc">$</span>dept <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(mydata<span class="sc">$</span>dept) <span class="co"># convert `dept` as a factor variable</span></span>
<span id="cb270-2"><a href="multiple-regression-analysis.html#cb270-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mydata, <span class="fu">aes</span>(<span class="at">x=</span>dept, <span class="at">y=</span>res)) <span class="sc">+</span></span>
<span id="cb270-3"><a href="multiple-regression-analysis.html#cb270-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>()</span></code></pre></div>
<p><img src="book_files/figure-html/unnamed-chunk-114-1.png" width="672" /></p>
<p>Do the boxplots indicate much variability in the median value of thh residuals in each group?</p>
<ul>
<li><p>If there is clustering, the estimated regression coefficients are unbiased, but standard errors will typically be too small, leading to incorrect significance tests and confidence intervals.</p></li>
<li><p>Dummy coded variables can be included as additional independent variables in the regression model to address clustering. Mulitivel modeling may be used as an alternative.</p></li>
</ul>
<p>For Serial Dependency, use Durbin-Watson Test. A value of 2 means the residuals are uncorrelated. A value less than 2 suggests positive autocorrelation; a value greater than 2 suggess negative autocorrelation.</p>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="multiple-regression-analysis.html#cb271-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dwt</span>(mymodel)</span></code></pre></div>
<pre><code>##  lag Autocorrelation D-W Statistic p-value
##    1     -0.06087733      2.099968    0.67
##  Alternative hypothesis: rho != 0</code></pre>
</div>
<div id="multicollinearity" class="section level3" number="5.6.5">
<h3><span class="header-section-number">5.6.5</span> Multicollinearity</h3>
<p><strong>Multicollinearity</strong>: High correlations among some independent (predictor) variables which are included in the regression model.</p>
<ul>
<li>The more a predictor is correlated with other predictors, the less unique information it can contribute to the prediction of the dependent (criterion) variable.</li>
</ul>
<p><strong>Exact Collinearity</strong>: Occurs when 1 independent variable has a correlation or a multiple correlation of 1.0 with the other independent variables. In this case, this independent variable cannot contribute any unique information that is not contained in the other independent variables.</p>
<ul>
<li>When exact collinearity occurs, there is no mathematically unique solution for the regression coefficients and regression is impossible.</li>
</ul>
<p><strong>Example 1</strong>: X1 is weight of person in pounds; X2 is weight of person in kilograms</p>
<ul>
<li>Both variables contain the same information. Each is just a transformation of the other variable and will correlate perfectly. When running this regression, one of the variables will automatically be kicked out because they are perfectly correlated.</li>
</ul>
<p><span class="math inline">\({\hat Y}=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}\)</span>, where X1 and X2 are perfectly correlated (each X1 score is 2 points higher than its corresponding X2 score; X1 is a linear transformation of X2), and each of these predictors is correlated with the dependent variable at .967. Watch what happens:</p>
<p><span class="math inline">\({{\beta }_{1}}=\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}=\frac{0}{0}\)</span></p>
<p><span class="math inline">\({{\beta }_{2}}=\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{1-r_{12}^{2}}=\frac{0}{0}\)</span></p>
<p><strong>Example 2</strong>: X1 is score at time 1; X2 is score at time 2; X3 is difference between scores at time 1 and time 2</p>
<p><span class="math inline">\({\hat Y}=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+{{b}_{3}}{{X}_{3}}\)</span></p>
<ul>
<li>X3 can be perfectly predicted by X1 and X2. When running this regression, one of the variables would be kicked out.</li>
</ul>
<div id="measures-of-the-degree-of-multicollinearity" class="section level4" number="5.6.5.1">
<h4><span class="header-section-number">5.6.5.1</span> measures of the degree of multicollinearity</h4>
<ul>
<li><p>Squared correlation between two independent (predictor) variables. The closer the value is to 1.0, the more multicollinearity is indicated. With increasing numbers of predictor variables, this measure is more likely to miss substantial multicollinearity.</p></li>
<li><p>Tolerance: A statistic used to determine how much the independent variables are linearly related to one another (multicollinear). It indicates the proportion of a variable’s variance not accounted for by other independent variables in the equation. A variable with very low tolerance contributes little information to a model, and can cause computational problems. It is calculated as 1 minus R squared for an independent variable when it is predicted by the other independent variables already included in the analysis: <span class="math inline">\(1-R_{j}^{2}\)</span>. If Tolerance is less than .10, severe multicollinearity for the corresponding variable exists. Some think this value is too lenient and recommend a cutoff value of .20 instead.</p></li>
<li><p>Variance Inflation Factor: The reciprocal of the Tolerance: <span class="math inline">\(\frac{1}{\text{Toleranc}{{\text{e}}_{\text{j}}}}\)</span>. As the variance inflation factor increases, so does the variance of the regression coefficient, making it an unstable estimate. Large VIF values are an indicator of multicollinearity. VIF of 10 or higher indicates severe multicollinearity for the corresponding predictor variable. Some think this value is too lenient and instead recommend a cutoff value of 4.<br />
</p></li>
<li><p>Condition Indices and Variance-Decomposition Proportions: These are results from a process known as singular value decomposition (SVD). Condition Number (CN) = <span class="math inline">\(\sqrt {\frac{{{\lambda _{\max }}}}{{{\lambda _{\min }}}}}\)</span></p>
<ul>
<li><p>Condition Index (CI) = <span class="math inline">\(\sqrt{\frac{{{\lambda }_{\max }}}{{{\lambda }_{i}}}}\)</span></p></li>
<li><p>Rule of thumb for CI: 0-10, weak dependency; 11-30, moderate dependency; &gt;30, strong dependency. CI is best used together with the variance-decomposition proportions. Variance-decomposition proportions are the proportions of variance of the intercept (a) and each of the regression coefficient (b) association with each CI.</p></li>
</ul></li>
</ul>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="multiple-regression-analysis.html#cb273-1" aria-hidden="true" tabindex="-1"></a>mydata <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="st">&quot;data/profs.sav&quot;</span>)</span>
<span id="cb273-2"><a href="multiple-regression-analysis.html#cb273-2" aria-hidden="true" tabindex="-1"></a>mymodel <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> time <span class="sc">+</span> cits, <span class="at">data =</span> mydata)</span>
<span id="cb273-3"><a href="multiple-regression-analysis.html#cb273-3" aria-hidden="true" tabindex="-1"></a><span class="co">#library(car)</span></span>
<span id="cb273-4"><a href="multiple-regression-analysis.html#cb273-4" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(mymodel)</span></code></pre></div>
<pre><code>##     time     cits 
## 1.161517 1.161517</code></pre>
</div>
<div id="problems-of-multicollinearity" class="section level4" number="5.6.5.2">
<h4><span class="header-section-number">5.6.5.2</span> problems of multicollinearity</h4>
<ul>
<li>Unstable regression coefficients that are associated with large standard errors.</li>
</ul>
<p><span class="math inline">\({{t}_{1}}=\frac{{{b}_{1}}-0}{S{{E}_{1}}}\tilde{\ }df=n-k-1\)</span> where <span class="math inline">\(S{{E}_{1}}=\frac{s{{d}_{res}}}{\sqrt{\sum{x_{1}^{2}(1-r_{12}^{2})}}}\)</span> where <span class="math inline">\({sd_{res}}\)</span> is the standard deviation of the residuals, and the denominator is the square root of the product of the sum of squares of X scores and the variance not shared between X1 and X2, <span class="math inline">\(1-{r_{12}^{2}}\)</span>. Smaller standard errors represent better estimation of the true population parameters using our sample estimates of <span class="math inline">\({{b}_{1}}\)</span> and <span class="math inline">\({{b}_{2}}\)</span>, meaning that they won’t vary much from sample to sample. All other things being equal, as the correlation between X1 and X2 increases, the standard error increases, thus, decreasing our likelihood of a significant finding.</p>
<ul>
<li>Another problem is difficulty when interpreting regression coefficients due to the shared variance between X1 and X2.</li>
</ul>
</div>
</div>
</div>
<div id="multiple-regression-and-patterns-of-association" class="section level2" number="5.7">
<h2><span class="header-section-number">5.7</span> Multiple Regression and Patterns of Association</h2>
<p>Complete Independence in Multiple Regression</p>
<p>When the predictors are all independent (not correlated), the following relationship holds:</p>
<p><span class="math inline">\({{R}^{2}}=r_{Y1}^{2}+r_{Y2}^{2}+\cdots +r_{Yk}^{2}\)</span>. The sum of the squared correlations between each independent variable and the criterion sum up to the multiple regression model’s <span class="math inline">\(R^2\)</span>.</p>
<div id="complete-independence" class="section level3" number="5.7.1">
<h3><span class="header-section-number">5.7.1</span> Complete independence</h3>
<p><img src="image/complete_independence.png" width="150%" /></p>
<p><span class="math inline">\({{r}_{Y1}}=.5\)</span> <span class="math inline">\({{r}_{Y2}}=.5\)</span> <span class="math inline">\({{r}_{X1X2}}=0.0\)</span></p>
<p>Recall: <span class="math inline">\({{R}^{2}}=\frac{r_{Y1}^{2}+r_{Y2}^{2}-2{{r}_{Y1}}{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}\)</span></p>
<p><span class="math inline">\(a=r_{Y1}^{2}=sr_{1}^{2}=\beta _{1}^{2}\)</span> and <span class="math inline">\(b=r_{Y2}^{2}=sr_{2}^{2}=\beta _{2}^{2}\)</span></p>
<p>Recall: <span class="math inline">\(s{{r}_{1}}=\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{\sqrt{1-r_{12}^{2}}}\)</span> <span class="math inline">\(s{{r}_{2}}=\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{\sqrt{1-r_{12}^{2}}}\)</span></p>
<p><span class="math inline">\({{\beta }_{1}}=\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}\)</span> <span class="math inline">\({{\beta }_{2}}=\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{1-r_{12}^{2}}\)</span></p>
<p>In this case, the squared semi-partial correlation for each predictor is equal to the squared value of its correlation with the dependent variable and the squared value of its associated standardized regression coefficient.</p>
</div>
<div id="partial-redundancy" class="section level3" number="5.7.2">
<h3><span class="header-section-number">5.7.2</span> Partial redundancy</h3>
<p><img src="image/partial_redundancy.png" width="150%" /></p>
<p>An indication of this occurrence is when <span class="math inline">\({{r}_{Y1}}&gt;{{r}_{Y2}}{{r}_{12}}\)</span> and <span class="math inline">\({{r}_{Y2}}&gt;{{r}_{Y1}}{{r}_{12}}\)</span>. The semi-partial correlation and the standardized partial regression coefficient for each predictor will be smaller than its correlation with the dependent/criterion variable.</p>
</div>
<div id="complete-redundancy" class="section level3" number="5.7.3">
<h3><span class="header-section-number">5.7.3</span> Complete redundancy</h3>
<p><img src="image/complete_redundancy.png" width="150%" /></p>
<p>This does not necessarily mean that X1 and X2 are perfectly correlated. It implies that adding X2 to a regression containing X1 does not increase <span class="math inline">\({{R}^{2}}\)</span> at all. A predictor variable may correlate highly with the criterion, but this correlation becomes zero when controlling for the other predictor variable.</p>
</div>
<div id="suppression-in-multiple-regression" class="section level3" number="5.7.4">
<h3><span class="header-section-number">5.7.4</span> Suppression in multiple regression</h3>
<ul>
<li><p>Suppression is present when either <span class="math inline">\({{r}_{Y1}}&lt;{{r}_{Y2}}{{r}_{12}}\)</span> or <span class="math inline">\({{r}_{Y2}}&lt;{{r}_{Y1}}{{r}_{12}}\)</span>, or when the correlation between independent variables is negative and the correlations between each predictor and the criterion are positive. The standardized partial regression coefficient for each predictor will be larger than its correlation with the dependent/criterion variable and one of them may become negative.</p></li>
<li><p>The relationship between the independent variables is hiding (suppressing) their real relationships with the dependent variable, which would be larger or possibly of opposite sign if the independent variables were not correlated. The inclusion of the suppressor in the regression equation removes (suppresses) the unwanted variance in X1, in effect, and enhances the relationship between X1 and Y. It removes the irrelevant variance that it shares with the independent variable and not with the dependent variable.</p></li>
<li><p>If a variable with a positive correlation or a correlation of zero with the dependent variable has a significantly negative partial regression coefficient, it is a suppressor variable.</p></li>
</ul>
</div>
<div id="summary-of-patterns-of-association" class="section level3" number="5.7.5">
<h3><span class="header-section-number">5.7.5</span> Summary of patterns of association</h3>
<table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Relationship Between Predictors
</th>
<th style="text-align:left;">
Possible Effects on Statistics
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;border-bottom: 1px solid">
Complete Independence
</td>
<td style="text-align:left;width: 30em; border-bottom: 1px solid">
<span class="math inline">\(r_{Y1}^2 = sr_1^2 = \beta _1^2\)</span> and <span class="math inline">\(r_{Y2}^2 = sr_2^2 = \beta _2^2\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;border-bottom: 1px solid">
Partial Redundancy
</td>
<td style="text-align:left;width: 30em; border-bottom: 1px solid">
<span class="math inline">\(sr_1\&amp;amp; {\beta _1} &amp;lt; {r_{Y1}}\)</span> and <span class="math inline">\(sr_2\&amp;amp; {\beta _2} &amp;lt; {r_{Y2}}\)</span>; <span class="math inline">\({r_{Y1}} &amp;gt; {r_{Y2}}{r_{12}}\)</span> and <span class="math inline">\({r_{Y2}} &amp;gt; {r_{Y1}}{r_{12}}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;border-bottom: 1px solid">
Complete Redundancy
</td>
<td style="text-align:left;width: 30em; border-bottom: 1px solid">
<span class="math inline">\(s{r_1}\&amp;amp; {\beta _1} \approx 0\)</span> and <span class="math inline">\(s{r_2}\&amp;amp; {\beta _2} &amp;gt; 0\)</span>; OR <span class="math inline">\(s{r_1}\&amp;amp; {\beta _1} &amp;gt; 0\)</span> and <span class="math inline">\(s{r_2}\&amp;amp; {\beta _2} \approx 0\)</span> and both predictors highly correlated with criterion,
</td>
</tr>
<tr>
<td style="text-align:left;">
Suppression
</td>
<td style="text-align:left;width: 30em; ">
<span class="math inline">\({\beta _1} &amp;gt; {r_{Y1}}\)</span>, and <span class="math inline">\({\beta _2} &amp;gt; {r_{Y2}}\)</span>; <span class="math inline">\({\beta _1}\)</span> or <span class="math inline">\({\beta _2}\)</span> may become negative even if correlation with criterion is positive;
correlation between predictors is negative and correlation between each predictor and criterion is positive;
correlation between predictor and criterion is zero but has a significant negative partial regression coefficient.
</td>
</tr>
</tbody>
</table>
</div>
<div id="statistical-paradoxes" class="section level3" number="5.7.6">
<h3><span class="header-section-number">5.7.6</span> Statistical paradoxes</h3>
<p><strong>Suppression</strong> is a statistical paradox. The classical definition of suppression is that a potential covariate that is unrelated to the outcome variable (i.e. has a bivariate correlation of zero) increases the overall model fit within regression (as assessed by <span class="math inline">\(R_2\)</span>, for instance) when this covariate is added to the model. This seems counter-intuitive. Two other famous statistical paradoxes are:</p>
<ul>
<li><p><strong>Simpson’s Paradox:</strong> An association between two variables in a population emerges, disappears or reverses when the population is divided into subpopulations.</p></li>
<li><p><strong>Lord’s paradox:</strong> The relationship between a continuous outcome and a categorical variable being reversed when an additional continuous covariate is introduced to the analysis.</p></li>
</ul>
<p>These paradoxes are not problematic from the perspective of mathematics and probability theory but can be surprising for many people.</p>
<p>Statistical modeling techniques such as <a href="https://zewang.netlify.app/books/sem/_book/">structural equation modeling</a> and <strong>multilevel modeling</strong> can help understand these paradoxes. To dig deeper, you probably want to read or take a course on <em>causal inference</em>.</p>
<!-- Search Statistical Paradoxes (According to Tu, Gunnell, and Gilthorpe, Lord's paradox is the continuous version of Simpson's paradox. [7] Those authors state that Lord's Paradox, Simpson's Paradox, and the suppression of covariates by uncorrelated predictor variables are all the same thing, namely a reversal paradox.) -->
</div>
</div>
<div id="regression-diagnostics" class="section level2" number="5.8">
<h2><span class="header-section-number">5.8</span> Regression Diagnostics</h2>
<p>Outliers Diagnostics</p>
<p><strong>Outliers</strong>: One or more atypical points that do not fit with the data.</p>
<table>
<thead>
<tr>
<th style="text-align:right;">
case
</th>
<th style="text-align:right;">
time
</th>
<th style="text-align:right;">
pubs
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
18
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
17
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
11
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
60
</td>
<td style="text-align:right;">
6
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
38
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
48
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
9
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
22
</td>
</tr>
<tr>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
30
</td>
</tr>
<tr>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
21
</td>
</tr>
<tr>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
10
</td>
</tr>
<tr>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
27
</td>
</tr>
<tr>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
37
</td>
</tr>
</tbody>
</table>
<p>Just skimming the data, case number 6 appears to be an outlier.</p>
<p><img src="book_files/figure-html/unnamed-chunk-122-1.png" width="672" /></p>
<p>Three characteristics of badly behaved data:
- Leverage: How unusual is the case in terms of its values on the independent (predictor) variable?</p>
<ul>
<li><p>Discrepancy: Distance between predicted and observed values on the outcome variable.</p></li>
<li><p>Influence: Reflects amount that regression coefficients would change if the outlier were removed from the data set.</p></li>
</ul>
<div id="leverage" class="section level3" number="5.8.1">
<h3><span class="header-section-number">5.8.1</span> Leverage</h3>
<p><strong>Leverage</strong>: Reflects only the case’s standing on the set of independent (predictor) variables. For each case, leverage tells us how far observed values for the case are from the mean values on the set of independent variables.</p>
<p>–</p>
<p><em>FYI</em>: For one independent variable, leverage = <span class="math inline">\({{h}_{i}}=\frac{1}{n}+\frac{{{({{X}_{i}}-\bar{X})}^{2}}}{\sum{{{x}^{2}}}}\)</span> where <span class="math inline">\({{h}_{i}}\)</span> is the leverage for case i, n is the number of cases, <span class="math inline">\({{X}_{i}}\)</span> is the score for case i on the predictor variable, <span class="math inline">\(\bar{X}\)</span> is the mean of X, and <span class="math inline">\(\sum{{{x}^{2}}}\)</span>is the sum over the n cases of the squared deviations of <span class="math inline">\({{X}_{i}}\)</span> from the mean. For more than one independent variable, leverage = <span class="math inline">\(\mathbf{H}=\mathbf{X}{{(\mathbf{{X}&#39;X})}^{-\mathbf{1}}}\mathbf{{X}&#39;}\)</span> where X is the n x (k + 1) matrix of X values. These can range from 1/n to 1.</p>
<p>–
- Cases with leverage values greater than 2-3 times the average value are considered to have high leverage</p>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="multiple-regression-analysis.html#cb275-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(pubs <span class="sc">~</span> time)</span>
<span id="cb275-2"><a href="multiple-regression-analysis.html#cb275-2" aria-hidden="true" tabindex="-1"></a>hi <span class="ot">&lt;-</span> <span class="fu">hatvalues</span>(model)</span>
<span id="cb275-3"><a href="multiple-regression-analysis.html#cb275-3" aria-hidden="true" tabindex="-1"></a>hi[(hi <span class="sc">&gt;</span> <span class="dv">3</span> <span class="sc">*</span> <span class="fu">mean</span>(hi))] </span></code></pre></div>
<pre><code>##         6 
## 0.9044069</code></pre>
</div>
<div id="discrepancy" class="section level3" number="5.8.2">
<h3><span class="header-section-number">5.8.2</span> Discrepancy</h3>
<p><strong>Discrepancy</strong>: Extremity on the dependent variable. Distance between the predicted and observed values on the dependent variable. Use Studentized Residuals.</p>
<p>The <code>outlierTest()</code> function from the <code>car</code> package tests the single largest residual for significance as an outlier. If it isn’t significant, there are no outliers in the data. If it is significant, you must delete it and rerun the test to see if others are present.</p>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="multiple-regression-analysis.html#cb277-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rstudent</span>(model) <span class="co"># studentized residuals</span></span>
<span id="cb277-2"><a href="multiple-regression-analysis.html#cb277-2" aria-hidden="true" tabindex="-1"></a><span class="do">##           1           2           3           4           5           6           7           8           9          10          11 </span></span>
<span id="cb277-3"><a href="multiple-regression-analysis.html#cb277-3" aria-hidden="true" tabindex="-1"></a><span class="do">## -0.17131041 -1.28478010 -1.39891896 -0.21829507 -0.64134592 -3.29427276  1.37772714  2.35025411 -0.83472973  0.11829003  0.69147074 </span></span>
<span id="cb277-4"><a href="multiple-regression-analysis.html#cb277-4" aria-hidden="true" tabindex="-1"></a><span class="do">##          12          13          14          15 </span></span>
<span id="cb277-5"><a href="multiple-regression-analysis.html#cb277-5" aria-hidden="true" tabindex="-1"></a><span class="do">##  0.05234216 -0.72571300  0.49473073  1.30864113</span></span>
<span id="cb277-6"><a href="multiple-regression-analysis.html#cb277-6" aria-hidden="true" tabindex="-1"></a><span class="co"># library(car)</span></span>
<span id="cb277-7"><a href="multiple-regression-analysis.html#cb277-7" aria-hidden="true" tabindex="-1"></a><span class="fu">outlierTest</span>(model)</span>
<span id="cb277-8"><a href="multiple-regression-analysis.html#cb277-8" aria-hidden="true" tabindex="-1"></a><span class="do">## No Studentized residuals with Bonferroni p &lt; 0.05</span></span>
<span id="cb277-9"><a href="multiple-regression-analysis.html#cb277-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Largest |rstudent|:</span></span>
<span id="cb277-10"><a href="multiple-regression-analysis.html#cb277-10" aria-hidden="true" tabindex="-1"></a><span class="do">##    rstudent unadjusted p-value Bonferroni p</span></span>
<span id="cb277-11"><a href="multiple-regression-analysis.html#cb277-11" aria-hidden="true" tabindex="-1"></a><span class="do">## 6 -3.294273          0.0064084     0.096126</span></span></code></pre></div>
<ul>
<li><p>In large samples, cases with studentized deleted residuals greater than ± 3 or ± 4 are considered to have large discrepancy.</p></li>
<li><p>In small samples, cases with studentized deleted residuals greater than ± 2 are considered to have large discrepancy.</p></li>
</ul>
</div>
<div id="influence" class="section level3" number="5.8.3">
<h3><span class="header-section-number">5.8.3</span> Influence</h3>
<p><strong>Influence</strong>: Combines information from measures of leverage and discrepancy to inform us how the regression equation would change if a case were removed from the data set.</p>
<p><strong>Cook’s D</strong>: A measure of how much the residuals of all cases would change if a particular case were excluded from the calculation of the regression coefficients.</p>
<p><em>FYI</em>: Cook’s <span class="math inline">\({{D}_{i}}=\frac{\sum{{{({\hat Y}-{{{{\hat Y}}}_{(i)}})}^{2}}}}{(k+1){{MS}_{res}}}\)</span> where <span class="math inline">\({\hat Y}\)</span> is the predicted value of Y with all the cases included, <span class="math inline">\({{{\hat Y}&#39;}_{(i)}}\)</span> is the predicted value of Y with case i deleted, k is the number of independent variables, and <span class="math inline">\({{MS}_{Res}}\)</span> is the Mean Square residual with all the cases included. Thus, it compares the predicted value of Y with case i included and deleted for all cases in the data set. These differences are squared and then summed. The denominator serves to standardize the value. Values range upward from its potential minimum value of zero. The value will always be positive.</p>
<ul>
<li>Case with Cook’s D values greater than 1 or <span class="math display">\[4/(n - k - 1)\]</span> are considered high influential observations.</li>
</ul>
<p>Get Cook’s D</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="multiple-regression-analysis.html#cb278-1" aria-hidden="true" tabindex="-1"></a>di <span class="ot">&lt;-</span> <span class="fu">cooks.distance</span>(model)</span>
<span id="cb278-2"><a href="multiple-regression-analysis.html#cb278-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(di)</span>
<span id="cb278-3"><a href="multiple-regression-analysis.html#cb278-3" aria-hidden="true" tabindex="-1"></a><span class="do">##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. </span></span>
<span id="cb278-4"><a href="multiple-regression-analysis.html#cb278-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  0.000123  0.005616  0.021811  1.984229  0.072340 29.203927</span></span>
<span id="cb278-5"><a href="multiple-regression-analysis.html#cb278-5" aria-hidden="true" tabindex="-1"></a>di[di <span class="sc">&gt;</span><span class="dv">1</span>]</span>
<span id="cb278-6"><a href="multiple-regression-analysis.html#cb278-6" aria-hidden="true" tabindex="-1"></a><span class="do">##        6 </span></span>
<span id="cb278-7"><a href="multiple-regression-analysis.html#cb278-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 29.20393</span></span>
<span id="cb278-8"><a href="multiple-regression-analysis.html#cb278-8" aria-hidden="true" tabindex="-1"></a>di[di <span class="sc">&gt;</span> <span class="dv">4</span><span class="sc">/</span>(<span class="fu">length</span>(pubs)<span class="sc">-</span><span class="dv">1-1</span>)]</span>
<span id="cb278-9"><a href="multiple-regression-analysis.html#cb278-9" aria-hidden="true" tabindex="-1"></a><span class="do">##        6 </span></span>
<span id="cb278-10"><a href="multiple-regression-analysis.html#cb278-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 29.20393</span></span></code></pre></div>
<p>Additional Influence Measures:</p>
<ul>
<li>Standardized difference in fit. Indicates how much the predicted value for case i would change if case i were deleted from the data set.</li>
<li>Standardized DFBETA. Compares regression coefficients when case i is included versus not included in the data set.<br />
</li>
<li>Use <code>?influence.measures</code> to see more</li>
</ul>
</div>
<div id="sources-of-outliers-and-remedies" class="section level3" number="5.8.4">
<h3><span class="header-section-number">5.8.4</span> Sources of Outliers and Remedies</h3>
<p>Sources of outliers:</p>
<ol style="list-style-type: decimal">
<li>Error of execution of the research procedure (e.g., interviewer misread questions; experimenter delivered wrong treatment).</li>
<li>Inaccurate measurement of the dependent variable (e.g., equipment failed).</li>
<li>Errors in recording or keying of the data (e.g., wrote down wrong response; data entered into computer incorrectly).</li>
<li>Errors in calculation of the measures (e.g., incorrectly add up number of responses).</li>
<li>Nonattentive participants (e.g., participant fatigue, illness).</li>
<li>Rare cases (e.g., outlying observations are correct; represent a valid but rare observation in the population).</li>
</ol>
<p>Remedies:</p>
<p>Contaminated outlying data points (discussed in 1 through 5 above) are easy to correct. Correct, delete, or replace them as is appropriate. It is when you encounter rare cases it becomes more difficult.</p>
<ul>
<li><p>Option 1. Delete the outlier cases and reanalyze the remaining data. The regression coefficients typically won’t change much, hopefully. In this case, researchers typically discuss findings with the outlier included, but do mention (sometimes in a footnote) the results with outliers deleted and the nature of the outliers. If the results do change drastically when deleting outlying cases, researchers may need to use other options.</p></li>
<li><p>Option 2. Correct specification of the model needs to be ensured (e.g., does another variable need to be included to fit the data better? Is there a curvilinear relationship? Does an interaction term need to be included?).</p></li>
<li><p>Option 3. Transformation of the data. Transforming variables can also sometimes help with violation of normality, linearity and homoscedasticity.</p></li>
<li><p>Option 4. Robust regression procedures, such as Least Absolute Deviation estimation or Least Trimmed Squares estimation.</p></li>
</ul>
</div>
</div>
<div id="curvilinear-regression" class="section level2" number="5.9">
<h2><span class="header-section-number">5.9</span> Curvilinear Regression</h2>
<p>Curvilinear regression analysis is the modeling of nonlinear relationships between predictor variables and the criterion.</p>
<ul>
<li><p>Linear regression refers to the regression models that take the form: <span class="math inline">\(Y=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+...+{{b}_{k}}{{X}_{k}}\)</span></p></li>
<li><p>More formally, the regression model above is said to be linear in the parameters (or linear in the coefficients), where the parameters refer to the intercept and coefficients, <span class="math inline">\(a\)</span>, <span class="math inline">\({{b}_{1}}\)</span>, <span class="math inline">\({{b}_{2}}\)</span>, …, <span class="math inline">\({{b}_{k}}\)</span>. If a regression equation is linear in the parameters, then the predicted Y value is a linear combination of the predictor variables.</p></li>
<li><p>There are four broad classes of approaches to examining nonlinear relationships in multiple regression. Polynomial regression is traditional and commonly used in behavioral sciences. Second is the use of monotonic nonlinear transformations such as logarithms or exponents. Third is nonlinear regression in which the central point of the analysis is estimation of complex (nonlinear) relationships among variables that may be implied by theory. Fourth are nonparametric regression approaches.</p></li>
</ul>
<div id="polynomial-regression" class="section level3" number="5.9.1">
<h3><span class="header-section-number">5.9.1</span> Polynomial regression</h3>
<ul>
<li>Power polynomials are a convenient method of fitting curves. In a polynomial regression equation, the predictor is raised to one or more powers. The highest power to which a predictor variable is raised is known as the “degree” or “order” of the polynomial equation.</li>
</ul>
<p>Linear: <span class="math inline">\({\hat Y}=a+{{b}_{1}}X\)</span></p>
<p>Quadratic: <span class="math inline">\({\hat Y}=a+{{b}_{1}}X+{{b}_{2}}{{X}^{2}}\)</span></p>
<p>Cubic: <span class="math inline">\({\hat Y}&#39;=a+{{b}_{1}}X+{{b}_{2}}{{X}^{2}}+{{b}_{3}}{{X}^{3}}\)</span></p>
<p>Quartic: <span class="math inline">\({\hat Y}=a+{{b}_{1}}X+{{b}_{2}}{{X}^{2}}+{{b}_{3}}{{X}^{3}}+{{b}_{4}}{{X}^{4}}\)</span></p>
<p>Quintic: <span class="math inline">\({\hat Y}=a+{{b}_{1}}X+{{b}_{2}}{{X}^{2}}+{{b}_{3}}{{X}^{3}}+{{b}_{4}}{{X}^{4}}+{{b}_{5}}{{X}^{5}}\)</span></p>
<ul>
<li><p>The order, or degree of the polynomial indicates the number of bends in the regression curve.</p></li>
<li><p>The highest possible degree that a given model may take is (g-1), where g is the number of distinct values of the predictor variable.</p></li>
<li><p>The researcher is usually interested in identifying the highest-order model that fully describes the data. A regression model can be built that comprises of polynomial equations of increasing order until going further doesn’t account for a significant amount of incremental variance in the criterion variable (i.e., adding a higher-order term doesn’t contribute significantly to overall prediction above and beyond all lower order terms that are already in the model). However, from a research perspective, the nonlinear curve fitting with power polynomials should make substantive sense. Theory should guide the choice and for the most part theory in social sciences predicts quadratic, and at most cubic relationships. In addition, coefficients of higher order polynomials (above cubic) are difficulty to interpret.</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Center the (continuous) predictor variables. Lower order coefficients in higher order regression equations only have meaningful interpretation if the variable with which we are working has a meaningful zero. By centering ( <span class="math inline">\(X - \bar X\)</span>), zero on the predictor variable is its mean. DO NOT CENTER THE DEPENDENT VARIABLE.</p></li>
<li><p>In order that higher order terms have meaning, all lower order terms must be included, since higher order terms are reflective of the specific level of curvature they represent only if all lower order terms are partialed out.</p></li>
<li><p>Ploynomial regression is carried out as an ordinary hierarchical regression analysis.</p></li>
</ol>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="multiple-regression-analysis.html#cb279-1" aria-hidden="true" tabindex="-1"></a>mydata<span class="sc">$</span>newtime <span class="ot">&lt;-</span> mydata<span class="sc">$</span>time <span class="sc">-</span> <span class="fu">mean</span>(mydata<span class="sc">$</span>time)</span>
<span id="cb279-2"><a href="multiple-regression-analysis.html#cb279-2" aria-hidden="true" tabindex="-1"></a>mymodel2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> newtime <span class="sc">+</span> cits <span class="sc">+</span> <span class="fu">I</span>(newtime<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> mydata)</span>
<span id="cb279-3"><a href="multiple-regression-analysis.html#cb279-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mymodel2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ newtime + cits + I(newtime^2), data = mydata)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14925.6  -4293.6   -527.9   3908.2  22528.9 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  46350.421   2524.930  18.357  &lt; 2e-16 ***
## newtime       1085.629    298.989   3.631 0.000599 ***
## cits           212.495     57.155   3.718 0.000455 ***
## I(newtime^2)    -4.579     36.862  -0.124 0.901573    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7102 on 58 degrees of freedom
## Multiple R-squared:  0.4909, Adjusted R-squared:  0.4646 
## F-statistic: 18.64 on 3 and 58 DF,  p-value: 1.377e-08</code></pre>
<ul>
<li>What if the <code>time</code> variable was not centered?</li>
</ul>
</div>
</div>
<div id="useful-functions-and-symbols-commonly-used-in-r-when-fitting-linear-models" class="section level2" number="5.10">
<h2><span class="header-section-number">5.10</span> Useful Functions and Symbols Commonly Used in R When Fitting Linear Models</h2>
<table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-127">Table 5.2: </span>Useful Functions in R When Fitting Linear Models
</caption>
<thead>
<tr>
<th style="text-align:left;">
Function
</th>
<th style="text-align:left;">
Action
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;border-right:1px solid;">
summary()
</td>
<td style="text-align:left;">
Displays detailed results for the fitted model
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
coefficients()
</td>
<td style="text-align:left;">
Lists the model parameters (intercept and slopes) for the fitted model
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
confint()
</td>
<td style="text-align:left;">
Provides confidence intervals for teh model parameters (95% by default)
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
fitted()
</td>
<td style="text-align:left;">
Lists the predicted values in a fitted model
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
residuals()
</td>
<td style="text-align:left;">
Lists the residual values in a fitted model
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
anova()
</td>
<td style="text-align:left;">
Generates an ANOVA table for a fitted model, or an ANOVA table comparing two or more fitted models
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
vcov()
</td>
<td style="text-align:left;">
Lists the covariance matrix for model parameters
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
AIC()
</td>
<td style="text-align:left;">
Prints Akaike’s Information Criterion
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
plot()
</td>
<td style="text-align:left;">
Generates diagnostic plots for evaluating the fit of a model
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
predict()
</td>
<td style="text-align:left;">
Uses a fitted model to predict response values for a new dataset
</td>
</tr>
</tbody>
</table>
<table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-128">Table 5.3: </span>Symbols Commonly Use in R Formulas
</caption>
<thead>
<tr>
<th style="text-align:left;">
Symbol
</th>
<th style="text-align:left;">
Usage
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;border-right:1px solid;">
~
</td>
<td style="text-align:left;width: 40em; ">
Separates response variables on the left from the explanatory varialbes on the right. For example, a prediction of y from x, z, and w would be coded y ~ x + z + w.
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
<ul>
<li></td>
<td style="text-align:left;width: 40em; ">
Separates predictor variables.
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
:
</td>
<td style="text-align:left;width: 40em; ">
Denotes an interaction between predictor variables. A prediction of y from x, z, and the interaction between x and z would be coded y ~ x + z + x:z.
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
<ul>
<li></td>
<td style="text-align:left;width: 40em; ">
A shortcut for denoting all possibel interactions. The code y ~ x<em>z</em>w expands to y ~ x + z + w + x:z + x:w + z:w + x:z:w.
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
^
</td>
<td style="text-align:left;width: 40em; ">
Denotes interactions up to a specified degree. The code y ~ (x + z + w)^2 expands to y ~ x + z + w + x:z + x:w + z:w.
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
.
</td>
<td style="text-align:left;width: 40em; ">
A placeholder for all other variables in the data frame except the dependent variable. For example, if a data frame contained the varibels x, y, z, and w, then the code y ~ . would expands to y ~ x + z + w.
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
<ul>
<li></td>
<td style="text-align:left;width: 40em; ">
A minus sign removes a variable from teh equation. For example, y ~ (x + z + w)^2 - x:w expands to y ~ x + z + w + x:z + z:w.
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
-1
</td>
<td style="text-align:left;width: 40em; ">
Supresses the intercept. For example, the formula y ~ x -1 fits a regression of y on x, and forces the line through the origin at x=0.
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
I()
</td>
<td style="text-align:left;width: 40em; ">
Elements within the parentheses are interpreted arithmetically. For example, y ~ x + (z + w)^2 would expand to y ~ x + z + w + z:w. In contrast, y ~ x + I(z + w)^2 would expand to y ~ x + h, where h is a new variable created by squaring the sum of z and w.
</td>
</tr>
<tr>
<td style="text-align:left;border-right:1px solid;">
function
</td>
<td style="text-align:left;width: 40em; ">
Mathematical functions cna be used in formulas. For example, log(y) ~ x + z + w would predict log(y) from x, z, and w
</td>
</tr>
</tbody>
</table></li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="interactions-in-regression" class="section level2" number="5.11">
<h2><span class="header-section-number">5.11</span> Interactions in Regression</h2>
<p><strong>Interactions Among Continuous Predictors</strong>: Interactions represent an interplay among predictors that produces an effect on the outcome of Y that is different from the sum of the effects of the individual predictors.</p>
<ul>
<li><p>Consider how ability and motivation impact achievement in graduate school. The combined impact of ability and motivation on achievement equals the sum of their separate parts. Additive effects are indicated by separate independent variables in the regression equation: <span class="math inline">\({\hat Y}=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}\)</span> . Additive means that the regression of the criterion on one predictor is constant over all values of the other predictor. The whole equals the sum of their separate parts.</p></li>
<li><p>What if the impact of ability and motivation as a whole is greater than the sum of their separate parts? When two predictors interact, the regression of Y on one of the predictors <em>depends on</em> or is <em>conditional on</em> the value of the other predictor.</p></li>
<li><p>One alternative is that ability and motivation may interact such that graduate students with both high ability and high motivation achieve much more in graduate school than would be expected from the simple sum of the separate effects of ability and motivation.</p></li>
<li><p>Another alternative is that ability and motivation compensate for one another in that students with high ability have less motivation to achieve and students with low ability have more motivation to achieve.</p></li>
<li><p>We want to consider the effects of ability, motivation, and the interaction between ability and motivation on achievement. The interaction between <span class="math inline">\({{X}_{1}}\)</span> and <span class="math inline">\({{X}_{2}}\)</span> can be tested in multiple regression to determine the amount of variance in the criterion variable it accounts for over and above any additive combination of their separate effects: <span class="math inline">\({\hat Y}=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+{{b}_{3}}{{X}_{1}}{{X}_{2}}\)</span>. The interaction term is simply the product of <span class="math inline">\({{X}_{1}}\)</span> and <span class="math inline">\({{X}_{2}}\)</span>. a is the intercept (the predicted value of <span class="math inline">\(Y\)</span> when <span class="math inline">\({{X}_{1}}\)</span> and <span class="math inline">\({{X}_{2}}\)</span> are zero); <span class="math inline">\({{b}_{1}}\)</span> represent the unit change in <span class="math inline">\(Y\)</span> with a 1-unit increase in <span class="math inline">\({{X}_{1}}\)</span>, holding other variable constant; <span class="math inline">\({{b}_{2}}\)</span> represents the unit change in <span class="math inline">\(Y\)</span> with a 1-unit increase in <span class="math inline">\({{X}_{2}}\)</span>, holding other variables constant; <span class="math inline">\({{b}_{3}}\)</span> represents the interaction between <span class="math inline">\({{X}_{1}}\)</span> and <span class="math inline">\({{X}_{2}}\)</span>, holding other variables constant.</p></li>
</ul>
<p>To Create an Interaction Term</p>
<ol style="list-style-type: decimal">
<li><p>Center the predictor variables (always center the continuous predictor variables when an interaction term is to be included in the regression equation, unless a predictor variable has a meaningful zero point). Centering simply means subtracting the mean value of a predictor variable from each value of that predictor variable ( <span class="math inline">\({{X}_{1}}-{{\bar{X}}_{1}}\)</span>). .red[Do not center the dependent variable.]</p></li>
<li><p>Multiply the centered variables <span class="math inline">\({{X}_{1}}\)</span> and <span class="math inline">\({{X}_{2}}\)</span>, creating an interaction term for <span class="math inline">\({{X}_{1}}\)</span> and <span class="math inline">\({{X}_{2}}\)</span> to be included in the multiple regression equation.</p></li>
</ol>
<p>–</p>
<ul>
<li><p>Centering variables allows a meaningful interpretation of regression coefficents. When you center variables, those who are at the mean of a predictor variable get a value of zero on the predictor variable. For simple effects (e.g., <span class="math inline">\({{X}_{1}}\)</span> alone), you interpret partial regression coefficients as the change in <span class="math inline">\(Y\)</span> with a 1-unit increase in the predictor variable for people at the mean of the <span class="math inline">\({{X}_{2}}\)</span> predictor variable: <span class="math inline">\({Y}&#39;=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+{{b}_{3}}{{X}_{1}}{{X}_{2}}\)</span>.</p>
<ul>
<li>Centering does not change the overall fitting of the model. Therefore, the <em>F</em>, and <span class="math inline">\(R^2\)</span> values stay the same.</li>
</ul></li>
</ul>
<div id="an-example-1" class="section level3" number="5.11.1">
<h3><span class="header-section-number">5.11.1</span> An example</h3>
<p>Regress Physical Endurance (minutes on treadmill test) on Age ( <span class="math inline">\(X_1\)</span>) and Number of Years of Vigorous Physical Exercise ( <span class="math inline">\(X_2\)</span>).</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="multiple-regression-analysis.html#cb281-1" aria-hidden="true" tabindex="-1"></a>endurance <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="st">&quot;data/endurance.sav&quot;</span>)</span>
<span id="cb281-2"><a href="multiple-regression-analysis.html#cb281-2" aria-hidden="true" tabindex="-1"></a><span class="co"># str(endurance)</span></span>
<span id="cb281-3"><a href="multiple-regression-analysis.html#cb281-3" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(endurance)</span>
<span id="cb281-4"><a href="multiple-regression-analysis.html#cb281-4" aria-hidden="true" tabindex="-1"></a>newage <span class="ot">&lt;-</span> <span class="fu">scale</span>(age, <span class="at">scale=</span><span class="cn">FALSE</span>) <span class="co">#&lt;&lt; </span></span>
<span id="cb281-5"><a href="multiple-regression-analysis.html#cb281-5" aria-hidden="true" tabindex="-1"></a>newyears <span class="ot">&lt;-</span> <span class="fu">scale</span>(years, <span class="at">scale=</span><span class="cn">FALSE</span>)</span>
<span id="cb281-6"><a href="multiple-regression-analysis.html#cb281-6" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(endurance)</span>
<span id="cb281-7"><a href="multiple-regression-analysis.html#cb281-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb281-8"><a href="multiple-regression-analysis.html#cb281-8" aria-hidden="true" tabindex="-1"></a>mymodel1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(endurance <span class="sc">~</span> newage <span class="sc">+</span> newyears <span class="sc">+</span> newage<span class="sc">:</span>newyears, <span class="at">data =</span> endurance) </span>
<span id="cb281-9"><a href="multiple-regression-analysis.html#cb281-9" aria-hidden="true" tabindex="-1"></a>mymodel2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(endurance <span class="sc">~</span> newage<span class="sc">*</span>newyears, <span class="at">data =</span> endurance) <span class="co"># alternative formula</span></span>
<span id="cb281-10"><a href="multiple-regression-analysis.html#cb281-10" aria-hidden="true" tabindex="-1"></a><span class="co"># mymodel2 &lt;- lm(endurance ~ I(age-49.18)*I(years-10.67), data = endurance) # use the `I()` symbol</span></span>
<span id="cb281-11"><a href="multiple-regression-analysis.html#cb281-11" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mymodel1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = endurance ~ newage + newyears + newage:newyears, 
##     data = endurance)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -21.165  -6.939   0.269   6.300  21.299 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     25.88872    0.64662  40.037  &lt; 2e-16 ***
## newage          -0.26169    0.06406  -4.085 6.01e-05 ***
## newyears         0.97272    0.13653   7.124 1.20e-11 ***
## newage:newyears  0.04724    0.01359   3.476 0.000604 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.7 on 241 degrees of freedom
## Multiple R-squared:  0.2061, Adjusted R-squared:  0.1962 
## F-statistic: 20.86 on 3 and 241 DF,  p-value: 4.764e-12</code></pre>
<ul>
<li>The overall regression of Endurance on Age, Years, and Age*Years:</li>
</ul>
<p><span class="math inline">\({\hat Y}=25.89-.262newage+.973newyears+.047newage*newyears\)</span>.</p>
<p>All of the partial regression coefficients are significantly different from zero, controlling for the other variables in the model. For every 1 year increase in age, there is an estimated loss of endurance of .26 minutes for people at the mean level of years of exercise in the sample. For every 1 year increase in years of vigorous exercise, there is an estimated increase in endurance of .97 minutes for people at the mean level of age in the sample. The interaction indicates that the decline in endurance with age depends on a history of rigorous exercise.</p>
<p><strong>To Interpret Interactions</strong>: Write the simple regression equation for the regression of the criterion on one predictor at a specific value of the other predictor variable, then plot these values.</p>
<p>For example, write the simple regression equations for the regression of Physical Endurance on Age at three levels of Years of Rigorous Exercise. The three levels are typically 1) at the mean of the other predictor (when centered Years = 0); 2) at 1 standard deviation above the mean (when centered Years = 4.78); and 3) at 1 standard deviation below the mean (when centered Years = -4.78).</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="multiple-regression-analysis.html#cb283-1" aria-hidden="true" tabindex="-1"></a>psych<span class="sc">::</span><span class="fu">describe</span>(endurance)</span></code></pre></div>
<pre><code>##           vars   n   mean    sd median trimmed   mad min max range skew kurtosis   se
## case         1 245 124.99 72.48    124  124.86 91.92   1 250   249 0.02    -1.21 4.63
## age          2 245  49.18 10.11     48   49.11 10.38  20  82    62 0.15    -0.08 0.65
## years        3 245  10.67  4.78     11   10.56  4.45   0  26    26 0.27     0.23 0.31
## endurance    4 245  26.53 10.82     27   26.39 10.38   0  55    55 0.11    -0.30 0.69</code></pre>
<ul>
<li><p>Multiple Regression Equation: <span class="math inline">\({\hat Y}=25.89-.262newage+.973newyears+.047newage*newyears\)</span></p></li>
<li><p>Rearrange the equation: <span class="math inline">\({\hat Y}=25.89+.973newyears+(-.262+.047newyears)*newage\)</span></p></li>
<li><p>For Low Years ( <span class="math inline">\({{X}_{2}} = {newyear} = -4.78\)</span>):</p>
<p><span class="math inline">\({\hat Y}=25.89+.973(-4.78)+(-.262+.047(-4.78))*newage\)</span>
<span class="math inline">\({\hat Y}=25.89-4.65-.487*newage =21.24-.487*newage\)</span></p></li>
<li><p>For Mean Years ( <span class="math inline">\({{X}_{2}} = {newyear} = 0)\)</span>):<br />
<span class="math inline">\({\hat Y}=25.89+.973(0)+(-.262+.047(0))*newage = 25.89-.262*newage\)</span></p></li>
<li><p>For High Years ( <span class="math inline">\({{X}_{2}} = {newyear} = 4.78\)</span>):<br />
<span class="math inline">\({\hat Y}=25.89+.973(4.78)+(-.262+.047(4.78))*newage\)</span>
<span class="math inline">\({\hat Y}=25.89+4.65-.037*newage = 30.54-.037*newage\)</span></p></li>
</ul>
<p>Low Years Exercise: <span class="math inline">\({\hat Y}=21.24-.487*newage\)</span></p>
<p>Mean Years Exercise: <span class="math inline">\({\hat Y}=25.89-.262*newage\)</span></p>
<p>High Years Exercise: <span class="math inline">\({\hat Y}=30.54-.037*newage\)</span></p>
<p>Solve for the predicted Y values at .red[low] years exercise for each age level (low age = -10.11; mean age = 0; high age = 10.11). Notice that the low to high levels represent 1 standard deviation below the mean (-10.11), the mean (0), and 1 standard deviation above the mean (10.11):</p>
<p>Low Years/Low Age: <span class="math inline">\({\hat Y} =21.24-.487(-10.11)=26.16\)</span></p>
<p>Low Years/Mean Age: <span class="math inline">\({\hat Y}=21.24-.487(0)=21.24\)</span></p>
<p>Low Years/High Age: <span class="math inline">\({\hat Y}=21.24-.487(10.11)=16.32\)</span></p>
<ul>
<li>Solve for the predicted Y values at .red[mean] level of years exercise for each age level:</li>
</ul>
<p>Mean Years/Low Age: <span class="math inline">\({\hat Y}=25.89-.262(-10.11)=28.54\)</span></p>
<p>Mean Years/Mean Age: <span class="math inline">\({\hat Y}=25.89-.262(0)=25.89\)</span></p>
<p>Mean Years/High Age: <span class="math inline">\({\hat Y}=25.89-.262(10.11)=23.24\)</span></p>
<ul>
<li>Solve for the predicted Y values at .red[high] level of years exercise for each age level:</li>
</ul>
<p>High Years/Low Age: <span class="math inline">\({\hat Y}=30.54-.037(-10.11)=30.91\)</span></p>
<p>High Years/Mean Age: <span class="math inline">\({\hat Y}=30.54-.037(0)=30.54\)</span></p>
<p>High Years/High Age: <span class="math inline">\({\hat Y}=30.54-.037(10.11)=30.17\)</span></p>
<table class=" lightable-classic" style="font-family: &quot;Arial Narrow&quot;, &quot;Source Sans Pro&quot;, sans-serif; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="empty-cells: hide;" colspan="2">
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Age
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Low
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
High
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Low
</td>
<td style="text-align:right;">
26.16
</td>
<td style="text-align:right;">
21.24
</td>
<td style="text-align:right;">
16.32
</td>
</tr>
<tr>
<td style="text-align:left;">
Excercise
</td>
<td style="text-align:left;">
Mean
</td>
<td style="text-align:right;">
28.54
</td>
<td style="text-align:right;">
25.89
</td>
<td style="text-align:right;">
23.24
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
High
</td>
<td style="text-align:right;">
30.91
</td>
<td style="text-align:right;">
30.54
</td>
<td style="text-align:right;">
30.17
</td>
</tr>
</tbody>
</table>
<p><img src="book_files/figure-html/unnamed-chunk-131-1.png" width="672" /></p>
<p>This indicates that the decline in physical endurance across the life span is less dramatic for those with high number of years of rigorous exercise, whereas there appears to be a dramatic decline in physical endurance across the life span for those with low or average number of years of rigorous exercise.</p>
<p>Does including the interaction improve prediction?</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="multiple-regression-analysis.html#cb285-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(endurance)</span>
<span id="cb285-2"><a href="multiple-regression-analysis.html#cb285-2" aria-hidden="true" tabindex="-1"></a>newage <span class="ot">&lt;-</span> <span class="fu">scale</span>(age, <span class="at">scale=</span><span class="cn">FALSE</span>)  </span>
<span id="cb285-3"><a href="multiple-regression-analysis.html#cb285-3" aria-hidden="true" tabindex="-1"></a>newyears <span class="ot">&lt;-</span> <span class="fu">scale</span>(years, <span class="at">scale=</span><span class="cn">FALSE</span>)</span>
<span id="cb285-4"><a href="multiple-regression-analysis.html#cb285-4" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(endurance)</span>
<span id="cb285-5"><a href="multiple-regression-analysis.html#cb285-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb285-6"><a href="multiple-regression-analysis.html#cb285-6" aria-hidden="true" tabindex="-1"></a>mymodel1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(endurance <span class="sc">~</span> newage <span class="sc">+</span> newyears <span class="sc">+</span> newage<span class="sc">:</span>newyears, <span class="at">data =</span> endurance) </span>
<span id="cb285-7"><a href="multiple-regression-analysis.html#cb285-7" aria-hidden="true" tabindex="-1"></a>mymodel3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(endurance <span class="sc">~</span> newage <span class="sc">+</span> newyears, <span class="at">data =</span> endurance) </span>
<span id="cb285-8"><a href="multiple-regression-analysis.html#cb285-8" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(mymodel3, mymodel1) <span class="co">#using Type I Sum of Squares </span></span>
<span id="cb285-9"><a href="multiple-regression-analysis.html#cb285-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Analysis of Variance Table</span></span>
<span id="cb285-10"><a href="multiple-regression-analysis.html#cb285-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb285-11"><a href="multiple-regression-analysis.html#cb285-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Model 1: endurance ~ newage + newyears</span></span>
<span id="cb285-12"><a href="multiple-regression-analysis.html#cb285-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Model 2: endurance ~ newage + newyears + newage:newyears</span></span>
<span id="cb285-13"><a href="multiple-regression-analysis.html#cb285-13" aria-hidden="true" tabindex="-1"></a><span class="do">##   Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    </span></span>
<span id="cb285-14"><a href="multiple-regression-analysis.html#cb285-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 1    242 23810                                 </span></span>
<span id="cb285-15"><a href="multiple-regression-analysis.html#cb285-15" aria-hidden="true" tabindex="-1"></a><span class="do">## 2    241 22674  1    1136.5 12.08 0.0006042 ***</span></span>
<span id="cb285-16"><a href="multiple-regression-analysis.html#cb285-16" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb285-17"><a href="multiple-regression-analysis.html#cb285-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span></code></pre></div>
<p>Use the <code>interact_plot()</code> function in the <code>interactions</code> package.</p>
<ul>
<li>By default, all continuous variables <em>not involved in the interaction</em> are mean-centered. The <code>centered</code> argument can be used to overwrite the default. However, the response variable, pred, modx, and mod2 variables are never centered.</li>
</ul>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="multiple-regression-analysis.html#cb286-1" aria-hidden="true" tabindex="-1"></a>mymodel <span class="ot">&lt;-</span> <span class="fu">lm</span>(endurance <span class="sc">~</span> age<span class="sc">*</span>years, <span class="at">data =</span> endurance)  </span>
<span id="cb286-2"><a href="multiple-regression-analysis.html#cb286-2" aria-hidden="true" tabindex="-1"></a>interactions<span class="sc">::</span><span class="fu">interact_plot</span>(mymodel, <span class="at">pred =</span> age, <span class="at">modx =</span> years, <span class="at">plot.points =</span> <span class="cn">TRUE</span>, <span class="at">point.shape=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="book_files/figure-html/unnamed-chunk-133-1.png" width="672" /></p>
</div>
</div>
<div id="categorical-independent-variables" class="section level2" number="5.12">
<h2><span class="header-section-number">5.12</span> Categorical Independent Variables</h2>
<div id="binary-independent-variables" class="section level3" number="5.12.1">
<h3><span class="header-section-number">5.12.1</span> Binary independent variables</h3>
<ul>
<li><p>Examples of nominal or categorical variables include: religion, treatment groups in experiments, region of country, ethnic group, occupation, diagnosis, or marital status.</p></li>
<li><p>One Independent Variable with Two Categories: Regression of Salary on Gender (Females coded as 1 and males coded as 0).</p></li>
</ul>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="multiple-regression-analysis.html#cb287-1" aria-hidden="true" tabindex="-1"></a>profs <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="st">&quot;data/profs.sav&quot;</span>)</span></code></pre></div>
<div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:200px; overflow-x: scroll; width:500px; ">
<table class=" lightable-paper" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
case
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
dept
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
time
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
pubs
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
cits
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
salary
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
gender
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
34
</td>
<td style="text-align:right;">
61863
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
37
</td>
<td style="text-align:right;">
47034
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:right;">
48
</td>
<td style="text-align:right;">
66432
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
48
</td>
<td style="text-align:right;">
56
</td>
<td style="text-align:right;">
61100
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
47454
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
59677
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
37
</td>
<td style="text-align:right;">
61
</td>
<td style="text-align:right;">
61458
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
54528
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
69
</td>
<td style="text-align:right;">
56600
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
62895
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
23
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
53740
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
69
</td>
<td style="text-align:right;">
90
</td>
<td style="text-align:right;">
75822
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
60
</td>
<td style="text-align:right;">
56596
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
27
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:right;">
62091
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:right;">
56
</td>
<td style="text-align:right;">
74199
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
50729
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
23
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
70011
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
34
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
39652
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
69
</td>
<td style="text-align:right;">
68987
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
36
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
69
</td>
<td style="text-align:right;">
55579
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
37
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:right;">
54671
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
38
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
57704
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
42
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
54
</td>
<td style="text-align:right;">
60009
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
43
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
58632
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
38340
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
39
</td>
<td style="text-align:right;">
69
</td>
<td style="text-align:right;">
71219
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
48
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
55
</td>
<td style="text-align:right;">
83503
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
51
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
42
</td>
<td style="text-align:right;">
53650
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
55
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
83
</td>
<td style="text-align:right;">
74343
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
56
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
49
</td>
<td style="text-align:right;">
57710
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
57
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
52676
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
28
</td>
<td style="text-align:right;">
58582
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
51876
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
26
</td>
<td style="text-align:right;">
54511
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
53425
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
41
</td>
<td style="text-align:right;">
52926
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
41934
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
28
</td>
<td style="text-align:right;">
49832
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
47047
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
39115
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
36
</td>
<td style="text-align:right;">
60327
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
52542
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
50455
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
51647
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
26
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
55682
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
28
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
41
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
42162
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
52646
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
37939
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
39
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
44045
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
51122
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
41
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
26
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
47082
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
46
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
53712
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
43
</td>
<td style="text-align:right;">
54782
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
49
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
47212
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
28
</td>
<td style="text-align:right;">
52840
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
52
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
50931
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
53
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
66784
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
54
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:right;">
49751
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
58
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
36
</td>
<td style="text-align:right;">
41195
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
59
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
34
</td>
<td style="text-align:right;">
45662
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
60
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
70
</td>
<td style="text-align:right;">
47606
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
61
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:right;">
44301
</td>
<td style="text-align:right;">
1
</td>
</tr>
</tbody>
</table>
</div>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="multiple-regression-analysis.html#cb288-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor.test</span>(profs<span class="sc">$</span>gender, profs<span class="sc">$</span>salary) <span class="co"># point-biserial correlation</span></span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  profs$gender and profs$salary
## t = -4.6439, df = 60, p-value = 1.912e-05
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.6770144 -0.3033909
## sample estimates:
##        cor 
## -0.5141948</code></pre>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="multiple-regression-analysis.html#cb290-1" aria-hidden="true" tabindex="-1"></a>mymodel <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> <span class="fu">as.factor</span>(gender), <span class="at">data =</span> profs) <span class="co">#&lt;&lt; 0 for male, 1 for female on gender</span></span>
<span id="cb290-2"><a href="multiple-regression-analysis.html#cb290-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mymodel)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ as.factor(gender), data = profs)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -21268.9  -5045.2     98.5   3270.3  23894.1 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)           59609       1484  40.173  &lt; 2e-16 ***
## as.factor(gender)1    -9906       2133  -4.644 1.91e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8394 on 60 degrees of freedom
## Multiple R-squared:  0.2644, Adjusted R-squared:  0.2521 
## F-statistic: 21.57 on 1 and 60 DF,  p-value: 1.912e-05</code></pre>
<ul>
<li><p>The regression coefficient associated with gender is statistically significantly different from zero (Not surprising because we already know the correlation coefficient is significantly different from zero). Because gender in this exampel only has two categories, the regression coefficient represents the difference in mean salaries for males and females. Thus, the t-test in this situation tests whether the mean salary for males is significantly different from the mean salary for females. Because females were coded as 1 and males coded as 0 (dummy coding), and the regression coefficient is negative, we know that females have significantly lower salaries than males. If the regression coefficient was positive, it would indicate that females have significantly higher salaries than males.</p></li>
<li><p>When a group is coded as zero, it is the reference/comparison group. The males are the reference group in this situation. Notice the value of the constant (Y-intercept). Because males are the reference group, the constant is the mean salary for males.</p></li>
</ul>
<p>Regression Equation: <span class="math inline">\(\hat Y = a + b(gender)\)</span>. What does a represent? What does b represent?</p>
<p>Regression Equation: <span class="math inline">\({\hat Y} = 59608.94 - 9905.90*gender\)</span></p>
<p>For males (gender = 0): <span class="math inline">\({\hat Y} = 59608.94 - 9905.90*(0) = 59608.94\)</span></p>
<p>For females (gender = 1): <span class="math inline">\({\hat Y} = 59608.94 - 9905.90*(1) = 49703.04\)</span></p>
<p><strong>To sum this up</strong>: The Point-Biserial correlation was computed to examine the relationship between gender and salary level, which indicated that a moderate and significant relationship exists, <span class="math inline">\(r_{pb}(60) = -.514\)</span> , <em>p</em> &lt; .001. As indicated by the correlational analysis, gender was a significant predictor of professors’ salary, <span class="math inline">\(t(60) = -4.64\)</span>, <em>p</em> &lt; .001, accounting for approximately 25% of the variance in salary ( <span class="math inline">\(R_{adj}^2 = .252\)</span>). Female professors make significantly less money than male professors. On average, females make about <code>$</code>9906 less than males (95% CI: -14173, -5639, <span class="math inline">\(\beta = -.514\)</span>).</p>
<ul>
<li>Alternatively, the regression model can be specified in R as below. .</li>
</ul>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="multiple-regression-analysis.html#cb292-1" aria-hidden="true" tabindex="-1"></a>mymodel2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> gender, <span class="at">data =</span> profs)</span>
<span id="cb292-2"><a href="multiple-regression-analysis.html#cb292-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mymodel2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ gender, data = profs)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -21268.9  -5045.2     98.5   3270.3  23894.1 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    59609       1484  40.173  &lt; 2e-16 ***
## gender         -9906       2133  -4.644 1.91e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8394 on 60 degrees of freedom
## Multiple R-squared:  0.2644, Adjusted R-squared:  0.2521 
## F-statistic: 21.57 on 1 and 60 DF,  p-value: 1.912e-05</code></pre>
</div>
<div id="categorical-independent-variables-with-more-than-two-categories" class="section level3" number="5.12.2">
<h3><span class="header-section-number">5.12.2</span> Categorical independent variables with more than two categories</h3>
<p>What if a variable has more than two categories?</p>
<ul>
<li><p>When a variable has more than two categories, it must be coded to provide meaningful results in the regression analysis. To code a variable with more than two categories or groups, <em>g</em> – 1 variables must be created and used in the regression equation (where <em>g</em> = the number of categories or groups).</p></li>
<li><p>For example, if we were interested in the effects of different religious backgrounds (Catholic, Protestant, Jewish, Other) on attitudes toward abortion, we would need to create 3 code variables (<em>g</em> – 1 = 4 – 1 = 3).</p></li>
</ul>
</div>
<div id="an-example-2" class="section level3" number="5.12.3">
<h3><span class="header-section-number">5.12.3</span> An example</h3>
<p>Effects of religion on attitude toward abortion</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="multiple-regression-analysis.html#cb294-1" aria-hidden="true" tabindex="-1"></a>religion <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="st">&quot;data/religion.sav&quot;</span>)</span></code></pre></div>
<div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:200px; overflow-x: scroll; width:500px; ">
<table class=" lightable-paper" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
case
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
religion
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
ata
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
C
</td>
<td style="text-align:right;">
61
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
O
</td>
<td style="text-align:right;">
78
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
47
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
C
</td>
<td style="text-align:right;">
65
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
C
</td>
<td style="text-align:right;">
45
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
O
</td>
<td style="text-align:right;">
106
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
120
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
C
</td>
<td style="text-align:right;">
49
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:left;">
O
</td>
<td style="text-align:right;">
45
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
O
</td>
<td style="text-align:right;">
62
</td>
</tr>
<tr>
<td style="text-align:right;">
11
</td>
<td style="text-align:left;">
C
</td>
<td style="text-align:right;">
79
</td>
</tr>
<tr>
<td style="text-align:right;">
12
</td>
<td style="text-align:left;">
O
</td>
<td style="text-align:right;">
54
</td>
</tr>
<tr>
<td style="text-align:right;">
13
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
140
</td>
</tr>
<tr>
<td style="text-align:right;">
14
</td>
<td style="text-align:left;">
C
</td>
<td style="text-align:right;">
52
</td>
</tr>
<tr>
<td style="text-align:right;">
15
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
88
</td>
</tr>
<tr>
<td style="text-align:right;">
16
</td>
<td style="text-align:left;">
C
</td>
<td style="text-align:right;">
70
</td>
</tr>
<tr>
<td style="text-align:right;">
17
</td>
<td style="text-align:left;">
C
</td>
<td style="text-align:right;">
56
</td>
</tr>
<tr>
<td style="text-align:right;">
18
</td>
<td style="text-align:left;">
J
</td>
<td style="text-align:right;">
124
</td>
</tr>
<tr>
<td style="text-align:right;">
19
</td>
<td style="text-align:left;">
O
</td>
<td style="text-align:right;">
98
</td>
</tr>
<tr>
<td style="text-align:right;">
20
</td>
<td style="text-align:left;">
C
</td>
<td style="text-align:right;">
69
</td>
</tr>
<tr>
<td style="text-align:right;">
21
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
56
</td>
</tr>
<tr>
<td style="text-align:right;">
22
</td>
<td style="text-align:left;">
J
</td>
<td style="text-align:right;">
135
</td>
</tr>
<tr>
<td style="text-align:right;">
23
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
64
</td>
</tr>
<tr>
<td style="text-align:right;">
24
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
130
</td>
</tr>
<tr>
<td style="text-align:right;">
25
</td>
<td style="text-align:left;">
J
</td>
<td style="text-align:right;">
74
</td>
</tr>
<tr>
<td style="text-align:right;">
26
</td>
<td style="text-align:left;">
O
</td>
<td style="text-align:right;">
58
</td>
</tr>
<tr>
<td style="text-align:right;">
27
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
116
</td>
</tr>
<tr>
<td style="text-align:right;">
28
</td>
<td style="text-align:left;">
O
</td>
<td style="text-align:right;">
60
</td>
</tr>
<tr>
<td style="text-align:right;">
29
</td>
<td style="text-align:left;">
J
</td>
<td style="text-align:right;">
84
</td>
</tr>
<tr>
<td style="text-align:right;">
30
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
68
</td>
</tr>
<tr>
<td style="text-align:right;">
31
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
90
</td>
</tr>
<tr>
<td style="text-align:right;">
32
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
112
</td>
</tr>
<tr>
<td style="text-align:right;">
33
</td>
<td style="text-align:left;">
J
</td>
<td style="text-align:right;">
94
</td>
</tr>
<tr>
<td style="text-align:right;">
34
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
80
</td>
</tr>
<tr>
<td style="text-align:right;">
35
</td>
<td style="text-align:left;">
J
</td>
<td style="text-align:right;">
110
</td>
</tr>
<tr>
<td style="text-align:right;">
36
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
102
</td>
</tr>
</tbody>
</table>
</div>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="multiple-regression-analysis.html#cb295-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(religion<span class="sc">$</span>religion)</span></code></pre></div>
<pre><code>## 
##  C  J  O  P 
##  9  6  8 13</code></pre>
<div id="dummy-coding" class="section level4" number="5.12.3.1">
<h4><span class="header-section-number">5.12.3.1</span> <strong>Dummy Coding</strong></h4>
<p>One group is designated as the reference group and is assigned a value of 0 for every code variable. The reference group should serve as a useful comparison (e.g., a control group; the group expected to score the highest or lowest on Y; a standard treatment). The reference group should be well defined and not a “catch all” category (e.g., Other). The reference group should not have a very small sample size compared to the other groups.</p>
<table class=" lightable-classic" style="font-family: &quot;Arial Narrow&quot;, &quot;Source Sans Pro&quot;, sans-serif; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption>
<span id="tab:unnamed-chunk-142">Table 5.4: </span>Protestant as Reference Group:
</caption>
<thead>
<tr>
<th style="empty-cells: hide;" colspan="1">
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Code Variable
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
Religion
</th>
<th style="text-align:right;">
Catholic
</th>
<th style="text-align:right;">
Jewish
</th>
<th style="text-align:right;">
Other
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Protestant
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Catholic
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Jewish
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Other
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<sup></sup> Catholic represents Catholic versus non-Catholic; Jewish represnts Jewish versus non-Jewish; and Other represtns other versus non-other (Catholic, Protestant, and Jewish combined)
</td>
</tr>
</tfoot>
</table>
<p><span class="math inline">\(\hat Y = a + {b_1}Catholic + {b_2}Jewish + {b_3}Other\)</span></p>
<p>In R, dummy coding is called “treatment constrasts.” It’s the default coding with the first group “C” as the reference group.</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="multiple-regression-analysis.html#cb297-1" aria-hidden="true" tabindex="-1"></a>religion<span class="sc">$</span>group <span class="ot">&lt;-</span> <span class="fu">factor</span>(religion<span class="sc">$</span>religion)</span>
<span id="cb297-2"><a href="multiple-regression-analysis.html#cb297-2" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(religion<span class="sc">$</span>group) <span class="co"># Catholic is the reference group</span></span>
<span id="cb297-3"><a href="multiple-regression-analysis.html#cb297-3" aria-hidden="true" tabindex="-1"></a><span class="do">##   J O P</span></span>
<span id="cb297-4"><a href="multiple-regression-analysis.html#cb297-4" aria-hidden="true" tabindex="-1"></a><span class="do">## C 0 0 0</span></span>
<span id="cb297-5"><a href="multiple-regression-analysis.html#cb297-5" aria-hidden="true" tabindex="-1"></a><span class="do">## J 1 0 0</span></span>
<span id="cb297-6"><a href="multiple-regression-analysis.html#cb297-6" aria-hidden="true" tabindex="-1"></a><span class="do">## O 0 1 0</span></span>
<span id="cb297-7"><a href="multiple-regression-analysis.html#cb297-7" aria-hidden="true" tabindex="-1"></a><span class="do">## P 0 0 1</span></span>
<span id="cb297-8"><a href="multiple-regression-analysis.html#cb297-8" aria-hidden="true" tabindex="-1"></a>mymodel <span class="ot">&lt;-</span> <span class="fu">lm</span>(ata <span class="sc">~</span> group, <span class="at">data=</span>religion) </span>
<span id="cb297-9"><a href="multiple-regression-analysis.html#cb297-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mymodel)<span class="sc">$</span>coefficients</span>
<span id="cb297-10"><a href="multiple-regression-analysis.html#cb297-10" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Std. Error  t value     Pr(&gt;|t|)</span></span>
<span id="cb297-11"><a href="multiple-regression-analysis.html#cb297-11" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 60.666667   7.806057 7.771743 7.301349e-09</span></span>
<span id="cb297-12"><a href="multiple-regression-analysis.html#cb297-12" aria-hidden="true" tabindex="-1"></a><span class="do">## groupJ      42.833333  12.342460 3.470405 1.508602e-03</span></span>
<span id="cb297-13"><a href="multiple-regression-analysis.html#cb297-13" aria-hidden="true" tabindex="-1"></a><span class="do">## groupO       9.458333  11.379186 0.831196 4.120223e-01</span></span>
<span id="cb297-14"><a href="multiple-regression-analysis.html#cb297-14" aria-hidden="true" tabindex="-1"></a><span class="do">## groupP      32.641026  10.154800 3.214344 2.982232e-03</span></span>
<span id="cb297-15"><a href="multiple-regression-analysis.html#cb297-15" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mymodel)</span>
<span id="cb297-16"><a href="multiple-regression-analysis.html#cb297-16" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)      groupJ      groupO      groupP </span></span>
<span id="cb297-17"><a href="multiple-regression-analysis.html#cb297-17" aria-hidden="true" tabindex="-1"></a><span class="do">##   60.666667   42.833333    9.458333   32.641026</span></span></code></pre></div>
<p>Manually setting the contrasts</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="multiple-regression-analysis.html#cb298-1" aria-hidden="true" tabindex="-1"></a>Jewish <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb298-2"><a href="multiple-regression-analysis.html#cb298-2" aria-hidden="true" tabindex="-1"></a>Other <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb298-3"><a href="multiple-regression-analysis.html#cb298-3" aria-hidden="true" tabindex="-1"></a>Protestant <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb298-4"><a href="multiple-regression-analysis.html#cb298-4" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(religion<span class="sc">$</span>group) <span class="ot">&lt;-</span> <span class="fu">cbind</span>(Jewish, Other, Protestant) <span class="co">#add a `contrasts` attribute to `religion$group`</span></span>
<span id="cb298-5"><a href="multiple-regression-analysis.html#cb298-5" aria-hidden="true" tabindex="-1"></a>religion<span class="sc">$</span>group</span>
<span id="cb298-6"><a href="multiple-regression-analysis.html#cb298-6" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1] C O P C C O P C O O C O P C P C C J O C P J P P J O P O J P P P J P J P</span></span>
<span id="cb298-7"><a href="multiple-regression-analysis.html#cb298-7" aria-hidden="true" tabindex="-1"></a><span class="do">## attr(,&quot;contrasts&quot;)</span></span>
<span id="cb298-8"><a href="multiple-regression-analysis.html#cb298-8" aria-hidden="true" tabindex="-1"></a><span class="do">##   Jewish Other Protestant</span></span>
<span id="cb298-9"><a href="multiple-regression-analysis.html#cb298-9" aria-hidden="true" tabindex="-1"></a><span class="do">## C      0     0          0</span></span>
<span id="cb298-10"><a href="multiple-regression-analysis.html#cb298-10" aria-hidden="true" tabindex="-1"></a><span class="do">## J      1     0          0</span></span>
<span id="cb298-11"><a href="multiple-regression-analysis.html#cb298-11" aria-hidden="true" tabindex="-1"></a><span class="do">## O      0     1          0</span></span>
<span id="cb298-12"><a href="multiple-regression-analysis.html#cb298-12" aria-hidden="true" tabindex="-1"></a><span class="do">## P      0     0          1</span></span>
<span id="cb298-13"><a href="multiple-regression-analysis.html#cb298-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Levels: C J O P</span></span></code></pre></div>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="multiple-regression-analysis.html#cb299-1" aria-hidden="true" tabindex="-1"></a>mymodel2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(ata <span class="sc">~</span> group, <span class="at">data=</span>religion)</span>
<span id="cb299-2"><a href="multiple-regression-analysis.html#cb299-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mymodel2)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##                  Estimate Std. Error  t value     Pr(&gt;|t|)
## (Intercept)     60.666667   7.806057 7.771743 7.301349e-09
## groupJewish     42.833333  12.342460 3.470405 1.508602e-03
## groupOther       9.458333  11.379186 0.831196 4.120223e-01
## groupProtestant 32.641026  10.154800 3.214344 2.982232e-03</code></pre>
<p>Change the reference group for dummy coding.</p>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="multiple-regression-analysis.html#cb301-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(religion<span class="sc">$</span>group) <span class="ot">&lt;-</span><span class="fu">contr.treatment</span>(<span class="at">n=</span><span class="dv">4</span>, <span class="at">base=</span><span class="dv">4</span>) <span class="co"># &quot;religion&quot; has four groups: C, J, O, P</span></span>
<span id="cb301-2"><a href="multiple-regression-analysis.html#cb301-2" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(religion<span class="sc">$</span>group) <span class="co"># Protestant is the reference group</span></span>
<span id="cb301-3"><a href="multiple-regression-analysis.html#cb301-3" aria-hidden="true" tabindex="-1"></a><span class="do">##   1 2 3</span></span>
<span id="cb301-4"><a href="multiple-regression-analysis.html#cb301-4" aria-hidden="true" tabindex="-1"></a><span class="do">## C 1 0 0</span></span>
<span id="cb301-5"><a href="multiple-regression-analysis.html#cb301-5" aria-hidden="true" tabindex="-1"></a><span class="do">## J 0 1 0</span></span>
<span id="cb301-6"><a href="multiple-regression-analysis.html#cb301-6" aria-hidden="true" tabindex="-1"></a><span class="do">## O 0 0 1</span></span>
<span id="cb301-7"><a href="multiple-regression-analysis.html#cb301-7" aria-hidden="true" tabindex="-1"></a><span class="do">## P 0 0 0</span></span>
<span id="cb301-8"><a href="multiple-regression-analysis.html#cb301-8" aria-hidden="true" tabindex="-1"></a>mymodel2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(ata <span class="sc">~</span> group, <span class="at">data=</span>religion) </span>
<span id="cb301-9"><a href="multiple-regression-analysis.html#cb301-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mymodel2)<span class="sc">$</span>coefficients</span>
<span id="cb301-10"><a href="multiple-regression-analysis.html#cb301-10" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Std. Error    t value     Pr(&gt;|t|)</span></span>
<span id="cb301-11"><a href="multiple-regression-analysis.html#cb301-11" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  93.30769   6.495032 14.3660093 1.664683e-15</span></span>
<span id="cb301-12"><a href="multiple-regression-analysis.html#cb301-12" aria-hidden="true" tabindex="-1"></a><span class="do">## group1      -32.64103  10.154800 -3.2143444 2.982232e-03</span></span>
<span id="cb301-13"><a href="multiple-regression-analysis.html#cb301-13" aria-hidden="true" tabindex="-1"></a><span class="do">## group2       10.19231  11.557994  0.8818405 3.844393e-01</span></span>
<span id="cb301-14"><a href="multiple-regression-analysis.html#cb301-14" aria-hidden="true" tabindex="-1"></a><span class="do">## group3      -23.18269  10.523155 -2.2030173 3.491209e-02</span></span></code></pre></div>
<p>Change the reference group for dummy coding.</p>
<p>Manually setting the contrasts.</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="multiple-regression-analysis.html#cb302-1" aria-hidden="true" tabindex="-1"></a>Catholic <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb302-2"><a href="multiple-regression-analysis.html#cb302-2" aria-hidden="true" tabindex="-1"></a>Jewish <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb302-3"><a href="multiple-regression-analysis.html#cb302-3" aria-hidden="true" tabindex="-1"></a>Other <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb302-4"><a href="multiple-regression-analysis.html#cb302-4" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(religion<span class="sc">$</span>group) <span class="ot">&lt;-</span> <span class="fu">cbind</span>(Catholic, Jewish, Other) <span class="co">#&lt;&lt;</span></span>
<span id="cb302-5"><a href="multiple-regression-analysis.html#cb302-5" aria-hidden="true" tabindex="-1"></a>religion<span class="sc">$</span>group</span></code></pre></div>
<pre><code>##  [1] C O P C C O P C O O C O P C P C C J O C P J P P J O P O J P P P J P J P
## attr(,&quot;contrasts&quot;)
##   Catholic Jewish Other
## C        1      0     0
## J        0      1     0
## O        0      0     1
## P        0      0     0
## Levels: C J O P</code></pre>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="multiple-regression-analysis.html#cb304-1" aria-hidden="true" tabindex="-1"></a>mymodel3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(ata <span class="sc">~</span> group, <span class="at">data=</span>religion)</span>
<span id="cb304-2"><a href="multiple-regression-analysis.html#cb304-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mymodel3)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##                Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept)    93.30769   6.495032 14.3660093 1.664683e-15
## groupCatholic -32.64103  10.154800 -3.2143444 2.982232e-03
## groupJewish    10.19231  11.557994  0.8818405 3.844393e-01
## groupOther    -23.18269  10.523155 -2.2030173 3.491209e-02</code></pre>
<p>Regression Equation: <span class="math inline">\(\hat Y = 93.31 - 32.64Catholic + 10.19Jewish - 23.18Other\)</span></p>
<p>Protestant: <span class="math inline">\(\hat Y = 93.31 - 32.64(0) + 10.19(0) - 23.18(0) = 93.31\)</span></p>
<p>Catholic: <span class="math inline">\(\hat Y = 93.31 - 32.64(1) + 10.19(0) - 23.18(0) = 60.67\)</span></p>
<p>Jewish: <span class="math inline">\(\hat Y = 93.31 - 32.64(0) + 10.19(1) - 23.18(0) = 103.50\)</span></p>
<p>Other: <span class="math inline">\(\hat Y = 93.31 - 32.64(0) + 10.19(0) - 23.18(1) = 70.13\)</span></p>
<p><span class="math inline">\(\hat Y = a + {b_1}Catholic + {b_2}Jewish + {b_3}Other\)</span>, What do <span class="math inline">\(a\)</span>, <span class="math inline">\(b_1\)</span>, <span class="math inline">\(b_2\)</span>, <span class="math inline">\(b_3\)</span>, and <span class="math inline">\(b_4\)</span> represent?</p>
<p><strong>Sum this up</strong>: Multiple regression analysis was used to determine how well religion explains attitudes toward abortion. Religion was categorized as Protestant, Catholic, Jewish, or Other. Dummy-coding was used, creating three code variables (viz., Catholic, Jewish, and Other). Protestant was coded as the reference group. The analysis indicated that religion accounted for approximately 29% of the variance in attitudes toward abortion <span class="math inline">\((R_{adj}^{2}=.29)\)</span>, <em>F</em>(3, 32) = 5.87, <em>p</em> &lt; .01. There was a significant difference between Catholic and Protestant attitudes toward abortion, <em>t</em>(32) = -3.21, <em>p</em> &lt; .01. On average, Catholic attitudes toward abortion were about 33 points less favorable than Protestant attitudes toward abortion (95% CI: -53.33; -11.96). There was no significant difference between Jewish and Protestant attitudes toward abortion, <em>t</em>(32) = .882, <em>p</em> &gt; .05. There was a significant difference between Other and Protestant attitudes toward abortion, <em>t</em>(32) = -2.20, <em>p</em> &lt; .05. On average, Other attitudes toward abortion were about 23 points less favorable than Protestant attitudes toward abortion (95% CI: -44.62; -1.75; <span class="math inline">\(\beta = -.351\)</span>).</p>
<p><strong>Adding Other Variables to the Dummy Coded Model</strong>:</p>
<p><span class="math inline">\(\hat Y = a + {b_1}Catholic + {b_2}Jewish + {b_3}Other + {b_4}Gender + {b_5}Income\)</span></p>
<p>The dummy variables Catholic, Jewish, and Other are the same as before. Gender is 0 for male and 1 for female. Income is a continuous variable.</p>
<p>.red[What does each of the regression coefficients represent?]</p>
</div>
<div id="effect-coding" class="section level4" number="5.12.3.2">
<h4><span class="header-section-number">5.12.3.2</span> <strong>Effect Coding</strong></h4>
<p>The reference point is the unweighted mean of all of the group means, which is the intercept. The unstandardized regression coefficients represent the difference between each group’s mean and the unweighted mean. You must select a base group (a group for which comparisons with the mean are of least interest) and assign that group a value of -1. The results of multiple regression do not directly inform us about this base group (e.g., control) but may be obtained through subtraction. Useful with experimental designs.</p>
<table class=" lightable-classic" style="font-family: &quot;Arial Narrow&quot;, &quot;Source Sans Pro&quot;, sans-serif; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-149">Table 5.5: </span>Effect Coding
</caption>
<thead>
<tr>
<th style="empty-cells: hide;" colspan="1">
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Code Variable
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
Religion
</th>
<th style="text-align:right;">
Catholic
</th>
<th style="text-align:right;">
Jewish
</th>
<th style="text-align:right;">
Other
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Protestant
</td>
<td style="text-align:right;">
-1
</td>
<td style="text-align:right;">
-1
</td>
<td style="text-align:right;">
-1
</td>
</tr>
<tr>
<td style="text-align:left;">
Catholic
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Jewish
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Other
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
</tbody>
</table>
<p>Regression Equation: <span class="math inline">\(\hat Y = 81.90 - 21.23Catholic + 21.6Jewish - 11.78Other\)</span></p>
<ul>
<li><p>The intercept is equal to the grand mean of the group means.</p></li>
<li><p>Each regression coefficient is equal to the difference between the respective group mean and the grand mean of group means</p>
<ul>
<li><p>Catholic: 60.67 – 81.90 = -21.23</p></li>
<li><p>Jewish: 103.50 – 81.90 = 21.6</p></li>
<li><p>Other: 70.12 – 81.90 = -11.78</p></li>
</ul></li>
</ul>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="multiple-regression-analysis.html#cb306-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain the mean of `ata` by religion group</span></span>
<span id="cb306-2"><a href="multiple-regression-analysis.html#cb306-2" aria-hidden="true" tabindex="-1"></a><span class="fu">aggregate</span>(religion<span class="sc">$</span>ata, <span class="at">by =</span> <span class="fu">list</span>(religion<span class="sc">$</span>group), <span class="at">FUN =</span> mean) </span>
<span id="cb306-3"><a href="multiple-regression-analysis.html#cb306-3" aria-hidden="true" tabindex="-1"></a><span class="do">##   Group.1         x</span></span>
<span id="cb306-4"><a href="multiple-regression-analysis.html#cb306-4" aria-hidden="true" tabindex="-1"></a><span class="do">## 1       C  60.66667</span></span>
<span id="cb306-5"><a href="multiple-regression-analysis.html#cb306-5" aria-hidden="true" tabindex="-1"></a><span class="do">## 2       J 103.50000</span></span>
<span id="cb306-6"><a href="multiple-regression-analysis.html#cb306-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 3       O  70.12500</span></span>
<span id="cb306-7"><a href="multiple-regression-analysis.html#cb306-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 4       P  93.30769</span></span></code></pre></div>
<p>In R, effect coding is called “sum contrasts” (i.e., contrasts sum up to zero). Use the <code>contr.sum</code> function in the <code>memisc</code> package.</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="multiple-regression-analysis.html#cb307-1" aria-hidden="true" tabindex="-1"></a><span class="co"># library(memisc)</span></span>
<span id="cb307-2"><a href="multiple-regression-analysis.html#cb307-2" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(religion<span class="sc">$</span>group) <span class="ot">&lt;-</span> memisc<span class="sc">::</span><span class="fu">contr.sum</span>(<span class="at">n=</span><span class="dv">4</span>, <span class="at">base=</span><span class="dv">4</span>) <span class="co"># &quot;religion&quot; has four groups: C, J, O, P</span></span>
<span id="cb307-3"><a href="multiple-regression-analysis.html#cb307-3" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(religion<span class="sc">$</span>group) <span class="co"># Protestant is the reference group</span></span>
<span id="cb307-4"><a href="multiple-regression-analysis.html#cb307-4" aria-hidden="true" tabindex="-1"></a><span class="do">##    1  2  3</span></span>
<span id="cb307-5"><a href="multiple-regression-analysis.html#cb307-5" aria-hidden="true" tabindex="-1"></a><span class="do">## C  1  0  0</span></span>
<span id="cb307-6"><a href="multiple-regression-analysis.html#cb307-6" aria-hidden="true" tabindex="-1"></a><span class="do">## J  0  1  0</span></span>
<span id="cb307-7"><a href="multiple-regression-analysis.html#cb307-7" aria-hidden="true" tabindex="-1"></a><span class="do">## O  0  0  1</span></span>
<span id="cb307-8"><a href="multiple-regression-analysis.html#cb307-8" aria-hidden="true" tabindex="-1"></a><span class="do">## P -1 -1 -1</span></span>
<span id="cb307-9"><a href="multiple-regression-analysis.html#cb307-9" aria-hidden="true" tabindex="-1"></a>mymodel4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(ata <span class="sc">~</span> group, <span class="at">data=</span>religion)</span>
<span id="cb307-10"><a href="multiple-regression-analysis.html#cb307-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mymodel4)<span class="sc">$</span>coefficients</span>
<span id="cb307-11"><a href="multiple-regression-analysis.html#cb307-11" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Std. Error   t value     Pr(&gt;|t|)</span></span>
<span id="cb307-12"><a href="multiple-regression-analysis.html#cb307-12" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  81.89984   4.054882 20.197835 8.897761e-20</span></span>
<span id="cb307-13"><a href="multiple-regression-analysis.html#cb307-13" aria-hidden="true" tabindex="-1"></a><span class="do">## group1      -21.23317   6.849039 -3.100168 4.015602e-03</span></span>
<span id="cb307-14"><a href="multiple-regression-analysis.html#cb307-14" aria-hidden="true" tabindex="-1"></a><span class="do">## group2       21.60016   7.883081  2.740066 9.961199e-03</span></span>
<span id="cb307-15"><a href="multiple-regression-analysis.html#cb307-15" aria-hidden="true" tabindex="-1"></a><span class="do">## group3      -11.77484   7.121639 -1.653389 1.080306e-01</span></span></code></pre></div>
<p>Manually setting the sum contrasts</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="multiple-regression-analysis.html#cb308-1" aria-hidden="true" tabindex="-1"></a>Catholic <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb308-2"><a href="multiple-regression-analysis.html#cb308-2" aria-hidden="true" tabindex="-1"></a>Jewish <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb308-3"><a href="multiple-regression-analysis.html#cb308-3" aria-hidden="true" tabindex="-1"></a>Other <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb308-4"><a href="multiple-regression-analysis.html#cb308-4" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(religion<span class="sc">$</span>group) <span class="ot">&lt;-</span> <span class="fu">cbind</span>(Catholic, Jewish, Other) </span>
<span id="cb308-5"><a href="multiple-regression-analysis.html#cb308-5" aria-hidden="true" tabindex="-1"></a>religion<span class="sc">$</span>group</span>
<span id="cb308-6"><a href="multiple-regression-analysis.html#cb308-6" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1] C O P C C O P C O O C O P C P C C J O C P J P P J O P O J P P P J P J P</span></span>
<span id="cb308-7"><a href="multiple-regression-analysis.html#cb308-7" aria-hidden="true" tabindex="-1"></a><span class="do">## attr(,&quot;contrasts&quot;)</span></span>
<span id="cb308-8"><a href="multiple-regression-analysis.html#cb308-8" aria-hidden="true" tabindex="-1"></a><span class="do">##   Catholic Jewish Other</span></span>
<span id="cb308-9"><a href="multiple-regression-analysis.html#cb308-9" aria-hidden="true" tabindex="-1"></a><span class="do">## C        1      0     0</span></span>
<span id="cb308-10"><a href="multiple-regression-analysis.html#cb308-10" aria-hidden="true" tabindex="-1"></a><span class="do">## J        0      1     0</span></span>
<span id="cb308-11"><a href="multiple-regression-analysis.html#cb308-11" aria-hidden="true" tabindex="-1"></a><span class="do">## O        0      0     1</span></span>
<span id="cb308-12"><a href="multiple-regression-analysis.html#cb308-12" aria-hidden="true" tabindex="-1"></a><span class="do">## P       -1     -1    -1</span></span>
<span id="cb308-13"><a href="multiple-regression-analysis.html#cb308-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Levels: C J O P</span></span></code></pre></div>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="multiple-regression-analysis.html#cb309-1" aria-hidden="true" tabindex="-1"></a>mymodel5 <span class="ot">&lt;-</span> <span class="fu">lm</span>(ata <span class="sc">~</span> group, <span class="at">data=</span>religion)</span>
<span id="cb309-2"><a href="multiple-regression-analysis.html#cb309-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mymodel5)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##                Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)    81.89984   4.054882 20.197835 8.897761e-20
## groupCatholic -21.23317   6.849039 -3.100168 4.015602e-03
## groupJewish    21.60016   7.883081  2.740066 9.961199e-03
## groupOther    -11.77484   7.121639 -1.653389 1.080306e-01</code></pre>
</div>
</div>
</div>
<div id="interactions-between-categorical-and-continuous-variables" class="section level2" number="5.13">
<h2><span class="header-section-number">5.13</span> Interactions Between Categorical and Continuous Variables</h2>
<ul>
<li><p>Center the continuous predictor variables and multiply the centered continuous variable by the categorical variables.</p></li>
<li><p>For example, if we wanted to regress Endurance on Age, Gender (male = 0 vs. female =1), and Age*Gender, we would center the age variable and multiply that centered age variable by the gender variable:</p></li>
</ul>
<p><span class="math inline">\(\hat Y = a + {b_1}Age + {b_2}Gender + {b_3}Age*Gender\)</span></p>
<ul>
<li>If we wanted to regress Endurance on Age, Marital Status (Single, Married, Divorced/Separated, Widowed), and Age*Marital Status, we would first create 3 (g-1) dummy coded variables, center the age variable, and multiply the age variable by each of the three dummy coded variables:</li>
</ul>
<p><span class="math display">\[\begin{array}{l}
\hat Y = a + {b_1}Age + {b_2}Single + {b_3}Divorced/Separated + {b_4}Widowed + \\
{b_5}Age*Single + {b_6}Age*Divorced/Separated + {b_7}Age*Widowed
\end{array}\]</span></p>
</div>
<div id="an-open-book-on-regression" class="section level2" number="5.14">
<h2><span class="header-section-number">5.14</span> An Open Book on Regression</h2>
<p><a href="http://peopleanalytics-regression-book.org/index.html">Handbook of Regression Modeling in People Analytics: With Examples in R and Python</a></p>
<!-- ## Andy Field's "The General Linear Model" lecture on YouTube -->
<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/7cSArk7tU4w" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
</div>
<div id="create-apa-tables-with-apatables-package" class="section level2" number="5.15">
<h2><span class="header-section-number">5.15</span> Create APA Tables with <code>apaTables</code> Package</h2>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="multiple-regression-analysis.html#cb311-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apa.cor.table</span>(iris, <span class="at">filename=</span><span class="st">&quot;output/Table1_APA.doc&quot;</span>, <span class="at">table.number =</span> <span class="dv">1</span>) <span class="co"># filename must end in .rtf or .doc only)</span></span>
<span id="cb311-2"><a href="multiple-regression-analysis.html#cb311-2" aria-hidden="true" tabindex="-1"></a>reg.model <span class="ot">&lt;-</span> <span class="fu">lm</span>(sales <span class="sc">~</span> adverts <span class="sc">+</span> airplay, <span class="at">data =</span> album) <span class="co"># The &quot;album&quot; dataset comes with the &quot;apaTables&quot; package</span></span>
<span id="cb311-3"><a href="multiple-regression-analysis.html#cb311-3" aria-hidden="true" tabindex="-1"></a><span class="fu">apa.reg.table</span>(reg.model, <span class="at">filename =</span> <span class="st">&quot;output/Table2_APA.doc&quot;</span>, <span class="at">table.number =</span> <span class="dv">2</span>)</span></code></pre></div>
<p>For more examples, check <a href="https://cran.r-project.org/web/packages/apaTables/vignettes/apaTables.html">apaTables vignettes</a></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="binary-logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
