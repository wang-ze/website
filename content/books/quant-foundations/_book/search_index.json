[["index.html", "Quantitative Foundations in Educational Research Preface Contents", " Quantitative Foundations in Educational Research Ze Wang 2021-07-30 Preface This book was prepared for the ESC_PS 8850 Quantitative Foundations in Educational Research course I teach at the University of Missouri (MU). It covers class notes as well as R syntax for statistical analysis. It is a working document and I may update periodically. To cite this book, use the following: Wang, Z. (2021). Quantitative Foundations in Educational Research. Retrievable from https://zewang.netlify.app/books/quant-foundations/_book Contents The contents of the book roughly follow the order of lecture notes used in class. Chapter 1. Introduction to R and RStudio Chapter 2. Review of Basic Statistics Chapter 3. Correlation Chapter 4. Simple Linear Regression Chapter 5. Multile Regression Analysis Chapter 6. Binary Logistic Regression Chapter 7. Analysis of Variance Chapter 8. Analysis of Covariance Chapter 9. Repeated Measures ANOVA and Mixed Designs Chapter 10. Power Analysis "],["author.html", "About the Author", " About the Author Ze Wang is an Associate Professor of Statistics, Measurement, and Evaluation in Education in the Department of Educational, School and Counseling Psychology at the University of Missouri (MU). She earned her PhD in Educational Psychology with an emphasis in Measurement and Statistics from MU in 2008. Her research interests include data science, statistical modeling using large-scale educational assessment data, measurement, scale development, and program evaluation. She has over 15 years of experience in statistical consulting and won multiple awards for excellence in teaching and student advising. "],["intro.html", "Chapter 1 Introduction to R and RStudio 1.1 The R Language 1.2 Install R 1.3 Install RStudio 1.4 Use RStudio", " Chapter 1 Introduction to R and RStudio 1.1 The R Language R is a free, open-source programming language developed for statistical computing and graphics. It is open source meaning that everyone can access and contribute to its development. R was born out of S, which was intended to be a programming language focused on data analysis, and has evolved into a system used not only by computer programmers and data analysts but also by physical scientists, psychologists, journalists, etc. The first publicly available version of R was released in 2000. The latest version of R, released on 2021-05-18, is R-4.1.0. 1.1.1 Why R? For scholarly articles published in 2018 found on Google Scholar, R is the second most frequently used data science software following SPSS. KDnuggets Poll 2020 - Modern Data Science Skills KDnuggets Data Science Tools Popularity, animated KDnuggets Poll 2019 - Core Data Science Skills KDnuggets Poll 2019 - Top Analytics/Data Science Software Data Science Software Used in Journals in 2018 1.1.2 Statistical analysis software for data modeling There are other statistical analysis software packages that you may be aware of. For example, SPSS SAS STATA Mplus They typically expect that data are already nicely organized in columns and rows. R is a powerful tool for data modeling and data processing and visualization. R is considered a data-oriented programming language (other examples are Python, and Julia). It has the control and power of managing and analyzing data. 1.2 Install R R is available for Windows, Mac, and Linux operating systems. To install R, go to the Comprehensive R Archive Network (CRAN), download the version compatible with your operating system. For Windows or MacOS users, you probably want to download the precompiled binary distributions (i.e., ready-to-run applications) linked at the top of the CRAN webpage. The version downloaded includes the base R package. Often times, it is necessary to install other R packages to perform analysis. For example, for this course, we will use the [rio] (https://cran.r-project.org/web/packages/rio/index.html) package. I will show how to install packages in 1.4.4 1.3 Install RStudio RStudio is an integrated development environment (IDE) for R. It uses R to develop codes and analysis that can be executed and has greater usability than R itself. Essentially RStudio is an interface between the user and R (there are other interfaces for R, e.g., R Commander). It depends on and adds onto R, which means that the R program has to be installed before RStudio for RStudio to implement R. Any R package or function can be used in RStudio. To install RStudio, go to the RStudio download page, and download and install the free RStudio Desktop. Open R Studio and you will see a window with four panes. R script R console Environment/History/Connections/Tutorial Files/Plots/Packages You can change the appearances and layout of the panes. Go to Tools -&gt; Global Options -&gt; Appearance to change the appearance. 1.4 Use RStudio There are two basic ways for RStudio to execute your R syntax: (1) type your code directly in the R console and (2) write R script. If you type your code directly in the R console, the code will no longer be accessible after you close your R session. Writing R script is recommended if you would like to save the code (with .R extension). In this class, we will mainly use RStudio for our coding purpose.1 1.4.1 Basic operations 1.4.1.1 entering data 2+2 #press cmd/ctrol enter ## [1] 4 1:20 #sequence ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 print(&quot;Hello World!&quot;) ## [1] &quot;Hello World!&quot; 1.4.1.2 assign value a &lt;- 5 #use &lt;-, not = 3 -&gt; b #can go other way, but silly c &lt;- d &lt;- e &lt;- 2 #multiple assignments 1.4.1.3 multiple values x &lt;- c (5, 3, 6, 9) #c means combine or concatenate x ## [1] 5 3 6 9 (y &lt;- c(1, 3, 0, 10)) #surround command with parentheses to also print ## [1] 1 3 0 10 1.4.1.4 sequences 0:5 #0 through 5 ## [1] 0 1 2 3 4 5 5:0 #5 through 0 ## [1] 5 4 3 2 1 0 seq(5) #1 to 5 ## [1] 1 2 3 4 5 seq(5, 0, by = -2) #count down by 2 ## [1] 5 3 1 1.4.1.5 math x + y #adds corresponding elements in x and y ## [1] 6 6 6 19 x * 2 #multiplies each element in x by 2 ## [1] 10 6 12 18 2^4 #powers/exponents ## [1] 16 sqrt(36) #squareroot ## [1] 6 log(100) #natural log: base e (2.71828...) ## [1] 4.60517 log10(100) #base 10 log ## [1] 2 1.4.2 Data types in R 1.4.2.1 numeric (n1 &lt;- 3) ## [1] 3 typeof(n1) ## [1] &quot;double&quot; (n2 &lt;- 2.5) ## [1] 2.5 typeof(n2) ## [1] &quot;double&quot; 1.4.2.2 character (c1 &lt;- &quot;c&quot;) ## [1] &quot;c&quot; typeof(c1) ## [1] &quot;character&quot; (c2 &lt;- &quot;a string of text&quot;) ## [1] &quot;a string of text&quot; typeof(c2) ## [1] &quot;character&quot; 1.4.2.3 logical (l1 &lt;- TRUE) #must use uppercase ## [1] TRUE typeof(l1) ## [1] &quot;logical&quot; (l2 &lt;- F) #must use uppercase ## [1] FALSE typeof(l2) ## [1] &quot;logical&quot; 1.4.3 Data structures in R 1.4.3.1 vector A vector has elements of the same data type. (v1 &lt;- c(1, 2, 3, 4, 5)) ## [1] 1 2 3 4 5 is.vector(v1) ## [1] TRUE (v2 &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; is.vector(v2) ## [1] TRUE (v3 &lt;- c(TRUE, TRUE, FALSE, FALSE, TRUE)) ## [1] TRUE TRUE FALSE FALSE TRUE is.vector(v3) ## [1] TRUE 1.4.3.2 matrix A matrix is two-dimensional and has elements of the same type. (m1 &lt;- matrix(c(T, T, F, F, T, F), nrow = 2)) ## [,1] [,2] [,3] ## [1,] TRUE FALSE TRUE ## [2,] TRUE FALSE FALSE (m2 &lt;- matrix(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;), nrow = 2, byrow = T)) #default is FALSE ## [,1] [,2] ## [1,] &quot;a&quot; &quot;b&quot; ## [2,] &quot;c&quot; &quot;d&quot; 1.4.3.3 array An array can be more than two dimensions (a1 &lt;- array(c( 1:24), c(4, 3, 2))) #four rows, three columns, and two tables ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 13 17 21 ## [2,] 14 18 22 ## [3,] 15 19 23 ## [4,] 16 20 24 1.4.3.4 data frame A data frame combines vectors of the same length. Numeric &lt;- c(1, 2, 3) Character &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) Logical &lt;- c(T, F, T) (df1 &lt;- cbind(Numeric, Character, Logical)) #coerces all values to most basic data type ## Numeric Character Logical ## [1,] &quot;1&quot; &quot;a&quot; &quot;TRUE&quot; ## [2,] &quot;2&quot; &quot;b&quot; &quot;FALSE&quot; ## [3,] &quot;3&quot; &quot;c&quot; &quot;TRUE&quot; (df2 &lt;- as.data.frame(cbind(Numeric, Character, Logical))) #makes a data frame with three different data types ## Numeric Character Logical ## 1 1 a TRUE ## 2 2 b FALSE ## 3 3 c TRUE (df3 &lt;- data.frame(Numeric, Character, Logical)) #the right way! ## Numeric Character Logical ## 1 1 a TRUE ## 2 2 b FALSE ## 3 3 c TRUE 1.4.3.5 list A list can have anything. o1 &lt;- c(1, 2, 3) o2 &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) o3 &lt;- c(T, F, T, T, F) (list1 &lt;- list(o1, o2, o3)) ## [[1]] ## [1] 1 2 3 ## ## [[2]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ## ## [[3]] ## [1] TRUE FALSE TRUE TRUE FALSE (list2 &lt;- list(o1, o2, o3, list1)) #lists within lists! ## [[1]] ## [1] 1 2 3 ## ## [[2]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ## ## [[3]] ## [1] TRUE FALSE TRUE TRUE FALSE ## ## [[4]] ## [[4]][[1]] ## [1] 1 2 3 ## ## [[4]][[2]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ## ## [[4]][[3]] ## [1] TRUE FALSE TRUE TRUE FALSE 1.4.3.6 a few notes on R data structures matrices and arrays are vectors with the dim attribute. Factors are integer vectors with theclass and levels attributes.. A factor is a vector that can contain only predefined values. data frames are built on top of lists and therefore have the list type. vectors are the most important family of data types in R. list are sometimes called generic vectors and the regular vectors are sometimes called atomic vectors. Hadley Wickham had a good description of the relationships between different types in his Advanced R book. 1.4.4 R packages When you install R, the base package is installed. The datasets package with example datasets is also ready to use. Type plot(iris) in Rstudio Console, or type it in an R script file and run the syntax. The iris flower data set is a multivariate data set introduced by the British statistician Ronald Fisher. plot(iris) For this class, we will use a few packages. Use the install.packages() function to install an R package. For example, type install.packages(\"rio\") to install the rio package for importing and exporting data from external programs. You only need to install the package once on your computer. Note that you need to put the package names inside quotation marks. To use the package, type either library(rio) or require(rio). You need to load the package everytime you start a new R session. 1.4.5 RStudio projects An RStudio project saves a relative base. When you create a R Studio project, a folder is created and all the file paths are relative to this folder. You can copy and paste this folder to another place without having to change file paths. For example, you can create a data folder inside the RStudio project for this class and keep all data in this folder. And then you can import a dataset to R. 1.4.6 Import and export Data There are several packages (e.g., foreign and haven) that can help import and export data in different statistical formats (e.g., SPSS, SAS, Stata, Excel). The one that I particularly like is the rio package. Its import() and export() functions work with different data types. Install and load rio. install.packages(&quot;rio&quot;) library(rio) Import an SPSS data from the data folder within the RStudio project.2 mydata &lt;- import(&quot;data/example.sav&quot;) head(mydata) # view first 6 observations ## IDCNTRY IDBOOK IDSCHOOL IDCLASS IDSTUD IDGRADE ITSEX ITADMINI ITLANG BSBG01 BSBG03 BSBG04 BSBG05 BSBG06A BSBG06B BSBG06C BSBG06D ## 1 840 4 1 103 10301 8 1 1 1 1 1 2 5 1 1 1 1 ## 2 840 5 1 103 10302 8 2 1 1 2 2 3 3 1 1 1 1 ## BSBG06E BSBG06F BSBG06G BSBG06H BSBG06I BSBG06J BSBG06K BSBG07A BSBG07B BSBG08 BSBG09A BSBG09B BSBG10A BSBG10B BSBG11 BSBG12 BSBG13A ## 1 1 1 1 1 NA NA NA 7 5 5 1 1 1 NA 1 3 1 ## 2 1 1 1 1 NA NA NA 7 8 6 1 1 1 NA 4 2 4 ## BSBG13B BSBG13C BSBG14A BSBG14B BSBG14C BSBG14D BSBG14E BSBG14F BSBG15A BSBG15B BSBG15C BSBG15D BSBG15E BSBG15F BSBG15G BSBG16A ## 1 3 1 2 1 2 2 1 1 2 1 2 2 2 3 1 1 ## 2 3 4 2 1 2 1 1 1 3 2 2 1 2 1 2 2 ## BSBG16B BSBG16C BSBG16D BSBG16E BSBG16F BSBG16G BSBG16H BSBG16I BSBM17A BSBM17B BSBM17C BSBM17D BSBM17E BSBM17F BSBM17G BSBM17H ## 1 1 1 4 4 4 1 4 4 1 4 4 1 1 1 1 1 ## 2 1 4 3 4 4 4 4 4 1 4 1 3 1 1 1 3 ## BSBM17I BSBM18A BSBM18B BSBM18C BSBM18D BSBM18E BSBM18F BSBM18G BSBM18H BSBM18I BSBM18J BSBM19A BSBM19B BSBM19C BSBM19D BSBM19E ## 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 1 4 ## 2 1 1 1 3 2 1 1 1 1 1 1 1 4 4 1 4 ## BSBM19F BSBM19G BSBM19H BSBM19I BSBM20A BSBM20B BSBM20C BSBM20D BSBM20E BSBM20F BSBM20G BSBM20H BSBM20I BSBS21A BSBS21B BSBS21C ## 1 1 2 4 4 1 1 1 1 1 1 1 1 1 1 2 1 ## 2 1 1 4 4 1 1 2 1 1 1 1 1 1 4 3 1 ## BSBS21D BSBS21E BSBS21F BSBS21G BSBS21H BSBS21I BSBS22A BSBS22B BSBS22C BSBS22D BSBS22E BSBS22F BSBS22G BSBS22H BSBS22I BSBS22J ## 1 2 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 ## 2 3 1 3 1 1 1 1 1 3 2 1 1 1 1 1 1 ## BSBS23A BSBS23B BSBS23C BSBS23D BSBS23E BSBS23F BSBS23G BSBS23H BSBS24A BSBS24B BSBS24C BSBS24D BSBS24E BSBS24F BSBS24G BSBS24H ## 1 2 2 2 2 2 2 2 2 1 1 1 3 3 2 3 1 ## 2 1 4 4 1 2 2 4 3 1 2 3 1 1 1 1 2 ## BSBS24I BSBM25AA BSBS25AB BSBM25BA BSBS25BB BSBM26AA BSBS26AB BSBM26BA BSBS26BB BSBB21 BSBB22A BSBB22B BSBB22C BSBB22D BSBB22E BSBB22F ## 1 1 4 4 3 3 2 3 2 2 NA NA NA NA NA NA NA ## 2 1 2 4 2 3 3 3 1 1 NA NA NA NA NA NA NA ## BSBB22G BSBB22H BSBB22I BSBB23A BSBB23B BSBB23C BSBB23D BSBB23E BSBB23F BSBB23G BSBB23H BSBB23I BSBB23J BSBB24A BSBB24B BSBB24C ## 1 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## BSBB24D BSBB24E BSBB24F BSBB24G BSBB24H BSBE25 BSBE26A BSBE26B BSBE26C BSBE26D BSBE26E BSBE26F BSBE26G BSBE26H BSBE26I BSBE27A BSBE27B ## 1 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## BSBE27C BSBE27D BSBE27E BSBE27F BSBE27G BSBE27H BSBE27I BSBE27J BSBE28A BSBE28B BSBE28C BSBE28D BSBE28E BSBE28F BSBE28G BSBE28H BSBC29 ## 1 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## BSBC30A BSBC30B BSBC30C BSBC30D BSBC30E BSBC30F BSBC30G BSBC30H BSBC30I BSBC31A BSBC31B BSBC31C BSBC31D BSBC31E BSBC31F BSBC31G ## 1 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## BSBC31H BSBC31I BSBC31J BSBC32A BSBC32B BSBC32C BSBC32D BSBC32E BSBC32F BSBC32G BSBC32H BSBP33 BSBP34A BSBP34B BSBP34C BSBP34D BSBP34E ## 1 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## BSBP34F BSBP34G BSBP34H BSBP34I BSBP35A BSBP35B BSBP35C BSBP35D BSBP35E BSBP35F BSBP35G BSBP35H BSBP35I BSBP35J BSBP36A BSBP36B ## 1 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## BSBP36C BSBP36D BSBP36E BSBP36F BSBP36G BSBP36H BSBS37A BSBS37B BSBS37C BSBS37D BSBS37E BSBS37F BSBS37G BSBS37H BSBS37I BSBM38AA ## 1 NA NA NA NA NA NA 1 1 1 3 3 2 3 1 1 4 ## 2 NA NA NA NA NA NA 1 2 3 1 1 1 1 2 1 2 ## BSBB38AB BSBE38AC BSBC38AD BSBP38AE BSBM38BA BSBB38BB BSBE38BC BSBC38BD BSBP38BE BSBM39AA BSBS39AB BSBM39BA BSBS39BB ITACCOMM1 IDPOP ## 1 NA NA NA NA 3 NA NA NA NA 2 3 2 2 0 2 ## 2 NA NA NA NA 2 NA NA NA NA 3 3 1 1 1 2 ## IDGRADER BSDAGE TOTWGT HOUWGT SENWGT WGTADJ1 WGTADJ2 WGTADJ3 WGTFAC1 WGTFAC2 WGTFAC3 JKZONE JKREP BSMMAT01 BSMMAT02 BSMMAT03 ## 1 2 13.92 191.4986 0.535255 0.026184 1.102564 1 1.045454 36.91851 4.5 1 25 0 514.4249 505.7916 519.7016 ## 2 2 13.92 191.4986 0.535255 0.026184 1.102564 1 1.045454 36.91851 4.5 1 25 0 587.6541 579.3172 562.0366 ## BSMMAT04 BSMMAT05 BSSSCI01 BSSSCI02 BSSSCI03 BSSSCI04 BSSSCI05 BSMALG01 BSMALG02 BSMALG03 BSMALG04 BSMALG05 BSMDAT01 BSMDAT02 BSMDAT03 ## 1 531.0448 542.1232 490.0679 474.9255 521.2601 493.9883 513.8363 514.5657 503.5694 540.7127 495.9604 516.0884 538.7014 555.5254 504.2005 ## 2 550.9720 568.2771 571.5391 535.8830 558.9193 559.8913 592.2060 575.0255 603.4856 589.0744 516.0658 602.3520 545.7862 573.8504 579.5864 ## BSMDAT04 BSMDAT05 BSMNUM01 BSMNUM02 BSMNUM03 BSMNUM04 BSMNUM05 BSMGEO01 BSMGEO02 BSMGEO03 BSMGEO04 BSMGEO05 BSSCHE01 BSSCHE02 BSSCHE03 ## 1 553.7694 508.4274 512.7626 511.1604 487.0292 490.313 471.9762 516.0974 497.4462 494.0238 468.7053 434.6766 475.5856 554.9979 479.4414 ## 2 490.1844 609.3757 562.7668 579.2757 594.9696 548.411 630.5318 619.2435 573.1298 605.8552 496.3814 623.6547 615.6122 598.3530 644.2855 ## BSSCHE04 BSSCHE05 BSSEAR01 BSSEAR02 BSSEAR03 BSSEAR04 BSSEAR05 BSSBIO01 BSSBIO02 BSSBIO03 BSSBIO04 BSSBIO05 BSSPHY01 BSSPHY02 BSSPHY03 ## 1 509.9053 516.8859 454.3833 540.0875 446.0872 465.3053 540.4619 497.7536 534.3983 529.7410 507.6892 570.4911 456.0079 513.4090 480.3403 ## 2 558.5238 587.9128 600.9338 563.4683 620.5797 570.7632 591.3771 601.0626 567.6580 611.6534 563.5830 543.3118 550.5409 533.1618 577.5741 ## BSSPHY04 BSSPHY05 BSMKNO01 BSMKNO02 BSMKNO03 BSMKNO04 BSMKNO05 BSMAPP01 BSMAPP02 BSMAPP03 BSMAPP04 BSMAPP05 BSMREA01 BSMREA02 BSMREA03 ## 1 467.8219 543.4946 529.0403 558.7877 503.6952 492.9102 538.8042 500.4389 519.0317 510.1606 497.2687 489.3579 484.6627 494.2266 504.5591 ## 2 527.0618 575.5149 593.5888 550.6020 577.6243 567.9767 564.5701 574.4144 568.5675 547.7672 556.5969 539.5741 552.9415 572.1910 584.6759 ## BSMREA04 BSMREA05 BSSKNO01 BSSKNO02 BSSKNO03 BSSKNO04 BSSKNO05 BSSAPP01 BSSAPP02 BSSAPP03 BSSAPP04 BSSAPP05 BSSREA01 BSSREA02 BSSREA03 ## 1 518.4353 493.1853 488.4110 491.6673 524.3182 507.8949 502.7248 549.3676 512.9106 519.5156 526.0618 495.4574 503.3588 492.6981 507.1144 ## 2 601.2263 551.5208 593.9882 545.3113 591.2581 584.1462 577.5074 571.8668 570.2205 584.9571 590.2473 547.1351 582.2200 565.3724 556.9847 ## BSSREA04 BSSREA05 BSMIBM01 BSMIBM02 BSMIBM03 BSMIBM04 BSMIBM05 BSSIBM01 BSSIBM02 BSSIBM03 BSSIBM04 BSSIBM05 BSBGHER BSDGHER BSBGSSB ## 1 500.9916 470.2027 3 3 3 3 3 3 2 3 3 3 10.91822 2 8.98361 ## 2 570.5423 560.6985 4 4 4 4 4 4 3 4 4 4 11.62212 2 8.98361 ## BSDGSSB BSBGSB BSDGSB BSBGSLM BSDGSLM BSBGEML BSDGEML BSBGSCM BSDGSCM BSBGSVM BSDGSVM BSBGSLS BSDGSLS BSBGESL BSDGESL BSBGSCS ## 1 2 7.59182 2 13.97818 1 13.60365 1 14.10390 1 13.65262 1 9.58043 2 12.94849 1 9.12033 ## 2 2 8.78408 2 10.68282 2 11.13328 1 15.92523 1 12.14660 1 8.90626 2 10.86204 1 11.98495 ## BSDGSCS BSBGSVS BSDGSVS BSBGSLB BSDGSLB BSBGEBL BSDGEBL BSBGSCB BSDGSCB BSBGSLE BSDGSLE BSBGEEL BSDGEEL BSBGSCE BSDGSCE BSBGSLC ## 1 3 9.90677 2 NA NA NA NA NA NA NA NA NA NA NA NA NA ## 2 1 10.71378 1 NA NA NA NA NA NA NA NA NA NA NA NA NA ## BSDGSLC BSBGECL BSDGECL BSBGSCC BSDGSCC BSBGSLP BSDGSLP BSBGEPL BSDGEPL BSBGSCP BSDGSCP BSDG06S BSDGEDUP BSDMLOWP BSDSLOWP BSDMWKHW ## 1 NA NA NA NA NA NA NA NA NA NA NA 2 1 2 2 3 ## 2 NA NA NA NA NA NA NA NA NA NA NA 2 1 2 2 3 ## BSDSWKHS BSDBWKHB BSDCWKHC BSDPWKHP BSDEWKHE VERSION ## 1 3 NA NA NA NA 4 ## 2 3 NA NA NA NA 4 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 4 rows ] Export the data. export(mydata, file = &quot;data/dataset_example.csv&quot;) export(mydata, file = &quot;data/dataset_example.xlsx&quot;) 1.4.6.1 a few functions for checking data str(mydata) ## &#39;data.frame&#39;: 10221 obs. of 436 variables: ## $ IDCNTRY : num 840 840 840 840 840 840 840 840 840 840 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Country ID - Numeric Code&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F6.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num 1e+06 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;Omitted or invalid&quot; ## $ IDBOOK : num 4 5 6 7 8 9 10 11 13 14 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Booklet ID&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F2.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:16] 0 1 2 3 4 5 6 7 8 9 ... ## .. ..- attr(*, &quot;names&quot;)= chr [1:16] &quot;No booklet assigned as student is excluded&quot; &quot;Booklet 01&quot; &quot;Booklet 02&quot; &quot;Booklet 03&quot; ... ## $ IDSCHOOL : num 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;School ID&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F4.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num 9999 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;Omitted or invalid&quot; ## $ IDCLASS : num 103 103 103 103 103 103 103 103 103 103 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Class ID&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F6.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num 1e+06 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;Omitted or invalid&quot; ## $ IDSTUD : num 10301 10302 10303 10304 10305 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Student ID&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num 1e+08 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;Omitted or invalid&quot; ## $ IDGRADE : num 8 8 8 8 8 8 8 8 8 8 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Grade ID&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F2.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:9] 3 4 5 6 7 8 9 10 99 ## .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Grade 3&quot; &quot;Grade 4&quot; &quot;Grade 5&quot; &quot;Grade 6&quot; ... ## $ ITSEX : num 1 2 2 2 2 2 1 1 2 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Sex of Students&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Female&quot; &quot;Male&quot; &quot;Omitted or invalid&quot; ## $ ITADMINI : num 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Test Administrator Position&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:4] 1 2 3 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;National Center Staff&quot; &quot;Teacher from School&quot; &quot;Other&quot; &quot;Omitted or invalid&quot; ## $ ITLANG : num 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Language of Testing&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F2.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:73] 1 2 3 4 5 7 8 9 10 11 ... ## .. ..- attr(*, &quot;names&quot;)= chr [1:73] &quot;English&quot; &quot;Spanish&quot; &quot;French&quot; &quot;Afrikaans&quot; ... ## $ BSBG01 : num 1 2 2 2 2 2 1 1 2 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\SEX OF STUDENT&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Girl&quot; &quot;Boy&quot; &quot;Omitted or invalid&quot; ## $ BSBG03 : num 1 2 2 1 1 1 2 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\OFTEN SPEAK &lt;LANG OF TEST&gt; AT HOME&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Always&quot; &quot;Almost always&quot; &quot;Sometimes&quot; &quot;Never&quot; ... ## $ BSBG04 : num 2 3 2 2 3 3 1 4 1 5 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\AMOUNT OF BOOKS IN YOUR HOME&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:6] 1 2 3 4 5 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;010 books&quot; &quot;1125 books&quot; &quot;26100 books&quot; &quot;101200 books&quot; ... ## $ BSBG05 : num 5 3 3 4 4 4 3 4 2 5 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\DIGITAL INFORMATION DEVICES&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:6] 1 2 3 4 5 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;None&quot; &quot;1-3 devices&quot; &quot;4-6 devices&quot; &quot;7-10 devices&quot; ... ## $ BSBG06A : num 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOME POSSESS\\\\COMPUTER TABLET OWN&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG06B : num 1 1 1 1 2 1 1 1 2 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOME POSSESS\\\\COMPUTER TABLET SHARED&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG06C : num 1 1 2 1 1 1 2 1 2 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOME POSSESS\\\\STUDY DESK&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG06D : num 1 1 1 1 1 1 2 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOME POSSESS\\\\OWN ROOM&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG06E : num 1 1 1 1 1 1 2 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOME POSSESS\\\\INTERNET CONNECTION&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG06F : num 1 1 1 1 1 2 1 2 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOME POSSESS\\\\OWN MOBILE PHONE&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG06G : num 1 1 1 1 1 1 2 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOME POSSESS\\\\GAMING SYSTEM&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG06H : num 1 1 1 1 1 1 1 1 2 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOME POSSESS\\\\&lt;COUNTRY SPECIFIC&gt;&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG06I : num NA NA NA NA NA NA NA NA NA NA ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOME POSSESS\\\\&lt;COUNTRY SPECIFIC&gt;&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG06J : num NA NA NA NA NA NA NA NA NA NA ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOME POSSESS\\\\&lt;COUNTRY SPECIFIC&gt;&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG06K : num NA NA NA NA NA NA NA NA NA NA ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOME POSSESS\\\\&lt;COUNTRY SPECIFIC&gt;&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG07A : num 7 7 2 7 7 5 2 5 6 6 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HIGHEST LVL OF EDU OF MOTHER&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F2.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:9] 1 2 3 4 5 6 7 8 99 ## .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Some Primary or Lower secondary or did not go to school&quot; &quot;Lower secondary&quot; &quot;Upper secondary&quot; &quot;Post-secondary, non-tertiary&quot; ... ## $ BSBG07B : num 5 8 2 3 8 3 2 5 5 6 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HIGHEST LVL OF EDU OF FATHER&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F2.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:9] 1 2 3 4 5 6 7 8 99 ## .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Some Primary or Lower secondary or did not go to school&quot; &quot;Lower secondary&quot; &quot;Upper secondary&quot; &quot;Post-secondary, non-tertiary&quot; ... ## $ BSBG08 : num 5 6 5 6 6 5 5 4 5 5 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW FAR IN EDU DO YOU EXPECT TO GO&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:7] 1 2 3 4 5 6 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Finish Lower secondary&quot; &quot;Finish Upper secondary&quot; &quot;Finish Post-secondary, non-tertiary&quot; &quot;Finish Short-cycle tertiary&quot; ... ## $ BSBG09A : num 1 1 2 1 1 1 2 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\MOTHER BORN IN &lt;COUNTRY&gt;&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:4] 1 2 3 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Yes&quot; &quot;No&quot; &quot;I don&#39;t know&quot; &quot;Omitted or invalid&quot; ## $ BSBG09B : num 1 1 2 1 1 1 2 1 3 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\FATHER BORN IN &lt;COUNTRY&gt;&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:4] 1 2 3 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Yes&quot; &quot;No&quot; &quot;I don&#39;t know&quot; &quot;Omitted or invalid&quot; ## $ BSBG10A : num 1 1 2 1 1 1 2 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\BORN IN &lt;COUNTRY&gt;&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG10B : num NA NA 3 NA NA NA 3 NA NA NA ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\BORN IN &lt;COUNTRY&gt;\\\\AGE CAME TO COUNTRY&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 6 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Older than 10 years old&quot; &quot;5 to 10 years old&quot; &quot;Younger than 5 years old&quot; &quot;Logically not applicable&quot; ... ## $ BSBG11 : num 1 4 4 4 4 3 4 3 1 3 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\ABOUT HOW OFTEN ABSENT FROM SCHOOL&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Once a week or more&quot; &quot;Once every two weeks&quot; &quot;Once a month&quot; &quot;Never or almost never&quot; ... ## $ BSBG12 : num 3 2 1 2 2 2 1 1 2 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW OFTEN BREAKFAST ON SCHOOL DAYS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Every day&quot; &quot;Most days&quot; &quot;Sometimes&quot; &quot;Never or almost never&quot; ... ## $ BSBG13A : num 1 4 3 1 1 1 1 4 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW OFTEN USE COMPUTER TABLET\\\\HOME&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Every day or almost every day&quot; &quot;Once or twice a week&quot; &quot;Once or twice a month&quot; &quot;Never or almost never&quot; ... ## $ BSBG13B : num 3 3 3 2 1 1 2 4 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW OFTEN USE COMPUTER TABLET\\\\SCHOOL&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Every day or almost every day&quot; &quot;Once or twice a week&quot; &quot;Once or twice a month&quot; &quot;Never or almost never&quot; ... ## $ BSBG13C : num 1 4 4 1 4 1 4 4 1 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW OFTEN USE COMPUTER TABLET\\\\OTHER&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Every day or almost every day&quot; &quot;Once or twice a week&quot; &quot;Once or twice a month&quot; &quot;Never or almost never&quot; ... ## $ BSBG14A : num 2 2 2 2 1 1 1 1 2 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\INTERNET USE\\\\ACCESS TEXTBOOKS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG14B : num 1 1 1 1 1 2 1 1 2 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\INTERNET USE\\\\ACCESS ASSIGNMENTS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG14C : num 2 2 2 1 2 1 1 2 2 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\INTERNET USE\\\\COLLABORATE WITH CLASSMATES&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG14D : num 2 1 2 1 2 1 2 1 2 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\INTERNET USE\\\\COMMUNICATE WITH TEACHER&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG14E : num 1 1 1 1 1 1 1 1 2 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\INTERNET USE\\\\FIND INFO TO AID IN MATH&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG14F : num 1 1 1 2 1 1 1 1 2 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\INTERNET USE\\\\FIND INFO TO AID IN SCIENCE&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:3] 1 2 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Yes&quot; &quot;No&quot; &quot;Omitted or invalid&quot; ## $ BSBG15A : num 2 3 2 2 3 1 2 2 2 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\AGREE\\\\BEING IN SCHOOL&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBG15B : num 1 2 3 1 4 1 1 1 2 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\AGREE\\\\SAFE AT SCHOOL&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBG15C : num 2 2 2 1 2 2 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\AGREE\\\\BELONG AT SCHOOL&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBG15D : num 2 1 1 1 2 1 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\AGREE\\\\LIKE TO SEE CLASSMATES&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBG15E : num 2 2 3 3 3 1 3 1 3 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\AGREE\\\\FAIR TEACHERS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBG15F : num 3 1 2 3 2 1 1 1 2 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\AGREE\\\\PROUD TO GO TO THIS SCHOOL&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBG15G : num 1 2 2 1 1 1 1 1 2 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\AGREE\\\\LEARN A LOT&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBG16A : num 1 2 4 4 3 3 4 4 1 3 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW OFTEN\\\\MADE FUN OF&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;At least once a week&quot; &quot;Once or twice a month&quot; &quot;A few times a year&quot; &quot;Never&quot; ... ## $ BSBG16B : num 1 1 4 4 4 4 4 4 4 4 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW OFTEN\\\\LEFT OUT OF GAMES&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;At least once a week&quot; &quot;Once or twice a month&quot; &quot;A few times a year&quot; &quot;Never&quot; ... ## $ BSBG16C : num 1 4 4 4 3 3 4 4 4 4 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW OFTEN\\\\SPREAD LIES ABOUT ME&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;At least once a week&quot; &quot;Once or twice a month&quot; &quot;A few times a year&quot; &quot;Never&quot; ... ## $ BSBG16D : num 4 3 4 4 2 4 3 4 4 3 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW OFTEN\\\\STOLE STH FROM ME&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;At least once a week&quot; &quot;Once or twice a month&quot; &quot;A few times a year&quot; &quot;Never&quot; ... ## $ BSBG16E : num 4 4 4 4 4 4 4 4 4 4 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW OFTEN\\\\HURT BY OTHERS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;At least once a week&quot; &quot;Once or twice a month&quot; &quot;A few times a year&quot; &quot;Never&quot; ... ## $ BSBG16F : num 4 4 4 4 4 4 4 4 4 4 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW OFTEN\\\\FORCE TO DO STH&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;At least once a week&quot; &quot;Once or twice a month&quot; &quot;A few times a year&quot; &quot;Never&quot; ... ## $ BSBG16G : num 1 4 4 4 2 4 4 4 4 4 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW OFTEN\\\\EMBARRASSING INFO&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;At least once a week&quot; &quot;Once or twice a month&quot; &quot;A few times a year&quot; &quot;Never&quot; ... ## $ BSBG16H : num 4 4 4 4 4 4 4 4 4 4 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW OFTEN\\\\POSTED EMBARRASSING THINGS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;At least once a week&quot; &quot;Once or twice a month&quot; &quot;A few times a year&quot; &quot;Never&quot; ... ## $ BSBG16I : num 4 4 4 4 4 4 4 4 4 4 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;GEN\\\\HOW OFTEN\\\\THREATENED&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;At least once a week&quot; &quot;Once or twice a month&quot; &quot;A few times a year&quot; &quot;Never&quot; ... ## $ BSBM17A : num 1 1 1 1 2 1 3 2 3 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\ENJOY LEARNING MATHEMATICS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM17B : num 4 4 4 4 1 2 3 4 1 3 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\WISH HAVE NOT TO STUDY MATH&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM17C : num 4 1 4 4 4 2 2 4 1 3 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\MATH IS BORING&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM17D : num 1 3 1 1 1 1 3 2 4 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\LEARN INTERESTING THINGS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM17E : num 1 1 1 1 1 1 4 2 4 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\LIKE MATHEMATICS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM17F : num 1 1 1 2 2 3 4 3 4 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\LIKE NUMBERS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM17G : num 1 1 1 1 2 2 4 3 4 3 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\LIKE MATH PROBLEMS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM17H : num 1 3 1 1 3 2 4 2 4 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\LOOK FORWARD TO MATH CLASS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM17I : num 1 1 1 1 2 2 4 2 4 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\FAVORITE SUBJECT&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM18A : num 1 1 2 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\TEACHER EXPECTS TO DO&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM18B : num 1 1 2 1 2 2 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\TEACHER IS EASY TO UNDERSTAND&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM18C : num 1 3 2 1 2 1 2 1 4 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\INTERESTED IN WHAT TCHR SAYS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM18D : num 1 2 3 2 3 1 2 1 4 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\INTERESTING THINGS TO DO&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM18E : num 1 1 2 1 2 1 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\TEACHER CLEAR ANSWERS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM18F : num 1 1 2 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\TEACHER EXPLAINS GOOD&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM18G : num 1 1 3 2 3 1 1 1 1 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\TEACHER SHOWS LEARNED&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM18H : num 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\DIFFERENT THINGS TO HELP&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM18I : num 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\TELLS HOW TO DO BETTER&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM18J : num 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\TEACHER LISTENS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM19A : num 1 1 1 1 1 1 2 1 4 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\USUALLY DO WELL IN MATH&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM19B : num 4 4 4 4 4 2 2 4 1 3 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\MATHEMATICS IS MORE DIFFICULT&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM19C : num 4 4 4 4 2 2 1 4 1 3 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\MATHEMATICS NOT MY STRENGTH&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM19D : num 1 1 1 1 2 2 3 1 4 3 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\LEARN QUICKLY IN MATHEMATICS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM19E : num 4 4 4 3 4 1 1 4 1 3 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\MAT MAKES NERVOUS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM19F : num 1 1 1 2 3 1 4 2 4 3 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\GOOD AT WORKING OUT PROBLEMS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM19G : num 2 1 2 1 2 1 4 3 4 3 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\I AM GOOD AT MATHEMATICS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM19H : num 4 4 4 4 3 3 2 4 1 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\MATHEMATICS HARDER FOR ME&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM19I : num 4 4 4 4 4 2 1 4 1 3 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\MAT MAKES CONFUSED&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM20A : num 1 1 1 1 1 1 2 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\MATHEMATICS WILL HELP ME&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM20B : num 1 1 1 2 1 1 2 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\NEED MAT TO LEARN OTHER THINGS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM20C : num 1 2 1 3 1 1 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\NEED MATH TO GET INTO &lt;UNI&gt;&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM20D : num 1 1 1 3 1 1 2 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\NEED MAT TO GET THE JOB I WANT&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM20E : num 1 1 1 2 4 1 4 2 1 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\JOB INVOLVING MATHEMATICS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM20F : num 1 1 1 1 1 1 2 2 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\GET AHEAD IN THE WORLD&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM20G : num 1 1 1 2 1 1 2 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\MORE JOB OPPORTUNITIES&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM20H : num 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\PARENTS THINK MATH IMPORTANT&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBM20I : num 1 1 1 2 1 1 2 1 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;MATH\\\\AGREE\\\\IMPORTANT TO DO WELL IN MATH&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBS21A : num 1 4 2 2 1 1 3 2 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;SCI\\\\AGREE\\\\ENJOY LEARNING SCIENCE&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBS21B : num 2 3 3 3 2 2 3 4 1 4 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;SCI\\\\AGREE\\\\WISH HAVE NOT TO STUDY SCIENCE&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBS21C : num 1 1 3 3 4 4 1 2 1 4 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;SCI\\\\AGREE\\\\SCIENCE IS BORING&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## $ BSBS21D : num 2 3 2 1 1 1 1 1 1 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;SCI\\\\AGREE\\\\LEARN INTERESTING THINGS&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:5] 1 2 3 4 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;Agree a lot&quot; &quot;Agree a little&quot; &quot;Disagree a little&quot; &quot;Disagree a lot&quot; ... ## [list output truncated] typeof(mydata) ## [1] &quot;list&quot; class(mydata) ## [1] &quot;data.frame&quot; dim(mydata) # alternatively, use `nrow()` and `ncol()` ## [1] 10221 436 length(mydata) # number of variables in a data frame; information output by`length()` function depends on the data structure ## [1] 436 names(mydata) # list variable names ## [1] &quot;IDCNTRY&quot; &quot;IDBOOK&quot; &quot;IDSCHOOL&quot; &quot;IDCLASS&quot; &quot;IDSTUD&quot; &quot;IDGRADE&quot; &quot;ITSEX&quot; &quot;ITADMINI&quot; &quot;ITLANG&quot; &quot;BSBG01&quot; &quot;BSBG03&quot; ## [12] &quot;BSBG04&quot; &quot;BSBG05&quot; &quot;BSBG06A&quot; &quot;BSBG06B&quot; &quot;BSBG06C&quot; &quot;BSBG06D&quot; &quot;BSBG06E&quot; &quot;BSBG06F&quot; &quot;BSBG06G&quot; &quot;BSBG06H&quot; &quot;BSBG06I&quot; ## [23] &quot;BSBG06J&quot; &quot;BSBG06K&quot; &quot;BSBG07A&quot; &quot;BSBG07B&quot; &quot;BSBG08&quot; &quot;BSBG09A&quot; &quot;BSBG09B&quot; &quot;BSBG10A&quot; &quot;BSBG10B&quot; &quot;BSBG11&quot; &quot;BSBG12&quot; ## [34] &quot;BSBG13A&quot; &quot;BSBG13B&quot; &quot;BSBG13C&quot; &quot;BSBG14A&quot; &quot;BSBG14B&quot; &quot;BSBG14C&quot; &quot;BSBG14D&quot; &quot;BSBG14E&quot; &quot;BSBG14F&quot; &quot;BSBG15A&quot; &quot;BSBG15B&quot; ## [45] &quot;BSBG15C&quot; &quot;BSBG15D&quot; &quot;BSBG15E&quot; &quot;BSBG15F&quot; &quot;BSBG15G&quot; &quot;BSBG16A&quot; &quot;BSBG16B&quot; &quot;BSBG16C&quot; &quot;BSBG16D&quot; &quot;BSBG16E&quot; &quot;BSBG16F&quot; ## [56] &quot;BSBG16G&quot; &quot;BSBG16H&quot; &quot;BSBG16I&quot; &quot;BSBM17A&quot; &quot;BSBM17B&quot; &quot;BSBM17C&quot; &quot;BSBM17D&quot; &quot;BSBM17E&quot; &quot;BSBM17F&quot; &quot;BSBM17G&quot; &quot;BSBM17H&quot; ## [67] &quot;BSBM17I&quot; &quot;BSBM18A&quot; &quot;BSBM18B&quot; &quot;BSBM18C&quot; &quot;BSBM18D&quot; &quot;BSBM18E&quot; &quot;BSBM18F&quot; &quot;BSBM18G&quot; &quot;BSBM18H&quot; &quot;BSBM18I&quot; &quot;BSBM18J&quot; ## [78] &quot;BSBM19A&quot; &quot;BSBM19B&quot; &quot;BSBM19C&quot; &quot;BSBM19D&quot; &quot;BSBM19E&quot; &quot;BSBM19F&quot; &quot;BSBM19G&quot; &quot;BSBM19H&quot; &quot;BSBM19I&quot; &quot;BSBM20A&quot; &quot;BSBM20B&quot; ## [89] &quot;BSBM20C&quot; &quot;BSBM20D&quot; &quot;BSBM20E&quot; &quot;BSBM20F&quot; &quot;BSBM20G&quot; &quot;BSBM20H&quot; &quot;BSBM20I&quot; &quot;BSBS21A&quot; &quot;BSBS21B&quot; &quot;BSBS21C&quot; &quot;BSBS21D&quot; ## [100] &quot;BSBS21E&quot; &quot;BSBS21F&quot; &quot;BSBS21G&quot; &quot;BSBS21H&quot; &quot;BSBS21I&quot; &quot;BSBS22A&quot; &quot;BSBS22B&quot; &quot;BSBS22C&quot; &quot;BSBS22D&quot; &quot;BSBS22E&quot; &quot;BSBS22F&quot; ## [111] &quot;BSBS22G&quot; &quot;BSBS22H&quot; &quot;BSBS22I&quot; &quot;BSBS22J&quot; &quot;BSBS23A&quot; &quot;BSBS23B&quot; &quot;BSBS23C&quot; &quot;BSBS23D&quot; &quot;BSBS23E&quot; &quot;BSBS23F&quot; &quot;BSBS23G&quot; ## [122] &quot;BSBS23H&quot; &quot;BSBS24A&quot; &quot;BSBS24B&quot; &quot;BSBS24C&quot; &quot;BSBS24D&quot; &quot;BSBS24E&quot; &quot;BSBS24F&quot; &quot;BSBS24G&quot; &quot;BSBS24H&quot; &quot;BSBS24I&quot; &quot;BSBM25AA&quot; ## [133] &quot;BSBS25AB&quot; &quot;BSBM25BA&quot; &quot;BSBS25BB&quot; &quot;BSBM26AA&quot; &quot;BSBS26AB&quot; &quot;BSBM26BA&quot; &quot;BSBS26BB&quot; &quot;BSBB21&quot; &quot;BSBB22A&quot; &quot;BSBB22B&quot; &quot;BSBB22C&quot; ## [144] &quot;BSBB22D&quot; &quot;BSBB22E&quot; &quot;BSBB22F&quot; &quot;BSBB22G&quot; &quot;BSBB22H&quot; &quot;BSBB22I&quot; &quot;BSBB23A&quot; &quot;BSBB23B&quot; &quot;BSBB23C&quot; &quot;BSBB23D&quot; &quot;BSBB23E&quot; ## [155] &quot;BSBB23F&quot; &quot;BSBB23G&quot; &quot;BSBB23H&quot; &quot;BSBB23I&quot; &quot;BSBB23J&quot; &quot;BSBB24A&quot; &quot;BSBB24B&quot; &quot;BSBB24C&quot; &quot;BSBB24D&quot; &quot;BSBB24E&quot; &quot;BSBB24F&quot; ## [166] &quot;BSBB24G&quot; &quot;BSBB24H&quot; &quot;BSBE25&quot; &quot;BSBE26A&quot; &quot;BSBE26B&quot; &quot;BSBE26C&quot; &quot;BSBE26D&quot; &quot;BSBE26E&quot; &quot;BSBE26F&quot; &quot;BSBE26G&quot; &quot;BSBE26H&quot; ## [177] &quot;BSBE26I&quot; &quot;BSBE27A&quot; &quot;BSBE27B&quot; &quot;BSBE27C&quot; &quot;BSBE27D&quot; &quot;BSBE27E&quot; &quot;BSBE27F&quot; &quot;BSBE27G&quot; &quot;BSBE27H&quot; &quot;BSBE27I&quot; &quot;BSBE27J&quot; ## [188] &quot;BSBE28A&quot; &quot;BSBE28B&quot; &quot;BSBE28C&quot; &quot;BSBE28D&quot; &quot;BSBE28E&quot; &quot;BSBE28F&quot; &quot;BSBE28G&quot; &quot;BSBE28H&quot; &quot;BSBC29&quot; &quot;BSBC30A&quot; &quot;BSBC30B&quot; ## [199] &quot;BSBC30C&quot; &quot;BSBC30D&quot; &quot;BSBC30E&quot; &quot;BSBC30F&quot; &quot;BSBC30G&quot; &quot;BSBC30H&quot; &quot;BSBC30I&quot; &quot;BSBC31A&quot; &quot;BSBC31B&quot; &quot;BSBC31C&quot; &quot;BSBC31D&quot; ## [210] &quot;BSBC31E&quot; &quot;BSBC31F&quot; &quot;BSBC31G&quot; &quot;BSBC31H&quot; &quot;BSBC31I&quot; &quot;BSBC31J&quot; &quot;BSBC32A&quot; &quot;BSBC32B&quot; &quot;BSBC32C&quot; &quot;BSBC32D&quot; &quot;BSBC32E&quot; ## [221] &quot;BSBC32F&quot; &quot;BSBC32G&quot; &quot;BSBC32H&quot; &quot;BSBP33&quot; &quot;BSBP34A&quot; &quot;BSBP34B&quot; &quot;BSBP34C&quot; &quot;BSBP34D&quot; &quot;BSBP34E&quot; &quot;BSBP34F&quot; &quot;BSBP34G&quot; ## [232] &quot;BSBP34H&quot; &quot;BSBP34I&quot; &quot;BSBP35A&quot; &quot;BSBP35B&quot; &quot;BSBP35C&quot; &quot;BSBP35D&quot; &quot;BSBP35E&quot; &quot;BSBP35F&quot; &quot;BSBP35G&quot; &quot;BSBP35H&quot; &quot;BSBP35I&quot; ## [243] &quot;BSBP35J&quot; &quot;BSBP36A&quot; &quot;BSBP36B&quot; &quot;BSBP36C&quot; &quot;BSBP36D&quot; &quot;BSBP36E&quot; &quot;BSBP36F&quot; &quot;BSBP36G&quot; &quot;BSBP36H&quot; &quot;BSBS37A&quot; &quot;BSBS37B&quot; ## [254] &quot;BSBS37C&quot; &quot;BSBS37D&quot; &quot;BSBS37E&quot; &quot;BSBS37F&quot; &quot;BSBS37G&quot; &quot;BSBS37H&quot; &quot;BSBS37I&quot; &quot;BSBM38AA&quot; &quot;BSBB38AB&quot; &quot;BSBE38AC&quot; &quot;BSBC38AD&quot; ## [265] &quot;BSBP38AE&quot; &quot;BSBM38BA&quot; &quot;BSBB38BB&quot; &quot;BSBE38BC&quot; &quot;BSBC38BD&quot; &quot;BSBP38BE&quot; &quot;BSBM39AA&quot; &quot;BSBS39AB&quot; &quot;BSBM39BA&quot; &quot;BSBS39BB&quot; &quot;ITACCOMM1&quot; ## [276] &quot;IDPOP&quot; &quot;IDGRADER&quot; &quot;BSDAGE&quot; &quot;TOTWGT&quot; &quot;HOUWGT&quot; &quot;SENWGT&quot; &quot;WGTADJ1&quot; &quot;WGTADJ2&quot; &quot;WGTADJ3&quot; &quot;WGTFAC1&quot; &quot;WGTFAC2&quot; ## [287] &quot;WGTFAC3&quot; &quot;JKZONE&quot; &quot;JKREP&quot; &quot;BSMMAT01&quot; &quot;BSMMAT02&quot; &quot;BSMMAT03&quot; &quot;BSMMAT04&quot; &quot;BSMMAT05&quot; &quot;BSSSCI01&quot; &quot;BSSSCI02&quot; &quot;BSSSCI03&quot; ## [298] &quot;BSSSCI04&quot; &quot;BSSSCI05&quot; &quot;BSMALG01&quot; &quot;BSMALG02&quot; &quot;BSMALG03&quot; &quot;BSMALG04&quot; &quot;BSMALG05&quot; &quot;BSMDAT01&quot; &quot;BSMDAT02&quot; &quot;BSMDAT03&quot; &quot;BSMDAT04&quot; ## [309] &quot;BSMDAT05&quot; &quot;BSMNUM01&quot; &quot;BSMNUM02&quot; &quot;BSMNUM03&quot; &quot;BSMNUM04&quot; &quot;BSMNUM05&quot; &quot;BSMGEO01&quot; &quot;BSMGEO02&quot; &quot;BSMGEO03&quot; &quot;BSMGEO04&quot; &quot;BSMGEO05&quot; ## [320] &quot;BSSCHE01&quot; &quot;BSSCHE02&quot; &quot;BSSCHE03&quot; &quot;BSSCHE04&quot; &quot;BSSCHE05&quot; &quot;BSSEAR01&quot; &quot;BSSEAR02&quot; &quot;BSSEAR03&quot; &quot;BSSEAR04&quot; &quot;BSSEAR05&quot; &quot;BSSBIO01&quot; ## [331] &quot;BSSBIO02&quot; &quot;BSSBIO03&quot; &quot;BSSBIO04&quot; &quot;BSSBIO05&quot; &quot;BSSPHY01&quot; &quot;BSSPHY02&quot; &quot;BSSPHY03&quot; &quot;BSSPHY04&quot; &quot;BSSPHY05&quot; &quot;BSMKNO01&quot; &quot;BSMKNO02&quot; ## [342] &quot;BSMKNO03&quot; &quot;BSMKNO04&quot; &quot;BSMKNO05&quot; &quot;BSMAPP01&quot; &quot;BSMAPP02&quot; &quot;BSMAPP03&quot; &quot;BSMAPP04&quot; &quot;BSMAPP05&quot; &quot;BSMREA01&quot; &quot;BSMREA02&quot; &quot;BSMREA03&quot; ## [353] &quot;BSMREA04&quot; &quot;BSMREA05&quot; &quot;BSSKNO01&quot; &quot;BSSKNO02&quot; &quot;BSSKNO03&quot; &quot;BSSKNO04&quot; &quot;BSSKNO05&quot; &quot;BSSAPP01&quot; &quot;BSSAPP02&quot; &quot;BSSAPP03&quot; &quot;BSSAPP04&quot; ## [364] &quot;BSSAPP05&quot; &quot;BSSREA01&quot; &quot;BSSREA02&quot; &quot;BSSREA03&quot; &quot;BSSREA04&quot; &quot;BSSREA05&quot; &quot;BSMIBM01&quot; &quot;BSMIBM02&quot; &quot;BSMIBM03&quot; &quot;BSMIBM04&quot; &quot;BSMIBM05&quot; ## [375] &quot;BSSIBM01&quot; &quot;BSSIBM02&quot; &quot;BSSIBM03&quot; &quot;BSSIBM04&quot; &quot;BSSIBM05&quot; &quot;BSBGHER&quot; &quot;BSDGHER&quot; &quot;BSBGSSB&quot; &quot;BSDGSSB&quot; &quot;BSBGSB&quot; &quot;BSDGSB&quot; ## [386] &quot;BSBGSLM&quot; &quot;BSDGSLM&quot; &quot;BSBGEML&quot; &quot;BSDGEML&quot; &quot;BSBGSCM&quot; &quot;BSDGSCM&quot; &quot;BSBGSVM&quot; &quot;BSDGSVM&quot; &quot;BSBGSLS&quot; &quot;BSDGSLS&quot; &quot;BSBGESL&quot; ## [397] &quot;BSDGESL&quot; &quot;BSBGSCS&quot; &quot;BSDGSCS&quot; &quot;BSBGSVS&quot; &quot;BSDGSVS&quot; &quot;BSBGSLB&quot; &quot;BSDGSLB&quot; &quot;BSBGEBL&quot; &quot;BSDGEBL&quot; &quot;BSBGSCB&quot; &quot;BSDGSCB&quot; ## [408] &quot;BSBGSLE&quot; &quot;BSDGSLE&quot; &quot;BSBGEEL&quot; &quot;BSDGEEL&quot; &quot;BSBGSCE&quot; &quot;BSDGSCE&quot; &quot;BSBGSLC&quot; &quot;BSDGSLC&quot; &quot;BSBGECL&quot; &quot;BSDGECL&quot; &quot;BSBGSCC&quot; ## [419] &quot;BSDGSCC&quot; &quot;BSBGSLP&quot; &quot;BSDGSLP&quot; &quot;BSBGEPL&quot; &quot;BSDGEPL&quot; &quot;BSBGSCP&quot; &quot;BSDGSCP&quot; &quot;BSDG06S&quot; &quot;BSDGEDUP&quot; &quot;BSDMLOWP&quot; &quot;BSDSLOWP&quot; ## [430] &quot;BSDMWKHW&quot; &quot;BSDSWKHS&quot; &quot;BSDBWKHB&quot; &quot;BSDCWKHC&quot; &quot;BSDPWKHP&quot; &quot;BSDEWKHE&quot; &quot;VERSION&quot; Field, Miles, and Field (2012) uses R Commander - a graphical user interface to R. We will not use it. The example data is the USA student dataset from TIMSS 2015. "],["review-of-basic-statistics.html", "Chapter 2 Review of Basic Statistics 2.1 Calculating Variance and Covariance 2.2 Use R for Basic Statistics 2.3 Use R for Graphing Data 2.4 Use R to Graph a Correlation Matrix 2.5 Use R to Generate Random Data", " Chapter 2 Review of Basic Statistics library(rio); library(psych); library(Hmisc); library(ggplot2); library(reshape2); library(QuantPsyc); library(MASS); library(lavaan) 2.1 Calculating Variance and Covariance 2.1.1 Covariance population: \\[{{\\sigma }_{\\text{XY}}}=\\frac{\\sum{(X-{{\\mu }_{\\text{X}}})(Y-{{\\mu }_{\\text{Y}}})}}{{{N}_{pop}}}\\] sample: \\[{{s}_{XY}}={{\\hat{\\sigma }}_{XY}}=Cov(X,Y)=\\frac{\\sum{(X-\\bar{X})(Y-\\bar{Y})}}{(n-1)}\\] 2.1.2 Correlation population: \\[{{\\rho }_{\\text{XY}}}=\\frac{{{\\sigma }_{\\text{XY}}}}{\\sqrt{{{\\sigma }_{\\text{X}}}^{2}{{\\sigma }_{\\text{Y}}}^{2}}}\\] sample: \\[{{r}_{\\text{XY}}}={{\\hat{\\rho }}_{\\text{XY}}}=\\frac{Cov(X,Y)}{\\sqrt{{{s}_{\\text{X}}}^{2}{{s}_{\\text{Y}}}^{2}}}\\] 2.1.3 Using Matrix Algebra Suppose we have a 5 by 2 data matrix X (2 variables and 5 participants, for example). $${} = $$ First, we calculate the deviation score matrix \\({{\\bf{X}}_{\\bf{d}}}\\). \\[{{\\mathbf{X}}_{d}}\\text{=}\\mathbf{X-\\bar{X}}=\\left[ \\begin{matrix} 1 &amp; 3 \\\\ 4 &amp; -5 \\\\ 1 &amp; 7 \\\\ 3 &amp; 2 \\\\ 7 &amp; -1 \\\\ \\end{matrix} \\right]-\\left[ \\begin{matrix} 3.2 &amp; 1.2 \\\\ 3.2 &amp; 1.2 \\\\ 3.2 &amp; 1.2 \\\\ 3.2 &amp; 1.2 \\\\ 3.2 &amp; 1.2 \\\\ \\end{matrix} \\right]=\\left[ \\begin{matrix} -2.2 &amp; 1.8 \\\\ 0.8 &amp; -6.2 \\\\ -2.2 &amp; 5.8 \\\\ -0.2 &amp; 0.8 \\\\ 3.8 &amp; -2.2 \\\\ \\end{matrix} \\right]\\] Next, we multiple the transpose of \\({{\\bf{X}}_{\\bf{d}}}\\) ( \\({{\\bf{X}}_{\\bf{d}}}^{\\bf{&#39;}}\\) ) by \\({{\\bf{X}}_{\\bf{d}}}\\) itself using matrix operations. We will get the deviation SSCP (sums of squares and cross products) matrix. Deviation SSCP matrix: \\[{{\\mathbf{X}}_{\\mathbf{d}}}&#39;{{\\mathbf{X}}_{\\mathbf{d}}}=\\sum{{{x}_{i}}{{x}_{j}}=\\left[ \\begin{matrix} \\sum{x_{1}^{2}} &amp; \\sum{{{x}_{1}}{{x}_{2}}} \\\\ \\sum{{{x}_{2}}{{x}_{1}}} &amp; \\sum{x_{2}^{2}} \\\\ \\end{matrix} \\right]}=\\left[ \\begin{matrix} 24.8 &amp; -30.2 \\\\ -30.2 &amp; 80.8 \\\\ \\end{matrix} \\right]\\] (Note: Lower case \\({{x}_{i}}\\)s represent deviation scores.) Since variance of of the first variable \\(X_1\\) is \\(\\frac{1}{N-1}\\sum{x_{1}^{2}}\\) and the covariance between \\(X_1\\) and \\(X_2\\) is \\(\\frac{1}{N-1}\\sum{{{x}_{1}}{{x}_{2}}}\\), the variance and covariance matrix (usually denoted by S) can be obtained by multiplying \\(\\frac{1}{N-1}\\) to the deviation score SSCP: Variance and Covariance matrix: \\[\\mathbf{S}=\\frac{1}{5-1}\\left[ \\begin{matrix} 24.8 &amp; -30.2 \\\\ -30.2 &amp; 80.8 \\\\ \\end{matrix} \\right]=\\left[ \\begin{matrix} 6.2 &amp; -7.55 \\\\ -7.55 &amp; 20.2 \\\\ \\end{matrix} \\right]\\] $${}$$ Sample Covariance Matrix S $${}$$ Sample Correlation R $${}$$ Elements in sample covariance matrix divided by \\({s_i}{s_j}\\) would result in the sample correlation matrix. Elements in sample correlation matrix multiplied by \\({s_i}{s_j}\\) would result in the sample covariance matrix. 2.2 Use R for Basic Statistics 2.2.1 Import and export data mydata &lt;- import(&quot;data/example.sav&quot;) # head(mydata) # view first 6 observations export(mydata, file = &quot;data/dataset_example.csv&quot;) export(mydata, file = &quot;data/dataset_example.xlsx&quot;) 2.2.2 Several functions for basic data management names(mydata)[names(mydata) == &quot;ITSEX&quot;] &lt;- &quot;gender&quot; #rename a variable names(mydata)[names(mydata) == &quot;BSMMAT01&quot;] &lt;- &quot;math&quot; #rename another variable for math score names(mydata)[names(mydata) == &quot;BSBGHER&quot;] &lt;- &quot;resource&quot; #education resource names(mydata)[names(mydata) == &quot;BSBGSCM&quot;] &lt;- &quot;confidence&quot; #confidence in math names(mydata)[names(mydata) == &quot;BSBGSVM&quot;] &lt;- &quot;value&quot; #valuing math names(mydata)[names(mydata) == &quot;BSBGSSB&quot;] &lt;- &quot;belonging&quot; #sense of belonging var &lt;- c(&quot;gender&quot;, &quot;math&quot;, &quot;resource&quot;, &quot;confidence&quot;, &quot;value&quot;, &quot;belonging&quot;) dat1 &lt;- mydata[, var] #subset data dat2 &lt;- na.omit(dat1) #listwise deletion dat1$gender_c &lt;- factor(dat1$gender) #create a categorical variable for `gender` levels(dat1$gender_c) &lt;- c(&quot;f&quot;, &quot;m&quot;) #change levels for `gender` variable 2.2.3 Some descriptive statistics summary(dat1) table(dat1$gender_c) #compute frequencies for categorical variables psych::describe(dat1) #compute descriptives for quantitative variables. Need to install and load the `psych` package attach(dat1) # save some typing. Alternatively, `math &lt;- dat1$BSMMAT01` to create a separate vector. ## The following object is masked _by_ .GlobalEnv: ## ## gender mean(math, na.rm = TRUE) median(math, na.rm = TRUE) var(math, na.rm = TRUE) sd(math, na.rm = TRUE) quantile(math, na.rm = TRUE) detach(dat1) 2.2.4 Check normality shapiro.test(dat1$math) You got an error message. The Shapiro-Wilk test is for small to moderate samples. set.seed(12345) #for reproducibility selected &lt;- sample(nrow(dat1), 100) #randomly selection 100 numbers from 1 through the sample size dat1_small &lt;- dat1[selected,] #create a smaller dataset of 100 students shapiro.test(dat1_small$math) ## ## Shapiro-Wilk normality test ## ## data: dat1_small$math ## W = 0.97872, p-value = 0.1056 2.2.5 Check linearity plot(dat2$confidence, dat2$math) abline(lm(math ~ confidence, data =dat2), col = &quot;red&quot;) 2.2.6 Pearsons product moment correlation Correlation between two variables, hypothesis test, and confidence interval. cor.test(dat1$resource, dat1$math) ## ## Pearson&#39;s product-moment correlation ## ## data: dat1$resource and dat1$math ## t = 42.923, df = 10116, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3759021 0.4088708 ## sample estimates: ## cor ## 0.3925126 All pairwise correlations. cor(dat1[,var], use = &quot;pairwise.complete.obs&quot;) #for numeric variables only; &quot;gender&quot; is still a numeric variable! ## gender math resource confidence value belonging ## gender 1.000000000 0.01305597 -0.0281559 0.0719095 0.03690314 0.007168246 ## math 0.013055968 1.00000000 0.3925126 0.4280530 0.16511215 0.221113350 ## resource -0.028155898 0.39251256 1.0000000 0.1512655 0.10208246 0.152041678 ## confidence 0.071909498 0.42805300 0.1512655 1.0000000 0.37535602 0.280950896 ## value 0.036903141 0.16511215 0.1020825 0.3753560 1.00000000 0.366637734 ## belonging 0.007168246 0.22111335 0.1520417 0.2809509 0.36663773 1.000000000 round(cor(dat1[,var], use = &quot;pairwise.complete.obs&quot;), 2) #round to 2 decimal places ## gender math resource confidence value belonging ## gender 1.00 0.01 -0.03 0.07 0.04 0.01 ## math 0.01 1.00 0.39 0.43 0.17 0.22 ## resource -0.03 0.39 1.00 0.15 0.10 0.15 ## confidence 0.07 0.43 0.15 1.00 0.38 0.28 ## value 0.04 0.17 0.10 0.38 1.00 0.37 ## belonging 0.01 0.22 0.15 0.28 0.37 1.00 Correlations and significance levels for the correlation matrix. Need to install and load the Hmisc package. Need to coerce from dataframe to matrix to get both a correlation matrix and p-values. install.packages(&quot;Hmisc&quot;) library(Hmisc) df &lt;- as.matrix(dat1[,var]) # numeric variables only rcorr(df) ## gender math resource confidence value belonging ## gender 1.00 0.01 -0.03 0.07 0.04 0.01 ## math 0.01 1.00 0.39 0.43 0.17 0.22 ## resource -0.03 0.39 1.00 0.15 0.10 0.15 ## confidence 0.07 0.43 0.15 1.00 0.38 0.28 ## value 0.04 0.17 0.10 0.38 1.00 0.37 ## belonging 0.01 0.22 0.15 0.28 0.37 1.00 ## ## n ## gender math resource confidence value belonging ## gender 10217 10217 10117 10052 10014 10111 ## math 10217 10221 10118 10053 10015 10112 ## resource 10117 10118 10118 10017 9978 10073 ## confidence 10052 10053 10017 10053 10010 10035 ## value 10014 10015 9978 10010 10015 9998 ## belonging 10111 10112 10073 10035 9998 10112 ## ## P ## gender math resource confidence value belonging ## gender 0.1870 0.0046 0.0000 0.0002 0.4711 ## math 0.1870 0.0000 0.0000 0.0000 0.0000 ## resource 0.0046 0.0000 0.0000 0.0000 0.0000 ## confidence 0.0000 0.0000 0.0000 0.0000 0.0000 ## value 0.0002 0.0000 0.0000 0.0000 0.0000 ## belonging 0.4711 0.0000 0.0000 0.0000 0.0000 2.3 Use R for Graphing Data 2.3.1 Use plot() Use browseURL(\"https://datalab.cc/rcolors\") to view different colors and methods to refer to them. Use ?palette for palettes in R; use palette() for current palette plot(dat1$value, dat1$math) plot(dat1$value, jitter(dat1$math)) # add some random noise to Y plot(dat1$value, jitter(dat1$math), xlab = &quot;Valuing Math&quot;, ylab = &quot;Math Score&quot;) # change labels plot(dat1$value, jitter(dat1$math), xlab = &quot;Valuing Math&quot;, ylab = &quot;Math Score&quot;, col = &quot;red&quot;) # change color 2.3.2 Use ggplot2 package Read Chapter 4 of textbook Field, Miles, Field (2012). install.packages(&quot;ggplot2&quot;) library(ggplot2) ggplot(dat1, aes(math)) + theme(legend.position = &quot;none&quot;) + geom_histogram(aes(y = ..density..), colour = &quot;black&quot;, fill = &quot;white&quot;) + labs(x = &quot;Math&quot;, y = &quot;Density&quot;) + stat_function(fun = dnorm, args = list(mean = mean(dat1$math, na.rm = TRUE), sd = sd(dat1$math, na.rm = TRUE)), colour = &quot;red&quot;, size = 1) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 2.4 Use R to Graph a Correlation Matrix 2.4.1 Use plot() plot(dat1[,var]) 2.4.2 Use ggplot() install.packages(&quot;reshape2&quot;) library(reshape2) (cor_matrix &lt;- cor(dat1[,var], use = &quot;pairwise.complete.obs&quot;)) ## gender math resource confidence value belonging ## gender 1.000000000 0.01305597 -0.0281559 0.0719095 0.03690314 0.007168246 ## math 0.013055968 1.00000000 0.3925126 0.4280530 0.16511215 0.221113350 ## resource -0.028155898 0.39251256 1.0000000 0.1512655 0.10208246 0.152041678 ## confidence 0.071909498 0.42805300 0.1512655 1.0000000 0.37535602 0.280950896 ## value 0.036903141 0.16511215 0.1020825 0.3753560 1.00000000 0.366637734 ## belonging 0.007168246 0.22111335 0.1520417 0.2809509 0.36663773 1.000000000 (melt_cor_matrix &lt;- melt(cor_matrix)) ## Warning in melt(cor_matrix): The melt generic in data.table has been passed a matrix and will attempt to redirect to the relevant ## reshape2 method; please note that reshape2 is deprecated, and this redirection is now deprecated as well. To continue using melt methods ## from reshape2 while both libraries are attached, e.g. melt.list, you can prepend the namespace like reshape2::melt(cor_matrix). In the ## next version, this warning will become an error. ## Var1 Var2 value ## 1 gender gender 1.000000000 ## 2 math gender 0.013055968 ## 3 resource gender -0.028155898 ## 4 confidence gender 0.071909498 ## 5 value gender 0.036903141 ## 6 belonging gender 0.007168246 ## 7 gender math 0.013055968 ## 8 math math 1.000000000 ## 9 resource math 0.392512564 ## 10 confidence math 0.428053003 ## 11 value math 0.165112146 ## 12 belonging math 0.221113350 ## 13 gender resource -0.028155898 ## 14 math resource 0.392512564 ## 15 resource resource 1.000000000 ## 16 confidence resource 0.151265456 ## 17 value resource 0.102082458 ## 18 belonging resource 0.152041678 ## 19 gender confidence 0.071909498 ## 20 math confidence 0.428053003 ## 21 resource confidence 0.151265456 ## 22 confidence confidence 1.000000000 ## 23 value confidence 0.375356020 ## 24 belonging confidence 0.280950896 ## 25 gender value 0.036903141 ## 26 math value 0.165112146 ## 27 resource value 0.102082458 ## 28 confidence value 0.375356020 ## 29 value value 1.000000000 ## 30 belonging value 0.366637734 ## 31 gender belonging 0.007168246 ## 32 math belonging 0.221113350 ## 33 resource belonging 0.152041678 ## 34 confidence belonging 0.280950896 ## 35 value belonging 0.366637734 ## 36 belonging belonging 1.000000000 ggplot(data = melt_cor_matrix, aes(x = Var1, y = Var2, fill = value)) + geom_tile() 2.5 Use R to Generate Random Data 2.5.1 Sampling distribution Generate data. size &lt;- 50 sample_means &lt;- NULL for (i in 1:1000) { samp &lt;- rnorm(size, 10, 5) sample_means[i] &lt;- mean(samp) } Calculate the mean and SD of the sampling distribution. mean(sample_means) # population mean is 10 ## [1] 10.007 sd(sample_means) # SD of the sampling distribution ## [1] 0.708293 Use hist() to graph the sample means for sampling distribution. hist(sample_means, breaks = 50, main = &quot;N= 50&quot;, xlab = &quot;Sample Means&quot;, probability = TRUE) Use ggplot() to graph the sample means for sampling distribution. df_sample_means &lt;- data.frame(i = 1:1000, sample_means) # data for ggplot has to be a data frame. ggplot(df_sample_means, aes(x = sample_means)) + geom_histogram(aes(y = ..density..), colour = &quot;black&quot;, fill = &quot;white&quot;) + labs(x = &quot;Sample Means&quot;, y = &quot;Density&quot;) + stat_function(fun = dnorm, args = list(mean = mean(df_sample_means$sample_means, na.rm = TRUE), sd = sd(df_sample_means$sample_means, na.rm = TRUE)), colour = &quot;red&quot;, size = 1) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 2.5.2 Simulate data from a model ####Simulate data based on a regression model#### Simple Linear Regression with 0.7 correlation between x and y x &lt;- rnorm(1000, 10, 2) Zx &lt;- scale(x) Zy &lt;- .7*Zx + rnorm(1000, 0, sqrt(1-(.7^2))) y &lt;- 3*Zy + 5 dat1 &lt;- data.frame(x,y) cor(x,y) ## [,1] ## [1,] 0.693814 summary(lm(Zy ~ 0 + Zx)) ## ## Call: ## lm(formula = Zy ~ 0 + Zx) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.93796 -0.49421 -0.01251 0.51644 2.37284 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## Zx 0.71276 0.02341 30.44 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.74 on 999 degrees of freedom ## Multiple R-squared: 0.4813, Adjusted R-squared: 0.4807 ## F-statistic: 926.8 on 1 and 999 DF, p-value: &lt; 2.2e-16 summary(lm(y ~x)) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7646 -1.4334 0.0117 1.5986 7.1678 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.85417 0.36189 -16.18 &lt;2e-16 *** ## x 1.07541 0.03533 30.44 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.221 on 998 degrees of freedom ## Multiple R-squared: 0.4814, Adjusted R-squared: 0.4809 ## F-statistic: 926.3 on 1 and 998 DF, p-value: &lt; 2.2e-16 QuantPsyc::lm.beta(lm(y ~x)) ## x ## 0.693814 Multiple Linear Regression with with known population standardized regression coefficients x1 &lt;- rnorm(1000, 10, 2) x2 &lt;- rnorm(1000, 5, 1) x3 &lt;- x1*x2 Zx1 &lt;- scale(x1) Zx2 &lt;- scale(x2) Zx3 &lt;- scale(x3) Zy &lt;- .7*Zx1 + .5*Zx2 + 2*Zx3 + rnorm(1000, 0, sqrt(1-(.49+.25+.04))) y &lt;- 3*Zy + 5 dat2 &lt;- data.frame(x1, x2, x3, y) cor(dat2) ## x1 x2 x3 y ## x1 1.00000000 -0.04286003 0.6913052 0.7157652 ## x2 -0.04286003 1.00000000 0.6791463 0.6401593 ## x3 0.69130517 0.67914628 1.0000000 0.9848538 ## y 0.71576517 0.64015925 0.9848538 1.0000000 summary(lm(Zy ~ 0 + x1 + x2 + x3)) ## ## Call: ## lm(formula = Zy ~ 0 + x1 + x2 + x3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.66804 -0.42986 -0.03501 0.40424 2.11467 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x1 -0.964805 0.011053 -87.29 &lt;2e-16 *** ## x2 -2.076678 0.021641 -95.96 &lt;2e-16 *** ## x3 0.402948 0.003071 131.20 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6679 on 997 degrees of freedom ## Multiple R-squared: 0.9463, Adjusted R-squared: 0.9462 ## F-statistic: 5859 on 3 and 997 DF, p-value: &lt; 2.2e-16 summary(lm(y ~ x1 + x2 + x3)) ## ## Call: ## lm(formula = y ~ x1 + x2 + x3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.7735 -1.0049 -0.0068 0.9343 4.8428 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -35.01244 1.26879 -27.595 &lt; 2e-16 *** ## x1 0.98700 0.12530 7.877 8.72e-15 *** ## x2 1.39397 0.24608 5.665 1.93e-08 *** ## x3 0.46847 0.02437 19.227 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.418 on 996 degrees of freedom ## Multiple R-squared: 0.9731, Adjusted R-squared: 0.9731 ## F-statistic: 1.203e+04 on 3 and 996 DF, p-value: &lt; 2.2e-16 QuantPsyc::lm.beta(lm(y ~ x1 + x2 + x3)) ## x1 x2 x3 ## 0.2191362 0.1551256 0.7280108 Multiple Linear Regression with known population unstandardized regression coefficients b0 &lt;- 6 b1 &lt;- -3 b2 &lt;- 4 sd &lt;- 2 # sd for the error distribution x1 &lt;- rnorm(1000, 10, 2) x2 &lt;- rnorm(1000, 5, 1) y &lt;- b0 + b1*x1 + b2*x2 + rnorm(1000, 0, sd) dat3 &lt;- data.frame(x1, x2, y) summary(lm(y ~ x1 + x2)) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.0986 -1.3831 0.0726 1.3308 6.0776 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.75496 0.46551 12.36 &lt;2e-16 *** ## x1 -2.93915 0.03371 -87.19 &lt;2e-16 *** ## x2 3.92356 0.06521 60.17 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.034 on 997 degrees of freedom ## Multiple R-squared: 0.916, Adjusted R-squared: 0.9159 ## F-statistic: 5439 on 2 and 997 DF, p-value: &lt; 2.2e-16 MUltivariate Data Structure with known correlations sigma &lt;- matrix(c(1.0, 0.7, 0.5, 0.3, 0.7, 1.0, 0.4, 0.4, 0.5, 0.4, 1.0, 0.5, 0.3, 0.4, 0.5, 1.0), nrow = 4) dat4 &lt;- data.frame(MASS::mvrnorm(n = 1000, mu = rep(0, 4), Sigma = sigma, empirical = TRUE)) #replicate the exact correlations among the simulated variables summary(dat4) ## X1 X2 X3 X4 ## Min. :-2.92187 Min. :-3.12370 Min. :-3.24050 Min. :-3.65926 ## 1st Qu.:-0.67253 1st Qu.:-0.69337 1st Qu.:-0.68981 1st Qu.:-0.65546 ## Median :-0.06499 Median :-0.02844 Median : 0.04108 Median :-0.03206 ## Mean : 0.00000 Mean : 0.00000 Mean : 0.00000 Mean : 0.00000 ## 3rd Qu.: 0.69003 3rd Qu.: 0.67110 3rd Qu.: 0.68982 3rd Qu.: 0.67722 ## Max. : 3.45489 Max. : 2.97990 Max. : 3.14553 Max. : 3.55700 cor(dat4) ## X1 X2 X3 X4 ## X1 1.0 0.7 0.5 0.3 ## X2 0.7 1.0 0.4 0.4 ## X3 0.5 0.4 1.0 0.5 ## X4 0.3 0.4 0.5 1.0 Confirmatory Factor Analysis Model pop.model &lt;- &#39; f1 =~ 0.9*x1 + .7*x2 + .5*x3 + .3*x4 f2 =~ 0.9*x5 + .7*x6 + .5*x7 + .3*x8 f1 ~~ 1*f1 f1 ~~ 0.5*f2&#39; dat5 &lt;- lavaan::simulateData(model = pop.model, sample.nobs = 1000) summary(dat5) ## x1 x2 x3 x4 x5 x6 x7 ## Min. :-3.83863 Min. :-3.81556 Min. :-3.54199 Min. :-3.28999 Min. :-4.03083 Min. :-4.670419 Min. :-3.666570 ## 1st Qu.:-0.81480 1st Qu.:-0.77514 1st Qu.:-0.68091 1st Qu.:-0.72620 1st Qu.:-0.83746 1st Qu.:-0.809362 1st Qu.:-0.761440 ## Median : 0.04831 Median : 0.04984 Median : 0.06221 Median : 0.03548 Median :-0.01814 Median :-0.009423 Median : 0.001263 ## Mean : 0.07755 Mean : 0.04314 Mean : 0.05840 Mean : 0.01341 Mean : 0.06853 Mean :-0.018588 Mean : 0.025038 ## 3rd Qu.: 0.96484 3rd Qu.: 0.84818 3rd Qu.: 0.83818 3rd Qu.: 0.71464 3rd Qu.: 1.04531 3rd Qu.: 0.858382 3rd Qu.: 0.777438 ## Max. : 5.03915 Max. : 4.15661 Max. : 3.56051 Max. : 3.22474 Max. : 4.73594 Max. : 3.801685 Max. : 4.878669 ## x8 ## Min. :-3.797992 ## 1st Qu.:-0.682778 ## Median : 0.018717 ## Mean :-0.005384 ## 3rd Qu.: 0.680474 ## Max. : 3.741254 analysis.model &lt;- &#39; f1 =~ x1 + x2 + x3 + x4 f2 =~ x5 + x6 + x7 +x8 f1 ~~ f2&#39; fit.model &lt;- lavaan::sem(model = analysis.model, data = dat5, std.lv = TRUE) summary(fit.model) ## lavaan 0.6-8 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 17 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 24.440 ## Degrees of freedom 19 ## P-value (Chi-square) 0.180 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## f1 =~ ## x1 0.886 0.057 15.663 0.000 ## x2 0.629 0.049 12.919 0.000 ## x3 0.482 0.045 10.754 0.000 ## x4 0.337 0.043 7.883 0.000 ## f2 =~ ## x5 0.943 0.054 17.552 0.000 ## x6 0.734 0.048 15.264 0.000 ## x7 0.566 0.043 13.047 0.000 ## x8 0.310 0.040 7.817 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## f1 ~~ ## f2 0.445 0.046 9.780 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 0.915 0.087 10.485 0.000 ## .x2 1.062 0.063 16.767 0.000 ## .x3 1.056 0.055 19.259 0.000 ## .x4 1.057 0.051 20.908 0.000 ## .x5 0.938 0.084 11.122 0.000 ## .x6 1.042 0.066 15.827 0.000 ## .x7 1.008 0.054 18.581 0.000 ## .x8 0.985 0.046 21.259 0.000 ## f1 1.000 ## f2 1.000 "],["correlation-1.html", "Chapter 3 Correlation 3.1 Variance and Covariance 3.2 Pearson Product-Moment Correlation (Pearson Correlation) 3.3 Variations of the Pearson Product-Moment Correlation 3.4 An Example", " Chapter 3 Correlation library(rio); library(psych); library(Hmisc); library(ggplot2); library(reshape2) 3.1 Variance and Covariance Statistical techniques are used to help explain the variation in variables. Variance is an expression that indicates the amount of spread or dispersion of scores in a distribution. Variance is the average of the squared deviations around the mean. Researchers are typically interested in the covariation among variables (e.g., What happens to the likelihood of success in a graduate program as students IQs vary?; What happens to high school drop out rate as parental income varies?). How do two variables co-vary with each other? Recall the formula for Variance: \\[\\text{VAR}=s_{x}^{2}\\ =\\frac{\\sum\\limits_{i=1}^{N}{{{({{X}_{i}}-\\bar{X})}^{2}}}}{N-1}=\\frac{\\sum{{{x}^{2}}}}{N-1}=\\frac{\\sum\\limits_{i=1}^{N}{({{X}_{i}}-\\bar{X})({{X}_{i}}-\\bar{X})}}{N-1}\\] Variance of a variable can be thought of as its covariance with itself. Covariation of a set of scores from its mean. \\[\\text{Covariance}={{s}_{xy}}=\\frac{\\sum\\limits_{i=1}^{N}{({{X}_{i}}-\\bar{X})({{Y}_{i}}-\\bar{Y})}}{N-1}=\\frac{\\sum{xy}}{N-1}\\] Covariation of 2 sets of scores from their respective means. The covariance of X and Y scores may be expressed independently of their respective standard deviations by dividing by the product of their respective standard deviations. This standardizes the deviation scores and puts them on the same scale with the same standard deviation. This places the covariance of X and Y on a scale from -1.0 to +1.0 and gives us an index of the relationship between X and Y that is not affected by the differences in the individual variability of X and Y. This index is the Pearson Product-Moment Correlation: \\[{{r}_{xy}}=\\frac{\\sum\\limits_{i=1}^{N}{({{X}_{i}}-\\bar{X})({{Y}_{i}}-\\bar{Y})/(N-1)}}{{{s}_{x}}{{s}_{y}}}=\\frac{\\sum{xy/(N-1)}}{{{s}_{x}}{{s}_{y}}}\\] where \\(x=(X-\\bar{X})\\) and \\(y=(Y-\\bar{Y})\\); and \\({{s}_{x}}\\) and \\({{s}_{y}}=\\) the standard deviation of X and Y, respectively. 3.2 Pearson Product-Moment Correlation (Pearson Correlation) An index that measures on a scale from -1.0 to +1.0 the degree to which two groups of scores are related. Expressed as a proportion of the maximum amount of variation possible that is due to the association of X and Y. \\[{{r}_{xy}}=\\frac{\\sum\\limits_{i=1}^{N}{({{X}_{i}}-\\bar{X})({{Y}_{i}}-\\bar{Y})/(N-1)}}{{{s}_{x}}{{s}_{y}}}\\] \\(X\\) \\(Y\\) \\(X-\\bar X\\) \\(Y-\\bar Y\\) \\((X - \\bar X)^2\\) \\((Y - \\bar Y)^2\\) \\((X - \\bar X)(Y - \\bar Y)\\) 1 1 -3.57 -2.86 12.74 8.18 10.21 1 3 -3.57 -0.86 12.74 0.74 3.07 3 2 -1.57 -1.86 2.46 3.46 2.92 4 5 -0.57 1.14 0.32 1.30 -0.65 6 4 1.43 0.14 2.04 0.02 0.20 7 5 2.43 1.14 5.90 1.30 2.77 10 7 5.43 3.14 29.48 9.86 17.05 \\(\\Sigma\\) 32 27 0.01 -0.02 65.68 24.86 35.57 \\(\\bar X = 4.57\\), \\(\\bar Y = 3.86\\), \\(s_x = 3.31\\), \\(s_y = 2.03\\) \\[{{r}_{xy}}=\\frac{\\sum\\limits_{i=1}^{N}{({{X}_{i}}-\\bar{X})({{Y}_{i}}-\\bar{Y})/(N-1)}}{{{s}_{x}}{{s}_{y}}}=\\frac{35.57/(7-1)}{(3.31)(2.03)}=\\frac{5.93}{6.72}=.88\\] ### How to describe a Pearson correlation? Positive relationship: [As X increases, Y increases] OR [As X decreases, Y decreases]. Same direction of movement. Negative relationship: [As X increases, Y decreases] OR [As X decreases, Y increases]. Opposite directions of movement. Strength of the relationship between two variables is not represented by the sign of the correlation, but is represented by the absolute value of the correlation coefficient (i.e., 0 represents no relationship; the closer the correlation is to either -1.0 or +1.0, the stronger the relationship). A correlation does not imply causation. A correlation only indicates that an association or a relationship between two variables exists. For the Pearson Product-Moment Correlation to accurately reflect the correlation in the population, both X and Y must be continuous variables and normally distributed in the population. 3.2.1 Test that the population correlation equals zero Null Hypothesis: \\({{\\rho }_{xy}}=0\\) Alternative Hypothesis: \\({{\\rho }_{xy}}\\ne 0\\). Significance testing of the correlation coefficient can be done using several approaches. t statistic: \\(t=\\frac{{{r}_{xy}}\\sqrt{n-2}}{\\sqrt{1-r_{xy}^{2}}}\\) with df = n - 2. Critical value at alpha = .05 obtained from Distribution of t Table: 2.571.(In R, use qt(0.975, 5)). Our obtained t-value (4.19) is greater than the critical value (2.571), thus we reject the null that the correlation is equal to zero. There is a strong, positive, and significant relationship between the two variables, r(5) = .88, p &lt; .05. Use Excel function T.DIST.2T to obtain the probability that we will observe a more extreme correlation coefficient than that from the data, if \\({{\\rho }_{xy}}=0\\). In Excel, type =T.DIST.2T(4.19,5) (without quotation marks) in an empty cell, and hit Enter on your keyboard. Excel will give a value of .008572, which is less than alpha = .05. (NOTE: in Excel 2007 or earlier versions, type =TDIST(4.19,5,2)) x &lt;- c(1, 1, 3, 4, 6, 7, 10) y &lt;- c(1, 3, 2, 5, 4, 5, 7) cor.test(x,y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 4.1455, df = 5, p-value = 0.008949 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3768242 0.9821977 ## sample estimates: ## cor ## 0.8801279 3.2.2 Check normality shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.93761, p-value = 0.6173 shapiro.test(y) ## ## Shapiro-Wilk normality test ## ## data: y ## W = 0.97777, p-value = 0.948 3.2.3 Check for linearity with scatterplot using plot() plot(x,y) abline(lm(y ~ x)) using ggplot() df &lt;- data.frame(x,y) ggplot(data = df, mapping = aes(x = x, y = y)) + geom_point(color = &quot;red&quot;, size = 2) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.2.4 Factors that affect the Pearson Correlation Outliers Heterogeneous Subsamples Curvilinear Relationships Restriction of Range Restriction of Range 3.2.5 Test that the population correlation equals a certain value Null Hypothesis: \\({{\\rho }_{xy}}={{\\rho }_{0}}\\), where \\({{\\rho }_{0}}\\) is a specified value. Alternative Hypothesis: \\({{\\rho }_{xy}}\\ne {{\\rho }_{0}}\\). Fishers Z transformation: \\[{{z}_{r}}=\\frac{1}{2}\\ln \\frac{1+r}{1-r}\\] \\[\\text{S}{{\\text{E}}_{Z}}=\\frac{1}{\\sqrt{n-3}}\\] \\[z=\\frac{z_{r}^{{}}-z_{{{\\rho }_{0}}}^{{}}}{S{{E}_{z}}}\\] Example: \\[\\begin{align} &amp; {{r}_{xy}}=.66\\quad \\quad {{z}_{r}}=\\frac{1}{2}\\ln \\frac{1+.66}{1-.66}=0.7928\\quad \\quad S{{E}_{z}}=\\frac{1}{\\sqrt{30-3}}=0.19245 \\\\ &amp; {{\\rho }_{0}}=.85\\quad \\quad z_{{{\\rho }_{0}}}^{{}}=\\frac{1}{2}\\ln \\frac{1+.85}{1-.85}=1.2562\\quad \\quad \\\\ \\end{align}\\] \\(z=\\frac{0.7928-1.2562}{0.19245}=-2.41\\) with a z critical value of \\(\\pm 1.96\\) at alpha = .05. The obtained z value (-2.41) exceeds the critical two-tailed z value (1.96) at alpha = .05 (In R, use qnorm(0.975)), thus, we reject the null that the population correlation is equal to .85. Confidence interval for \\({{\\rho }_{xy}}\\) Obtain 100(1-)% confidence interval for the Fishers Z transformed parameter: \\[{{z}_{r}}\\pm {{z}_{1-\\alpha /2}}\\text{S}{{\\text{E}}_{z}}=0.7928\\pm 1.96*0.19245\\] which is (0.416, 1.170). Transfer Fishers Z statistics to correlations \\[{{L}_{\\rho }}=\\frac{{{e}^{2{{L}_{z}}}}-1}{{{e}^{2{{L}_{z}}}}+1}=\\frac{{{e}^{2(0.416)}}-1}{{{e}^{2(0.416)}}+1}=0.394\\] \\[{{U}_{\\rho }}=\\frac{{{e}^{2{{U}_{z}}}}-1}{{{e}^{2{{U}_{z}}}}+1}=\\frac{{{e}^{2(1.170)}}-1}{{{e}^{2(1.170)}}+1}=0.824\\] The 95% confidence interval for \\({{\\rho }_{xy}}\\) is (0.394, 0.824) n &lt;- 30 r &lt;- .66 rho &lt;- .85 pnorm(fisherz(r), mean = fisherz(rho), sd = 1/sqrt(n-3)) * 2 # two-sided p value ## [1] 0.0160586 r.test(n = 30, .66) # Is the population correlation significantly different from 0? ## Correlation tests ## Call:r.test(n = 30, r12 = 0.66) ## Test of significance of a correlation ## t value 4.65 with probability &lt; 7.2e-05 ## and confidence interval 0.39 0.82 3.2.6 Test that two population correlations are equal Null Hypothesis: \\({{\\rho }_{1}}-{{\\rho }_{2}}=0\\) OR \\({{\\rho }_{1}}={{\\rho }_{2}}\\) Alternative Hypothesis: \\({{\\rho }_{1}}-{{\\rho }_{2}}\\ne 0\\) OR \\({{\\rho }_{1}}\\ne {{\\rho }_{2}}\\) Example: Male: n = 86, \\(r_{xy} = .75\\), \\(z_{xy} = .973\\) Female: n = 95, \\(r_{xy} = .82\\), \\(z_{xy} = 1.157\\) \\[z=\\frac{z_{m}^{{}}-z_{f}^{{}}}{\\sqrt{1/({{n}_{m}}-3)+1/({{n}_{f}}-3)}}=\\frac{.973-1.157}{\\sqrt{1/(86-3)+1/(95-3)}}=-1.21\\] The obtained z value does not exceed the z critical value of \\(\\pm1.96\\), thus, we fail to reject the null that the population correlations are equal. r12 &lt;- .75 r34 &lt;- .82 n1 &lt;- 86 n2 &lt;- 95 r.test(n = n1, r12 = r12, r34 = r34, n2 = n2) ## Correlation tests ## Call:r.test(n = n1, r12 = r12, r34 = r34, n2 = n2) ## Test of difference between two independent correlations ## z value 1.21 with probability 0.22 3.3 Variations of the Pearson Product-Moment Correlation Point Biserial Correlation Biserial Correlation Spearman Rank Phi Coefficient Tetrachoric Correlation Polychoric Correlation Chi-Square Pearsons Chi-Square 3.3.1 Point Biserial Correlation Used to assess the relationship between two variables when one variable is at the interval/ratio level of measurement and the 2nd variable is dichotomous (true dichotomy: male vs. female; first born vs. last born; mammal vs. non-mammal). Example: Students total test score and a single item from another test that is scored right or wrong. Code the dichotomous variable as 0 (e.g., wrong) or 1 (e.g., right) and calculate the Pearson product-moment correlation. \\[{{r}_{pb}}=\\frac{({{{\\bar{Y}}}_{1}}-{{{\\bar{Y}}}_{0}})\\sqrt{PQ}}{{{s}_{Y}}}\\] where - \\({\\bar Y_1}\\) is the mean of Y scores for those coded as 1 \\({\\bar Y_0}\\) is the mean of Y scores for those coded as 0 \\(P\\) is the mean of the X(0,1) coded scores. \\(Q=1-P\\) \\(s_Y\\) is the standard deviation of the Y(continuous) scores with n in the denominator instead of n-1. Significance testing of the point biserial correlation \\(t=\\frac{{{r}_{pb}}\\sqrt{n-2}}{\\sqrt{1-r_{pb}^{2}}}\\) with df = n-2. Sign of the correlation depends on whether the group with the higher mean on Y was assigned a low (0) or high (1) coding. The point biserial correlation is mathematically equivalent to the Pearson correlation between a continuous and a dichotomous variable. x &lt;- c(65, 58, 52, 49, 57, 58) # total test score y &lt;- rep(c(0,1),each = 3) # item score cor.test(x,y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = -0.77782, df = 4, p-value = 0.4801 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9071701 0.6362620 ## sample estimates: ## cor ## -0.3624623 A negative correlation means that those that got the item wrong tended to perform better on the test. Weird item! 3.3.2 Biserial Correlation Used to assess the relationship between two variables when one variable is at the interval/ratio level of measurement and the 2nd variable is a continuous dichotomy (not a true dichotomy, but a forced dichotomy). Example: Scores on an achievement test and IQ score coded as low or high. \\({{r}_{b}}=\\frac{({{{\\bar{Y}}}_{1}}-{{{\\bar{Y}}}_{0}})PQ}{h({{s}_{Y}})}={{r}_{pb}}\\frac{\\sqrt{PQ}}{h}\\) where h is the height of the standard normal curve at which its area is divided into P and Q portions. Sign of the correlation depends on whether the group with the higher mean on Y was assigned a low (0) or high (1) coding. The biserial correlation may be taken as an estimate of the Pearson product-moment correlation that would have been obtained if the X variable was continuous with a normal distribution. The biserial will always exceed the corresponding point biserial correlation. x &lt;- c(65, 58, 52, 49, 57, 58) # achievement test score y &lt;- rep(c(0,1),each = 3) # IQ score coded as low or high biserial(x,y) ## [,1] ## [1,] -0.4146982 A negative correlation means that those with higher IQ scores tended to have lower achievement test score. Weird! 3.3.3 Spearmans Rank/Spearmans Correlation/Spearmans rho Nonparametric version of the Pearson product-moment correlation. Based on the ranks of the data rather than the actual values. - Both variables are ranked variables. - Used with ordinal data or interval data with non-normal distributions. - Sign of the coefficient indicates direction of the relationship. \\({{r}_{s}}=1-\\frac{6\\sum{{{d}^{2}}}}{n({{n}^{2}}-1)}\\), where d is the difference in ranks of the pair for an object or individual and n is the sample size.  Significance testing of the correlation coefficient: \\(t=\\frac{{{r}_{s}}\\sqrt{n-2}}{\\sqrt{1-r_{s}^{2}}}\\) with df = n -2. x &lt;- c(3, 4, 5, 10, 13) y &lt;- c(12, 5, 6, 4, 3) corr.test(x, y, method = &quot;spearman&quot;) ## Call:corr.test(x = x, y = y, method = &quot;spearman&quot;) ## Correlation matrix ## [1] -0.9 ## Sample Size ## [1] 5 ## Probability values adjusted for multiple tests. ## [1] 0.04 ## ## To see confidence intervals of the correlations, print with the short=FALSE option 3.3.4 Phi Coefficient Used to assess the relationship between two variables when both are true dichotomous variables. Example: Gender (male vs. female) and performance on a test item (right vs. wrong). If the 0, 1 coding is arbitrary, the sign of the Phi coefficient is meaningless. You can use Phi, but it is more common to use the chi-square statistic which tests the association between 2 variables. However, chi-square does not work well when using small samples. The phi coefficient is mathematically equivalent to the Pearson correlation between two dichotomous variables. gender &lt;- rep(c(0,1), each = 7) item &lt;- c(0,0,0,1,1,1,1,0,0,1,1,1,1,1) corr.test(gender,item) ## Call:corr.test(x = gender, y = item) ## Correlation matrix ## [1] 0.15 ## Sample Size ## [1] 14 ## Probability values adjusted for multiple tests. ## [1] 0.61 ## ## To see confidence intervals of the correlations, print with the short=FALSE option Male/Incorrect Female/Incorrect A, 3 B, 2 Male/Correct Female/Correct C, 4 D, 5 \\[{{r}_{\\varphi }}=\\frac{BC-AD}{\\sqrt{(A+B)(C+D)(A+C)(B+D)}}\\] \\[t=\\frac{{{r}_{xy}}\\sqrt{n-2}}{\\sqrt{1-r_{xy}^{2}}}\\] with df = n -2. phi(c(3,2,4,5)) ## [1] 0.15 There is a weak, positive, and non-significant relationship between gender and performance on a test item. \\({{r}_{\\varphi }} = .149\\), p = .61. 3.3.5 Tetrachoric Correlation Used to assess the relationship between two variables when both are dichotomous (not a true dichotomy, but a forced dichotomy). Example: IQ scores (low vs. high) and GPA (low vs. high). Assume the underlying distributions of the variables are normally distributed. Use the tetrachoric() function of the psych package. 3.3.6 Polychoric Correlation Used to assess the relationship between two variables when both are ordered (ordinal; ranked) categories (forced categories). Example: IQ scores (low, medium, high) and GPA (low, medium, high). Assume the underlying distributions of the variables are normally distributed. Use the polychoric() function of the psych package. 3.3.7 Chi-Square Tests the hypothesis that two categorical variables are independent, without indicating strength or direction of the relationship. General Varieties: Pearson Chi-Square Likelihood-Ratio Chi-Square More varieties for 2 X 2 tables: Fishers exact test Yates corrected chi-square They are for analysis of categorical data! (See Chapter 18 of the textbook.) 3.3.8 Summary of measures of association Table 3.1: Summary of Measures of Association Correlation Coefficient X Variable Y Variable Pearson product-moment continuous continuous Point Biserial continuous true dichotomy Biserial continuous forced dichotomy Phi true dichotomy true dichotomy Tetrachoric forced dichotomy forced dichotomy Polychoric forced polytomy forced polytomy Spearmans Rank ranked ranked Pearson Chi-Square categorical categorical 3.4 An Example Check 2.2 for an example of basic data management and descriptive statistics including how to calculate Pearson correlations. "],["simple-linear-regression.html", "Chapter 4 Simple Linear Regression 4.1 Plot the Data 4.2 The Regression Equation 4.3 Test of Significance and Confidence Interval 4.4 Summary of Important Statistics 4.5 Confidence Intervals, Hypothesis Testing, and Prediction Intervals 4.6 A Complete Example", " Chapter 4 Simple Linear Regression library(rio); library(psych); library(Hmisc); library(ggplot2); library(QuantPsyc) Regression is one of the most flexible, most powerful, and most frequently used methods for modeling data, both in standard data analysis, and in advanced data science. Make sure you know how to do it. Use it as a baseline for comparing your models. The simplest functional relationship between two variables X and Y can be expressed by the linear model (population): \\({{Y}_{i}}=\\alpha +\\beta {{X}_{i}}+{{\\varepsilon }_{i}}\\), where \\({{Y}_{i}}\\)= the score for person i on the dependent variable Y; \\(\\alpha\\) = the Y-intercept and is equal to the Y value when the value of X = 0 (where the line intersects the Y-axis); \\(\\beta\\) = the slope of the line and indicates the change in Y as the X value increases (or decreases) by 1 point (the steepness of the line) (Also called a regression coefficient); \\({{X}_{i}}\\) = the value of the independent variable X for person i; \\({{\\varepsilon }_{i}}\\) = random error for person i. For a sample: \\(Y=a+bX+e\\), where a is an estimate of \\(\\alpha\\); b is an estimate of \\(\\beta\\); and e is an estimate of \\({{\\varepsilon }_{i}}\\). Y cannot be perfectly predicted from X. Other sources of variability will not be measured (represented by e). Because e is unobservable or is unable to be measured, we rewrite the equation to indicate that we are unable to measure all sources of error: \\(\\hat Y = a + bX\\), where \\({\\hat Y}\\) is the predicted Y value determined from a knowledge of X values. Prediction Error (called Residuals) is represented as the difference between observed Y values and the predicted Y values: \\(e = Y - \\hat Y\\) Ultimately, we want to find a solution for a and b that will minimize error as much as possible so that we can explain as much of Y as possible. Anything unexplained by X is leftover error (again, represented by e). Ordinary Least Squares Regression minimizes the sum of squared residuals ( \\(SS_{res}\\) ): \\(\\sum{{{(Y-{\\hat Y})}^{2}}=\\sum{{{e}^{2}}}}\\) The linear regression equation, \\({\\hat Y}=a+bX\\), will produce a line that will fit the data most accurately in the sense that it will minimize the residual sum of squares ( \\(SS_{res}\\) ) (Ordinary Least Squares). Intercept: \\(a=\\bar{Y}-b\\bar{X}\\) Slope: \\(b=\\frac{\\sum{xy}}{\\sum{{{x}^{2}}}}=\\frac{\\sum{(X-\\bar{X})(Y-\\bar{Y})}}{\\sum{{{(X-\\bar{X})}^{2}}}}\\) \\(X\\) \\(Y\\) \\(X-\\bar X\\) \\(Y-\\bar Y\\) \\((X - \\bar X)^2\\) \\((Y - \\bar Y)^2\\) \\((X - \\bar X)(Y - \\bar Y)\\) 1 1 -3.57 -2.86 12.74 8.18 10.21 1 3 -3.57 -0.86 12.74 0.74 3.07 3 2 -1.57 -1.86 2.46 3.46 2.92 4 5 -0.57 1.14 0.32 1.30 -0.65 6 4 1.43 0.14 2.04 0.02 0.20 7 5 2.43 1.14 5.90 1.30 2.77 10 7 5.43 3.14 29.48 9.86 17.05 \\(\\Sigma\\) 32 27 0.01 -0.02 65.68 24.86 35.57 \\(\\bar X = 4.57\\), \\(\\bar Y = 3.86\\), \\(s_x = 3.31\\), \\(s_y = 2.03\\) Calculate slope: \\(b=\\frac{\\sum{xy}}{\\sum{{{x}^{2}}}}=\\frac{\\sum{(X-\\bar{X})(Y-\\bar{Y})}}{\\sum{{{(X-\\bar{X})}^{2}}}}=\\frac{35.57}{65.68}=.54\\) Calculate intercept: \\(a=\\bar{Y}-b\\bar{X}=3.85-(.54)(4.57)=3.85-2.47=1.38\\) Our linear regression equation: \\({\\hat Y}=1.38+.54X\\). This equation can be used to predict Y using X for other samples. What if we wanted to predict Y for a certain value of X (e.g., X = 8)? Just plug in the value 8 for X in the equation: \\({\\hat Y}=1.38+.54X=1.38+(.54)(8)=1.38+4.32=5.7\\) 4.1 Plot the Data 4.1.1 Using plot() x &lt;- c(1, 1, 3, 4, 6, 7, 10) y &lt;- c(1, 3, 2, 5, 4, 5, 7) plot(x,y) abline(lm(y ~ x)) 4.1.2 Using ggplot() df &lt;- data.frame(x,y) ggplot(data = df, mapping = aes(x = x, y = y)) + geom_point(color = &quot;red&quot;, size = 2) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; 4.2 The Regression Equation Note that the intercept (a = 1.38) is where the line intersects the Y axis on the scatterplot. Also, look at the slope value (b = .54) which indicates that as X increases by 1 point, Y increases about a half of a point. Recall that \\({\\hat Y}=a+bX\\) and that \\(a=\\bar{Y}-b\\bar{X}\\). \\[{\\hat Y}=(\\bar{Y}-b\\bar{X})+bX=\\bar{Y}+b(X-\\bar{X})=\\bar{Y}+bx\\] \\[b=\\frac{\\sum{xy}}{\\sum{{{x}^{2}}}}=\\frac{\\sum{(X-\\bar{X})(Y-\\bar{Y})}}{\\sum{{{(X-\\bar{X})}^{2}}}}\\] Thus, if X does not affect Y or covary with Y, b = 0 and the predicted Y value will be equal to the mean of Y: \\({\\hat Y}=\\bar{Y}+bx=\\bar{Y}+(0)(x)=\\bar{Y}\\). Without any useful information, the mean will be the best predicted value. Plugging in each persons X value into our regression equation will yield predicted Y scores for every person: \\({\\hat Y}=1.38+.54X\\) \\(X\\) \\(Y\\) \\(\\hat Y\\) 1 1 1.92 1 3 1.92 3 2 3.00 4 5 3.54 6 4 4.62 7 5 5.16 10 7 6.78 \\(\\Sigma\\) 32 27 26.94 4.2.1 Variance explained How much of the variance in Y is due to X and how much is due to error? \\(\\sum{{{y}^{2}}=\\sum{{{({\\hat Y}-\\bar{Y})}^{2}}+\\sum{{{(Y-{\\hat Y})}^{2}}}}}\\) which can be expressed as \\(\\sum{{{y}^{2}}=S{{S}_{reg}}+S{{S}_{res}}}\\) where the deviation sum of squares of the dependent variable Y ( \\(S{S_{total}}\\) ) is a function of two components: sum of squares due to regression ( \\(S{{S}_{reg}}\\) ) and sum of squares due to error ( \\(S{S_{res}}\\) ): \\(S{{S}_{total}}=S{{S}_{reg}}+S{{S}_{res}}\\). Dividing by the total sum of squares, we can determine the proportion of variance due to the regression of Y on X and the proportion of variance due to error. \\(\\frac{\\sum{{{y}^{2}}}}{\\sum{{{y}^{2}}}}=\\frac{S{{S}_{reg}}}{\\sum{{{y}^{2}}}}+\\frac{S{{S}_{res}}}{\\sum{{{y}^{2}}}}\\) OR \\(\\frac{S{{S}_{total}}}{S{{S}_{total}}}=\\frac{S{{S}_{reg}}}{S{{S}_{total}}}+\\frac{S{{S}_{res}}}{S{{S}_{total}}}\\) Different notations in textbook: \\(SS_T\\) is the total sum of squares; \\(SS_M\\) is the model sum of squares (i.e., sum of squares due to the regression model), \\(SS_R\\) is the residual sum of squares. \\(X\\) \\(Y\\) \\(\\hat Y\\) \\(\\hat Y-\\bar Y\\) \\((\\hat Y-\\bar Y)^2\\) \\(Y - \\hat Y\\) \\((Y - \\hat Y)^2\\) \\((Y - \\bar Y)^2\\) 1 1 1.92 -1.94 3.75 -0.92 0.85 8.16 1 3 1.92 -1.94 3.75 1.08 1.17 0.73 3 2 3.00 -0.86 0.73 -1.00 1.00 3.45 4 5 3.54 -0.32 0.10 1.46 2.13 1.31 6 4 4.62 0.76 0.58 -0.62 0.38 0.02 7 5 5.16 1.30 1.70 -0.16 0.03 1.31 10 7 6.78 2.92 8.54 0.22 0.05 9.88 \\(\\Sigma\\) 32 27 26.94 -0.08 19.15 0.06 5.61 24.86 \\(\\bar X = 4.57\\), \\(\\bar Y = 3.86\\) \\(\\sum y^2 = \\sum {(\\hat Y - \\bar Y)^2} + \\sum {(Y - \\hat Y)^2} = SS_{reg} + SS_{res} = 19.15 + 5.61 = 24.76\\) \\({{\\sum y^2} \\over {\\sum y^2}} = {SS_{reg} \\over {\\sum y^2 }} + {SS_{res} \\over {\\sum y^2}}\\) OR \\({SS_{total} \\over {SS_{total}}} = {{SS_{reg}} \\over {SS_{total}}} + {{SS_{res}} \\over {SS_{total}}} = {{19.15} \\over {24.76}} + {{5.61} \\over {24.76}} = .77 + .23\\) About 77% of the variance in Y is due to X whereas about 23% of the variance is due to error. 4.2.2 Relationship with Pearson correlation \\(X\\) \\(Y\\) \\(X-\\bar X\\) \\(Y-\\bar Y\\) \\((X - \\bar X)^2\\) \\((Y - \\bar Y)^2\\) \\((X - \\bar X)(Y - \\bar Y)\\) 1 1 -3.57 -2.86 12.74 8.18 10.21 1 3 -3.57 -0.86 12.74 0.74 3.07 3 2 -1.57 -1.86 2.46 3.46 2.92 4 5 -0.57 1.14 0.32 1.30 -0.65 6 4 1.43 0.14 2.04 0.02 0.20 7 5 2.43 1.14 5.90 1.30 2.77 10 7 5.43 3.14 29.48 9.86 17.05 \\(\\Sigma\\) 32 27 0.01 -0.02 65.68 24.86 35.57 \\[{{r}_{xy}}=\\frac{\\sum{(X-\\bar{X})(Y-\\bar{Y})/(N-1)}}{{{s}_{x}}{{s}_{y}}}=\\frac{35.57/(7-1)}{(3.31)(2.03)}=\\frac{5.93}{6.72}=.88\\] \\(r_{xy}^{2}={{.88}^{2}}=.77\\) which represents the proportion of variance in Y accounted for by X. Recall that this is the same value as dividing \\(S{{S}_{reg}}\\) by \\(S{{S}_{total}}\\): \\(\\frac{S{{S}_{reg}}}{S{{S}_{total}}}=\\frac{19.15}{24.86}=.77\\). 4.2.3 Coefficient of Determination (\\({{R}^{2}}\\)) The Squared multiple correlation, or R Square ( \\({{R}^{2}}\\) ), is interpreted as the proportion of variance in the outcome variable Y that can be explained by the predictor variable X. The adjusted R Square is an adjustment to better reflect the fit of the model in the population. \\[{{R}^{2}}=1-\\frac{S{{S}_{res}}}{S{{S}_{total}}}\\] where \\(S{{S}_{res}}\\) = \\(\\sum{{{(Y-{\\hat Y})}^{2}}=\\sum{{{e}^{2}}}}\\) and \\(S{{S}_{total}}\\) = \\(\\sum{{{(Y-\\bar{Y})}^{2}}}\\). \\[R_{adj}^{2}={{R}^{2}}-\\frac{(1-{{R}^{2}})k}{n-k-1}\\] where n = sample size and k = the number of independent variables included in the model. \\({{R}^{2}}\\) is the correlation between observed and predicted/fitted values squared. In simple linear regression, \\({{R}^{2}}\\) is the square of the correlation between the predictor and outcome variables. 4.2.4 R Syntax Use the lm() function to run the regression model. x &lt;- c(1, 1, 3, 4, 6, 7, 10) y &lt;- c(1, 3, 2, 5, 4, 5, 7) model &lt;- lm(y ~ x) coef(model) ## (Intercept) x ## 1.3826087 0.5413043 To get the predicted values: \\({\\hat Y}=1.38+.54X\\). To get the residuals: \\(e = Y - {\\hat Y}\\) y.pred &lt;- fitted(model) y.res &lt;- resid(model) cbind(x, y, y.pred, y.res) ## x y y.pred y.res ## 1 1 1 1.923913 -0.9239130 ## 2 1 3 1.923913 1.0760870 ## 3 3 2 3.006522 -1.0065217 ## 4 4 5 3.547826 1.4521739 ## 5 6 4 4.630435 -0.6304348 ## 6 7 5 5.171739 -0.1717391 ## 7 10 7 6.795652 0.2043478 Coefficient of Determination (\\({{R}^{2}}\\)) summary(model) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## 1 2 3 4 5 6 7 ## -0.9239 1.0761 -1.0065 1.4522 -0.6304 -0.1717 0.2043 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.3826 0.7186 1.924 0.11234 ## x 0.5413 0.1306 4.146 0.00895 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.059 on 5 degrees of freedom ## Multiple R-squared: 0.7746, Adjusted R-squared: 0.7296 ## F-statistic: 17.19 on 1 and 5 DF, p-value: 0.008949 SS_total &lt;- sum((y-mean(y))^2) SS_reg &lt;- sum((fitted(model)-mean(y))^2) SS_res &lt;- sum((y - fitted(model))^2) SS_reg/SS_total ## [1] 0.7746252 1 - SS_res/SS_total ## [1] 0.7746252 \\({{R}^{2}}\\) is the correlation between observed and predicted/fitted values squared cor(y, fitted(model))^2 ## [1] 0.7746252 In simple linear regression, \\({{R}^{2}}\\) is the square of the correlation between the predictor and outcome variables. cor(x, y) ## [1] 0.8801279 cor(x, y)^2 ## [1] 0.7746252 4.3 Test of Significance and Confidence Interval Formula for the Slope (Regression Coefficient): \\(b=\\frac{\\sum{xy}}{\\sum{{{x}^{2}}}}=\\frac{\\sum{(X-\\bar{X})(Y-\\bar{Y})}}{\\sum{{{(X-\\bar{X})}^{2}}}}\\) Alternate Formula for the Slope (Regression Coefficient): \\(b=r\\frac{{{s}_{y}}}{{{s}_{x}}}\\), where r is the correlation coefficient between the independent and dependent variable, \\({s_y}\\) is the standard deviation of the dependent variable, and \\({s_x}\\) is the standard deviation of the independent variable. Standardized Regression Coefficients: Unstandardized regression coefficients we have been discussing are calculated using the raw scores of our variables (X and Y) in which they are still in their units of measurement (e.g., time in years, money in dollars). If we had standardized X and Y scores, in which the raw score units of X and Y were changed into z scores, the regression coefficients we would calculate would no longer be unstandardized, but would be called standardized regression coefficients. They are called standardized because they are the slopes of an equation using standardized scores (i.e., z scores). When using standardized scores, our regression equation would then be: \\(z_{\\hat y}=\\beta {{z}_{x}}\\), where \\(z_{\\hat y}\\) is the predicted standardized score of Y, \\(\\beta\\) is the standardized regression coefficient, and \\({{z}_{x}}\\) is the standardized score of X. You may notice that the intercept is no longer in the regression equation when using standardized scores. The intercept is equal to the value of zero in this case. This is due to the fact that the mean of z scores is equal to a value of zero. Recall the formula for the intercept: \\(a=\\bar{Y}-b\\bar{X}\\)= 0  b(0) = 0. Formula for the standardized regression coefficient: \\(\\beta =\\frac{\\sum{{{z}_{x}}{{z}_{y}}}}{\\sum{z_{x}^{2}}}\\), where the term in the numerator, \\(\\sum{{{z}_{x}}{{z}_{y}}}\\), is the sum of cross products and the term in the denominator, \\(\\sum{z_{x}^{2}}\\), is the sum of squared standardized scores. Alternate formula for the standardized regression coefficient: \\(\\beta =b\\frac{{{s}_{x}}}{{{s}_{y}}}\\), where b is the unstandardized regression coefficient, sy is the standard deviation of the dependent variable Y, and sx is the standard deviation of the independent variable X. In simple linear regression, the standardized regression coefficient is the correlation coefficient between X and Y: \\(\\beta =b\\frac{{{s}_{x}}}{{{s}_{y}}}=\\frac{\\sum{xy}\\sqrt{\\sum{{{x}^{2}}}}\\sqrt{n-1}}{\\sum{{{x}^{2}}}\\sqrt{n-1}\\sqrt{\\sum{{{y}^{2}}}}}=\\frac{\\sum{xy}}{\\sqrt{\\sum{{{x}^{2}}}\\sqrt{\\sum{{{y}^{2}}}}}}={{r}_{xy}}\\) In Simple Linear Regression, the standardized regression coefficient has fixed upper lower and upper limits of a correlation coefficient (i.e., ranges from -1 to +1). Unstandardized regression coefficients indicate the direction and by how many units Y will change with a 1 unit increase in X. They are expressed in their original scale of measurement of the independent variable (e.g., number of years). Recall that the mean of z scores is zero and the standard deviation is 1. Because the standard deviation of z scores is 1, a unit change in X indicates a change of one standard deviation. Thus, standardized regression coefficients indicate the direction and by how many standard deviations Y will change with 1 standard deviation increase in X. 4.3.1 Testing the regression of Y on X Recall that the total sum of squares \\(\\sum{{{(Y-\\bar{Y})}^{2}}}\\) is equal to the sum of the regression sum of squares \\(\\sum{{{({\\hat Y}-\\bar{Y})}^{2}}}\\)and the residual sum of squares \\(\\sum{{{(Y-{\\hat Y})}^{2}}}\\): \\(S{{S}_{total}}=S{{S}_{reg}}+S{{S}_{res}}\\). These sums of squares represent the variation of Y around its respective mean (Total), the variation of each predicted Y around the mean of Y (Regression), and the variation of each observed Y around its respective predicted Y value (Residual). Dividing these sums of squares by their respective degrees of freedom (df) makes these statistics larger and more accurate estimates of the variability they represent in the population. The resulting value is referred to as the Mean Square or MS because it represents the mean of a particular sum of squares, which will be an unbiased estimate of the population variance. df for the Total Sum of Squares: n  1 df for the Regression Sum of Squares: k (number of independent variables) df for the Residual Sum of Squares: n  k  1 \\(M{{S}_{reg}}=\\frac{\\sum{{{({\\hat Y}-\\bar{Y})}^{2}}}}{k}=\\frac{S{{S}_{reg}}}{d{{f}_{reg}}}\\) \\(M{{S}_{res}}=\\frac{\\sum{{{(Y-{\\hat Y})}^{2}}}}{n-k-1}=\\frac{S{{S}_{res}}}{d{{f}_{res}}}\\) Using the \\(M{{S}_{reg}}\\) and \\(M{{S}_{res}}\\), we can calculate an F ratio to test the regression of Y on X. \\(F=\\frac{MS_{reg}}{MS_{res}}\\) with \\(df_{reg}\\) in the numerator and \\(df_{res}\\) in the denominator. If the F ratio exceeds the critical value with ( \\(df_{reg}\\), \\(df_{res}\\) ) at alpha = .05, we reject the null that the regression of Y on X is equal to zero. That is, we are testing whether R Square is significantly different from zero: \\(F=\\frac{{{R}^{2}}/k}{(1-{{R}^{2}})/(n-k-1)}\\tilde{\\ }df=k\\ \\text{and}\\ n-k-1\\) 4.3.2 Testing the regression coefficient (slope) and the confidence interval You can also test the significance of a regression coefficient (slope) b to see if it is significantly different from zero. Variance of Estimate: The variance of estimate indicates the variance of the scores about the regression line. It is the variance of the residuals. \\[s_{y.x}^{2}=\\frac{\\sum{{{(Y-{\\hat Y})}^{2}}}}{n-k-1}=\\frac{S{{S}_{res}}}{n-k-1}\\] Recall: \\(M{{S}_{res}}=\\frac{\\sum{{{(Y-{\\hat Y})}^{2}}}}{n-k-1}=\\frac{S{{S}_{res}}}{d{{f}_{res}}}\\) Standard Error of Estimate: The standard error of estimate is the square root of the variance of estimate or the standard deviation of the residuals. \\[{{s}_{y.x}}=\\sqrt{\\frac{\\sum{{{(Y-{\\hat Y})}^{2}}}}{n-k-1}}=\\sqrt{\\frac{S{{S}_{res}}}{n-k-1}}\\] When testing whether the slope or regression coefficient is significantly different from zero, the standard error associated with the slope b must be calculated: \\({{s}_{b}}=\\sqrt{\\frac{s_{y.x}^{2}}{\\sum{{{x}^{2}}}}}=\\frac{{{s}_{y.x}}}{\\sqrt{\\sum{{{x}^{2}}}}}\\) where sb is the standard error of b, \\(s_{y.x}^{2}\\) is the variance of estimate, \\(s_{y.x}^{{}}\\) is the standard error of estimate, and \\(\\sum\\limits_{{}}{{{x}^{2}}}\\)is the sum of squares for the independent variable X \\({s}_{b}\\) is the standard deviation of the sampling distribution of b and is used when testing the significance of the b using the t ratio: \\(t=\\frac{b}{{{s}_{b}}}=\\frac{b-0}{{{s}_{b}}}\\) with df of n  k  1. \\(t=\\frac{b}{{{s}_{b}}}=\\frac{b-0}{{{s}_{b}}}\\). Note the special relationship between the F ratio and the t ratio when using only 1 independent variable: The F ratio is equal to the squared value of the t ratio (\\({{t}^{2}}=F\\)). With knowledge of the standard error of b, we can create a confidence interval around the regression coefficient: \\(b\\pm {{t}_{(\\alpha /2,df)}}{{s}_{b}}\\). 4.3.3 R Syntax Use summary() and confint() for statistical inference # summary(model) confint(model, level = 0.95) # for all parameters ## 2.5 % 97.5 % ## (Intercept) -0.4645905 3.2298079 ## x 0.2056480 0.8769607 confint(model, &quot;x&quot;, level = 0.95) # for specific parameters ## 2.5 % 97.5 % ## x 0.205648 0.8769607 Get standardized regression coefficients Method 1 lm(scale(y) ~ scale(x)) ## ## Call: ## lm(formula = scale(y) ~ scale(x)) ## ## Coefficients: ## (Intercept) scale(x) ## 2.417e-16 8.801e-01 Method 2: use the lm.beta() function from the QuantPsyc package model &lt;- lm(y ~ x) lm.beta(model) ## x ## 0.8801279 4.4 Summary of Important Statistics R Square Adjusted R Square F Statistic to Test the Regression Model Regression Coefficients (Slopes) Standard Error of the Regression Coefficient T Statistic to Test Individual Slope Values Confidence Interval for the Slope (b) Standardized Regression Coefficient 4.5 Confidence Intervals, Hypothesis Testing, and Prediction Intervals 4.5.1 Prediction Intervals for New Observations Prediction Interval for the Mean (also called Confidence Bands) Confidence interval around the mean predictions Prediction Interval for the Individual (also called Prediction Bands) It gives uncertainty around a single value Prediction bands will be wider than the confidence bands 4.5.2 R Syntax Prediction Intervals new.xs &lt;- data.frame( x = c(2, 5, 8) ) predict(model, newdata = new.xs, interval = &quot;confidence&quot;) # Prediction interval for the mean ## fit lwr upr ## 1 2.465217 1.122592 3.807843 ## 2 4.089130 3.050686 5.127575 ## 3 5.713043 4.169650 7.256437 predict(model, newdata = new.xs, interval = &quot;prediction&quot;) # Prediction interval for the individual ## fit lwr upr ## 1 2.465217 -0.568980 5.499415 ## 2 4.089130 1.176730 7.001531 ## 3 5.713043 2.584821 8.841266 Create a scatterplot with regression line, confidence band, and prediction band model &lt;- lm(y ~ x) pred.int &lt;- predict(model, interval = &quot;prediction&quot;) ## Warning in predict.lm(model, interval = &quot;prediction&quot;): predictions on current data refer to _future_ responses mydata &lt;- data.frame(x, y, pred.int) ggplot(mydata, aes(x, y)) + geom_point() + stat_smooth(method = lm) + geom_line(aes(y = lwr), color = &quot;red&quot;, linetype = &quot;dashed&quot;) + geom_line(aes(y = upr), color = &quot;red&quot;, linetype = &quot;dashed&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 4.6 A Complete Example mydata &lt;- import(&quot;data/profs.sav&quot;) #View(mydata) corr.test(mydata$salary, mydata$time) mymodel &lt;- lm(salary ~ time, data = mydata) summary(mymodel) confint(mymodel, level = 0.95) lm.beta(mymodel) mydata$y.pred &lt;- fitted(mymodel) mydata$y.res &lt;- resid(mymodel) head(mydata) ggplot(mydata, aes(x = time, y = salary)) + geom_point() + stat_smooth(method = lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; new.obs &lt;- data.frame( time = 12 ) predict(mymodel, newdata = new.obs, interval = &quot;confidence&quot;) # Prediction interval for the mean predict(mymodel, newdata = new.obs, interval = &quot;prediction&quot;) # Prediction interval for the individual Sum up: A correlational analysis indicated that a moderate, positive, and statistically significant relationship exists between time since earning a Ph.D. and professors current salary r(60) = .61, p &lt; .001. A simple linear regression analysis was used to regress salary on time since earning a Ph.D. The regression model was statistifally significant, F(1, 60) = 35.17, p &lt; .001. As indicated by the correlational analysis, time since earning a Ph.D. was a significant predictor of professors current salary, t(60) = 5.93, p &lt; .001, accounting for approximately 36% of the variance ( \\(R_{adj}^{2} = .359\\)). The analysis indicated that as time since earning a Ph.D. increases by 1 year, professors salary is estimated to increase by about $1379 (95% CI: $914, $1845). As the number of years since earning a Ph.D. increases by 1 standard deviation, professors salary is estimate to increase by 0.61 standard deviations. For professors 12 years post Ph.D., it is predicted that their salary would be about $62001 (95% CI: $58875, $65127). For any professor 12 years post Ph.D., the predicted salary is about $62001 (95% CI: $46146, $77856). "],["multiple-regression-analysis.html", "Chapter 5 Multiple Regression Analysis 5.1 F Test and Partial F Test 5.2 Testing the Partial Regression Coefficients 5.3 Partial and Semi-paritial (Part) Correlations 5.4 A Complete Example 5.5 Types of Multiple Regression 5.6 (A Few) Assumptions in Regression 5.7 Multiple Regression and Patterns of Association 5.8 Regression Diagnostics 5.9 Curvilinear Regression 5.10 Useful Functions and Symbols Commonly Used in R When Fitting Linear Models 5.11 Interactions in Regression 5.12 Categorical Independent Variables 5.13 Interactions Between Categorical and Continuous Variables 5.14 An Open Book on Regression 5.15 Create APA Tables with apaTables Package", " Chapter 5 Multiple Regression Analysis library(rio); library(ggplot2); library(QuantPsyc); library(psych); library(car); library(lmtest); library(MASS); library(interactions); library(memisc); library(apaTables) Recall: Simple Linear Regression: \\(Y=a+bX+e\\) Regression with 2 Independent Variables: \\(Y=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+e\\), where a is the intercept/constant (value of Y when X1 and X2 are both equal to zero), \\({{b}_{1}}\\ \\text{and}\\ {{b}_{2}}\\) are regression coefficients associated with X1 and X2, respectively, X1 and X2 are raw scores on the independent variables, and e is the error or residual. Can be extended to incorporate more independent variables: \\(Y=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+...+{{b}_{k}}{{X}_{k}}+e\\), where k is the number of independent variables. Recall: Simple Linear Regression: \\({\\hat Y}=a+bX\\). Solved for a and b using Least Squares solution that minimizes the sum of squared residuals ( \\(SS_{res}\\); \\(\\sum{{{(Y-{\\hat Y})}^{2}}=\\sum{{{e}^{2}}}}\\) ) Intercept: \\(a=\\bar{Y}-b\\bar{X}\\) Slope: \\(b=\\frac{\\sum{xy}}{\\sum{{{x}^{2}}}}=\\frac{\\sum{(X-\\bar{X})(Y-\\bar{Y})}}{\\sum{{{(X-\\bar{X})}^{2}}}}\\) Regression with 2 Independent Variables: \\({\\hat Y}=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}\\) Now, \\({{b}_{1}}\\ \\text{and}\\ {{b}_{2}}\\) are considered partial regression coefficients associated with variables X1 and X2, respectively. X1 and X2 are used together in predicting Y. \\({{b}_{1}}\\) is the partial regression coefficient for Y on X1 with X2 in the regression equation/model. \\({{b}_{2}}\\) is the partial regression coefficient for Y on X2 with X1 in the regression equation/model. Still trying to minimize the difference between observed Y scores and predicted Y scores (i.e., \\(\\sum{(Y-{\\hat Y})}^{2}\\) ; Least Squares solution) when solving for the intercept (a) and the partial regression coefficients ( \\({{b}_{1}}\\) and \\({{b}_{2}}\\) ). Using our professors salary example, we want to estimate/predict Salary from Time since Ph.D. was earned (X1) and Number of citations (X2). Regress Salary on Time since Ph.D. and Number of citations. To compute the partial regression coefficients, \\({{b}_{1}}\\ \\text{and}\\ {{b}_{2}}\\), lets first calculate the partial standardized regression coefficients or beta weights ( \\(\\beta_1\\) and \\(\\beta_2\\) ): \\(b_1 = \\beta_1\\frac{{s_Y}}{{s_{X1}}}\\) ; \\(b_2={\\beta_2}\\frac{{s_Y}}{{s_{X2}}}\\) In simple linear regression, the standardized regression coefficient (Beta) is the correlation coefficient between X and Y: \\[\\beta =b\\frac{{{s}_{x}}}{{{s}_{y}}}=\\frac{\\sum{xy}\\sqrt{\\sum{{{x}^{2}}}}\\sqrt{n-1}}{\\sum{{{x}^{2}}}\\sqrt{n-1}\\sqrt{\\sum{{{y}^{2}}}}}=\\frac{\\sum{xy}}{\\sqrt{\\sum{{{x}^{2}}}\\sqrt{\\sum{{{y}^{2}}}}}}={{r}_{xy}}\\] In Simple Linear Regression, the standardized regression coefficient (Beta) has fixed upper lower and upper limits a correlation coefficient (i.e., ranges from -1 to +1). In Multiple Regression, the partial standardized regression coefficients (Beta weights) no longer have this fixed lower and upper limit. Generally speaking, however, the partial standardized regression coefficient should not exceed the absolute value of 1. If it does, this may be an indication of a problem among the independent variables (to be discussed later). They are called partial standardized regression coefficients because they are the partial slopes of an equation using standardized scores (i.e., z scores): \\(z_{\\hat y}={{\\beta }_{1}}{{z}_{1}}+{{\\beta }_{2}}{{z}_{2}}\\), where \\({{\\beta }_{1}}\\ and\\ {{\\beta }_{2}}\\) are standardized regression coefficients; \\({{z}_{1}}\\ and\\ {{z}_{2}}\\) are standard scores on X1 and X2, respectively. Unstandardized regression coefficients indicate the direction and by how many units Y will change with a 1 unit increase in X. They are expressed in their original scale of measurement of the independent variable (e.g., number of years). Recall that the mean of z scores is zero and the standard deviation is 1. Because the standard deviation of z scores is 1, a unit change in \\(z_x\\) indicates a change of one standard deviation in X. Thus, standardized regression coefficients indicate the direction and by how many standard deviations Y will change with 1 standard deviation increase in X. The relationships between standardized regression coefficients and correlations with two independent variables are below: \\[{{\\beta }_{1}}=\\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}\\] \\[{{\\beta }_{2}}=\\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{1-r_{12}^{2}}\\] When we interpret the partial regression coefficient for one independent variable, we should state that we are holding the other independent variable constant. Using the Professors salary example, we would like to regress Salary (Y) on Time (X1) and Citations (X2). We get \\[{{\\beta }_{1}}=\\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}=\\frac{0.608-(0.55)(0.373)}{1-{{(0.373)}^{2}}}=0.468\\] \\[{{\\beta }_{2}}=\\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{1-r_{12}^{2}}=\\frac{0.55-(0.608)(0.373)}{1-{{(0.373)}^{2}}}=0.375\\] With every 1 standard deviation increase in Time, predicted Salary increases by about .47 standard deviations while holding number of citations constant. With every 1 standard deviation increase in number of citations, predicted Salary increases by about .38 standard deviations while holding time since Ph.D. was earned constant. Why do we state that we are holding the other independent variable constant? In multiple regression, there is redundancy or shared variance between X1 and X2 which must now be taken into account. For example, if two predictors, X1 and X2, are positively correlated and both affect Y (are correlated with the criterion/dependent variable), then the change in Y that is associated with an increase in X1 is partly caused by the increase in X1 and partly caused by the corresponding increase in X2. The regression coefficient or effect of a given predictor variable should indicate the change in Y produced by a unit increase in that predictor variable alone, that is, when the other predictor variable does not change. Thus, when computing multiple regression statistics for a given predictor, we must hold the effects of the other predictor variable statistically constant to determine the extent to which changes in Y result from a change in a given predictor alone. This basically means that multiple regression statistics, such as the partial regression coefficients (standardized or unstandardized), remove the variance from X1 that it shares with X2 in the multiple regression model. Once we have calculated the partial standardized regression coefficients, convert them back to original scale (time in years; number of citations) to get the unstandardized regression coefficients and intercept. Multiple R is the correlation coefficient between the observed and predicted Y values ( \\({r_{Y{\\hat Y}}}\\); ranges from 0 to 1). The Squared multiple correlation, or R Square (\\({{R}^{2}}\\)), is interpreted as the proportion of variance in the criterion/dependent variable Y that can be explained by the predictor variables X1 and X2. The adjusted R Square is an adjustment to better reflect the fit of the model in the population. \\[R=\\sqrt{\\frac{r_{Y1}^{2}+r_{Y2}^{2}-2{{r}_{Y1}}{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}}\\] \\[R=\\sqrt{{{\\beta }_{1}}{{r}_{Y1}}+{{\\beta }_{2}}{{r}_{Y2}}}\\] Multiple R can never be smaller than the absolute value of the largest correlation between a predictor variable and the dependent variable. \\({{R}^{2}}=\\frac{S{{S}_{reg}}}{\\sum{{{y}^{2}}}}=\\frac{S{{S}_{reg}}}{S{{S}_{total}}}\\) , where \\(S{S_{reg}} = \\sum {{{({\\hat Y} - \\bar Y)}^2}}\\) and \\(S{S_{total}} = \\sum {{{(Y - \\bar Y)}^2}}\\). \\({{R}^{2}}=\\frac{r_{Y1}^{2}+r_{Y2}^{2}-2{{r}_{Y1}}{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}\\) \\({{R}^{2}}={{\\beta }_{1}}{{r}_{Y1}}+{{\\beta }_{2}}{{r}_{Y2}}\\) R Square in multiple regression is also referred to as the Coefficient of Multiple Determination (Indicating the amount of variance in the dependent variable explained by the independent variables). Coefficient of Multiple Alienation: \\(1-{{R}^{2}}\\), indicating the amount of variance in the dependent variable not explained by the independent variables. \\(R_{adj}^{2}={{R}^{2}}-\\frac{(1-{{R}^{2}})k}{n-k-1}\\) where n = sample size and k = the number of independent variables included in the model. 5.1 F Test and Partial F Test Just as in Simple Linear Regression, we can calculate an F ratio to test the regression of Y on both X1 and X2. \\(F=\\frac{M{{S}_{reg}}}{M{{S}_{res}}}\\) with \\(d{{f}_{reg}}\\) in the numerator and \\(d{{f}_{res}}\\) in the denominator (df for the Regression Sum of Squares = k; df for the Residual Sum of Squares = n  k  1). If the F ratio exceeds the critical value with ( \\(d{{f}_{reg}}\\), \\(d{{f}_{res}}\\) ) at alpha = .05, we reject the null that the regression of Y on both X1 and X2 is equal to zero. That is, we are testing whether R Square is significantly different from zero: \\(F=\\frac{{{R}^{2}}(n-k-1)}{(1-{{R}^{2}})k}\\tilde{\\ }df=k\\ \\text{and}\\ n-k-1\\) The null hypothesis of the F test is that all partial regression coefficients are equal to zero. Three Questions That can be Answered with F tests: Overall test. Taken collectively, does the entire set of IVs (or equivalently, the fitted model itself) contribute significantly to the prediction of Y? Test for addition of a single variable. Does the addition of one particular IV of interest add significantly to the prediction of Y achieved by other IVs already present in the model? Test for addition of a group of variables. Does the addition of some group of IVs of interest add significantly to the prediction of Y obtained through other IVs already present in the model? 5.1.1 Overall test (F Test) \\[F = \\frac{{M{S_{reg}}}}{{M{S_{res}}}} = \\frac{{S{S_{reg}}/k}}{{S{S_{res}}/(n - k - 1)}}\\] How many degrees of freedom for the numerator? How many degrees of freedom for the denominator? 5.1.2 Partial F test for additional variable(s) Add k IVs to the model with p IVs already in the model: \\[F = \\frac{{[S{S_{reg}}(full) - S{S_{reg}}(reduced)]/k}}{{S{S_{res}}(full)/[n - (p + k) - 1]}} = \\frac{{[S{S_{reg}}(full) - S{S_{reg}}(reduced)]/k}}{{M{S_{res}}(full)}}\\] This partial F test compares two models: one with p IVs and the other with (p+k) IVs. df for the numerator is k; df for the denominator is [n-(p+k)-1] Estimate both models In R, mydata &lt;- import(&quot;data/profs.sav&quot;) model_full &lt;- lm(salary ~ time + cits, data = mydata) model_reduced &lt;- lm(salary ~ time, data = mydata) Conduct Partial F test using the anova() function anova(model_full, model_reduced) ## Analysis of Variance Table ## ## Model 1: salary ~ time + cits ## Model 2: salary ~ time ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 59 2926312249 ## 2 60 3623032005 -1 -696719756 14.047 0.0004078 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Variables Added in Order, and Variables Added Last Recall: A partial F test compares two models Type I F test is for variables added in order The current model is compared to the previous model with one IV less Order of entry matters Type III F test is for variables added last -The full model is compared to the model with all IVs except for the one IV currently under consideration Order of entry does not matter It is more important to know which models you are comparing than which F test (Type I or Type III) you use! Type I F test (using Type I Sum of Squares). This is the default in the anova function. anova(model_full) ## Analysis of Variance Table ## ## Response: salary ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## time 1 2123587818 2123587818 42.816 1.596e-08 *** ## cits 1 696719756 696719756 14.047 0.0004078 *** ## Residuals 59 2926312249 49598513 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Type III F test (using Type III Sum of Squares). Use the Anova function from the car package. car::Anova(model_full, type = &quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: salary ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 1.3185e+10 1 265.842 &lt; 2.2e-16 *** ## time 1.0834e+09 1 21.844 1.761e-05 *** ## cits 6.9672e+08 1 14.047 0.0004078 *** ## Residuals 2.9263e+09 59 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.2 Testing the Partial Regression Coefficients Just as in Simple Linear Regression, you can test the significance of a partial regression coefficient (slope) b to see if it is significantly different from zero. Recall the Variance of Estimate: The variance of estimate indicates the variance of the scores about the regression line. It is the variance of the residuals. \\(s_{y.x}^{2}=\\frac{\\sum{{{(Y-{\\hat Y})}^{2}}}}{n-k-1}=\\frac{S{{S}_{res}}}{n-k-1}\\) Recall: \\(M{{S}_{res}}=\\frac{\\sum{{{(Y-{\\hat Y})}^{2}}}}{n-k-1}=\\frac{S{{S}_{res}}}{d{{f}_{res}}}\\) Recall the Standard Error of Estimate: The standard error of estimate is the square root of the variance of estimate or the standard deviation of the residuals. \\[{{s}_{y.x}}=\\sqrt{\\frac{\\sum{{{(Y-{\\hat Y})}^{2}}}}{n-k-1}}=\\sqrt{\\frac{S{{S}_{res}}}{n-k-1}}\\] When testing whether the partial slope or partial regression coefficient is significantly different from zero, the standard error associated with the slope must be calculated. This differs slightly from the formula used in Simple Linear Regression due to the shared variance between the independent variables that must be taken into account: \\({{s}_{b1}}=\\sqrt{\\frac{s_{y.x}^{2}}{\\sum{x_{1}^{2}(1-r_{12}^{2})}}}=\\frac{{{s}_{y.x}}}{\\sqrt{\\sum{x_{1}^{2}(1-r_{12}^{2})}}}\\) where \\(s_{b1}\\) is the standard error associated with b1, \\(s_{y.x}^{2}\\) is the variance of estimate, \\(s_{y.x}\\) is the standard error of estimate, and \\(\\sum\\limits_{{}}{x_{1}^{2}}\\) is the sum of squares for the independent variable \\(X1\\). \\({{s}_{b2}}=\\sqrt{\\frac{s_{y.x}^{2}}{\\sum{x_{2}^{2}(1-r_{12}^{2})}}}=\\frac{{{s}_{y.x}}}{\\sqrt{\\sum{x_{2}^{2}(1-r_{12}^{2})}}}\\) where \\(s_{b2}\\) is the standard error associated with b2, \\(s_{y.x}^{2}\\) is the variance of estimate, \\(s_{y.x}\\) is the standard error of estimate, and \\(\\sum\\limits_{{}}{x_{2}^{2}}\\) is the sum of squares for the independent variable \\(X2\\). \\(s_b\\) is the standard error (standard deviation of the sampling distribution of b) and is used when testing the significance of the b using the t ratio: \\(t=\\frac{b}{{{s}_{b}}}=\\frac{b-0}{{{s}_{b}}}\\) with df of n  k  1. \\[{{t}_{b1}}=\\frac{{{b}_{1}}}{{{s}_{b1}}}\\] \\[{{t}_{b2}}=\\frac{{{b}_{2}}}{{{s}_{b2}}}\\] With knowledge of the standard error of the partial bs, we can create a confidence interval around each of the partial regression coefficients: \\(b\\pm {{t}_{(\\alpha /2,df)}}{{s}_{b}}\\). 5.3 Partial and Semi-paritial (Part) Correlations Questions Typically Asked in Multiple Regression How well does a group of independent variables together estimate Y? R Square or Adjusted R Square may be used to answer this question. In the professors salary example, time since Ph.D. was earned and number of citations accounted for/explained 49% or 47% ( \\(R^2 = .491\\); \\(R_{adj}^{2}=.474\\)) of the variance in Professors Salary. The Coefficient of Multiple Alienation = \\(1-{{R}^{2}}=1-.491=.509\\) is the proportion of variance in Salary not explained by/associated with Time and Citations. Fifty-one percent of the variance in Salary is not associated with Time and Citations. The F-test indicates significance of R Square. Questions Typically Asked in Multiple Regression How much does any single variable add to the estimation of Y already explained by other variables? The Semipartial (or Part) Correlation may be used to answer this question. A semipartial correlation is a correlation between Y and an independent variable/predictor from which the other independent variables/predictors have been partialled. It is the correlation between the dependent variable and an independent variable when the linear effects of the other independent variables in the model have been removed from the independent variable. It is related to the change in R square when a variable is added to an equation. Thus, the semipartial correlation for Time (X1) is the correlation between Time (X1) and Salary (Y) with the association between Time (X1) and Citations (X2) removed, but the association between Citations (X2) and Salary (Y) is not removed ( \\({{r}_{Y({{X}_{1}}|{{X}_{2}})}}\\) ). The semipartial correlation for Citations (X2) is the correlation between Citations (X2) and Salary (Y) with the association between Citations (X2) and Time (X1) removed, but the association between Time (X1) and Salary (Y) is not removed ( \\({{r}_{Y({{X}_{2}}|{{X}_{1}})}}\\)). \\[a = sr_1^2 = {R^2} - r_{Y2}^2\\] \\[b = sr_2^2 = {R^2} - r_{Y1}^2\\] \\[r_{Y1}^2 = a + c\\] \\[r_{Y2}^2 = b + c\\] \\[{R^2} = a + b + c\\] - a and b are proportions of Y variance uniquely accounted for by X1 and X2, respectively and equal the squared semipartial correlation (the increase in R Square when one independent variable/predictor is added to the other independent variable/predictor). \\[s{{r}_{1}}=\\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{\\sqrt{1-r_{12}^{2}}}\\] \\[s{{r}_{2}}=\\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{\\sqrt{1-r_{12}^{2}}}\\] When other independent variables/predictors are held constant statistically, how much of Y does a given variable account for? The Partial Correlation may be used to answer this question. A Partial correlation is a correlation between Y and an independent variable/predictor while controlling for the other independent variables/predictors in the model. It is the correlation between the dependent variable and an independent variable when the linear effects of the other independent variables in the model have been removed from both. Thus, the partial correlation for Time (X1) is the correlation between Time (X1) and Salary (Y) while controlling for Citations (X2) or when removing the effects of Citations ( \\({{r}_{Y{{X}_{1}}|{{X}_{2}}}}\\)). The partial correlation for Citations (X2) is the correlation between Citations (X2) and Salary (Y) while controlling for Time (X1) or when removing the effects of Time ( \\({{r}_{Y{{X}_{2}}|{{X}_{1}}}}\\)). The squared partial correlation indicates how much of the Y variance not estimated by other predictors is estimated by this predictor. For Time (X1), it is the proportion of variance in Y not associated with Citations (X2) but is associated with Time (X1). For Citations (X2), it is the proportion of variance in Y not associated with Time (X1) but is associated with Citations (X2). \\[pr_{1}^{2}=\\frac{a}{a+e}=\\frac{{{R}^{2}}-r_{Y2}^{2}}{1-r_{Y2}^{2}}\\] \\[pr_{2}^{2}=\\frac{b}{b+e}=\\frac{{{R}^{2}}-r_{Y1}^{2}}{1-r_{Y1}^{2}}\\] 5.3.1 R Syntax mydata &lt;- import(&quot;data/profs.sav&quot;) corr.test(mydata$salary, mydata$time) corr.test(mydata$salary, mydata$cits) corr.test(mydata$time, mydata$cits) y &lt;- mydata$salary y.x2 &lt;- (lm(salary ~ cits, data = mydata))$residuals x1.x2 &lt;- (lm(time ~ cits, data = mydata))$residuals y.x1 &lt;- (lm(salary ~ time, data = mydata))$residuals x2.x1 &lt;- (lm(cits ~ time, data = mydata))$residuals cor(y.x2, x1.x2) # partial correlation between y and x1 controlling for x2 cor(y.x1, x2.x1) # partial correlation between y and x2 controlling for x1 cor(y, x1.x2) # semi-partial correlation between y and x1 controlling for x2 cor(y, x2.x1) # semi-partial correlation between y and x2 controlling for x1 5.3.2 Sum up Multiple regression analysis was used to examine how well time since Ph.D. was earned and the number of citations for professors would explain/predict their salary. Time since Ph.D. and number of citations together accounted for approximately 47% of the variance in Salary \\((R_{adj}^{2}=.474)\\), F(2, 59) = 28.43, p &lt; .001. Time since professors earned their Ph.D. was a statistically significant predictor of Salary, t(59) = 4.67, p &lt; .001, which accounted for 27% of the variance in Salary not accounted for by number of citations (pr = .520) and uniquely accounted for 19% of the variance in Salary (sr = .434). Holding number of citations constant, as time since professors earned their Ph.D. increased by 1 year, salary was estimated to increase by $1062 (95%: 607,19, 1516,34). Number of citations for professors was also a statistically significant predictor of salary, t(59) = 3.75, p &lt; .001, which accounted for 19% of the variance in salary not accounted for by time since Ph.D. was earned (pr = .439) and uniquely accounted for 12% of the variance in Salary (sr = .348). Holding time since Ph.D. was earned constant, as the number of citations for professors increased by 1, salary was estimated to increase by $212 (95% CI: 98.87, 325.36). Note: - The regular pearson correlation is also called zero-order correlation. - Section 6.6 of textbook is on partial and semi-partial correlations. 5.4 A Complete Example mydata &lt;- import(&quot;data/profs.sav&quot;) #View(mydata) mymodel &lt;- lm(salary ~ time + cits, data = mydata) summary(mymodel) confint(mymodel, level = 0.95) lm.beta(mymodel) mydata$y.pred &lt;- fitted(mymodel) mydata$y.res &lt;- resid(mymodel) head(mydata) new.obs &lt;- data.frame( time = 12, cits = 80 ) predict(mymodel, newdata = new.obs, interval = &quot;confidence&quot;) # Prediction interval for the mean predict(mymodel, newdata = new.obs, interval = &quot;prediction&quot;) # Prediction interval for the individual Sum up: Multiple regression analysis was used to examine how well time since Ph.D. was earned and the number of citations for professors would explain/predict their salary. Time since Ph.D. and number of citations together accounted for approximately 47% of the variance in Salary \\((R_{adj}^{2}=.474)\\), F(2, 59) = 28.43, p &lt; .001. Time since professors earned their Ph.D. was a statistically significant predictor of Salary, t(59) = 4.67, p &lt; .001. Holding number of citations constant, as time since professors earned their Ph.D. increased by 1 year, salary was estimated to increase by $1062 (95%: 607,19, 1516,34). Number of citations for professors was also a statistically significant predictor of salary, t(59) = 3.75, p &lt; .001. Holding time since Ph.D. was earned constant, as the number of citations for professors increased by 1, salary was estimated to increase by $212 (95% CI: 98.87, 325.36). For professors who earned their Ph.D. 12 years ago and who had 80 citations, it is predicted that their salary would be about $68783 (95% CI: $64185, $73382). For any professor who earned their Ph.D. 12 years ago and who had 80 citations, the predicted salary is about $68783 (95% CI: $53960, $83607). 5.5 Types of Multiple Regression Table 5.1: Types of Multiple Regression Type of Regression Alternative Name IV Entered IV Evaluated Stnadard simultaneous all IVs enter at once each evaluated in terms of what it contributes as though it is last and all others have already made their contribution Sequential hierarchical IVs enter in the order specified by the researcher evaluated in terms of what it contributes at the time it was entered (therefore, sometimes the weaker IVs are entered first Statistical entry order based solely on statistical criteria IVs are not evaluted in the same sense forward selection equation begins empty and each IV is entered one at a time criterion can be a significance level backward selection equation being full (all in) &amp; each IV deleted one at a time deleteion based on whether IV contributes substantially; if not, then delete stepwise selection equation starts out empty; IVs entered if they met statistical criterion, and deleted at any time when they no longer contribute a compromise between forward &amp; backward selection 5.5.1 Regression model selection based on statistics Use the step() function from the stats package (already installed with base) or the stepAIC function from the MASS pacakge. fit &lt;- lm(salary ~ time + cits + pubs, data = mydata) step1 &lt;- step(fit, direction = &quot;both&quot;) # stepwise ## Start: AIC=1102.26 ## salary ~ time + cits + pubs ## ## Df Sum of Sq RSS AIC ## - pubs 1 59458298 2926312249 1101.5 ## &lt;none&gt; 2866853951 1102.3 ## - time 1 468968436 3335822387 1109.7 ## - cits 1 634124345 3500978295 1112.7 ## ## Step: AIC=1101.53 ## salary ~ time + cits ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 2926312249 1101.5 ## + pubs 1 59458298 2866853951 1102.3 ## - cits 1 696719756 3623032005 1112.8 ## - time 1 1083431156 4009743405 1119.1 step2 &lt;- step(lm(salary ~ 1, data = mydata), scope = formula(fit), direction = &quot;forward&quot;) ## Start: AIC=1139.37 ## salary ~ 1 ## ## Df Sum of Sq RSS AIC ## + time 1 2123587818 3623032005 1112.8 ## + cits 1 1736876419 4009743405 1119.1 ## + pubs 1 1472195326 4274424497 1123.0 ## &lt;none&gt; 5746619823 1139.4 ## ## Step: AIC=1112.77 ## salary ~ time ## ## Df Sum of Sq RSS AIC ## + cits 1 696719756 2926312249 1101.5 ## + pubs 1 122053710 3500978295 1112.7 ## &lt;none&gt; 3623032005 1112.8 ## ## Step: AIC=1101.53 ## salary ~ time + cits ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 2926312249 1101.5 ## + pubs 1 59458298 2866853951 1102.3 step3 &lt;- step(fit, direction = &quot;backward&quot;) ## Start: AIC=1102.26 ## salary ~ time + cits + pubs ## ## Df Sum of Sq RSS AIC ## - pubs 1 59458298 2926312249 1101.5 ## &lt;none&gt; 2866853951 1102.3 ## - time 1 468968436 3335822387 1109.7 ## - cits 1 634124345 3500978295 1112.7 ## ## Step: AIC=1101.53 ## salary ~ time + cits ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 2926312249 1101.5 ## - cits 1 696719756 3623032005 1112.8 ## - time 1 1083431156 4009743405 1119.1 5.6 (A Few) Assumptions in Regression Linear Relationship Exists Between Independent Variables and Dependent Variable Homoscedasticity of Residuals (Residuals Have Constant Variance) Residuals are Normally Distributed Residuals are Independent Lack of Multicollinearity Note: See section 7.7.2.1 of textbook for a more complete list of assumptions 5.6.1 Linearity Plot residuals against each independent variable and against the predicted values. mydata &lt;- import(&quot;data/profs.sav&quot;) mymodel &lt;- lm(salary ~ time + cits, data = mydata) res &lt;- resid(mymodel) fitted &lt;- fitted(mymodel) ggplot(mydata, aes(x = time, y = res)) + geom_point() + geom_smooth(method = lm, se=FALSE) + geom_smooth(color=&quot;red&quot;) # Add a loess smoothed fit curve ggplot(mydata, aes(x = cits, y = res)) + geom_point() + geom_smooth(method = lm, se=FALSE) + geom_smooth(color=&quot;red&quot;) ggplot(mydata, aes(x = fitted, y = res)) + geom_point() + geom_smooth(method = lm, se=FALSE) + geom_smooth(color=&quot;red&quot;) Plot residuals against each independent variable and against the predicted values. The straight horizontal line (0-line) indicates where the residuals are zero. The mean of the residuals should be zero. The curved line is the loess fit line which follows the general trend of the data. If the relationship is linear, the loess fit line should not demonstrate any large or systematic deviations from the 0-line. Violation of linearity leads to biased regression coefficients and standard errors, resulting in incorrect significance tests and confidence intervals. You can also use the crPlots() function (Components + Residual Plots) or the ceresPlots() function from the car package mydata &lt;- import(&quot;data/profs.sav&quot;) mymodel &lt;- lm(salary ~ time + cits, data = mydata) crPlots(mymodel) #ceresPlots(mymodel) 5.6.2 Homoscedasticity (Constant Variance Assumption) Plot residuals against each independent variable and against the predicted values. mydata &lt;- import(&quot;data/profs.sav&quot;) mymodel &lt;- lm(salary ~ time + cits, data = mydata) res &lt;- resid(mymodel) fitted &lt;- fitted(mymodel) ggplot(mydata, aes(x = time, y = res)) + geom_point() + geom_smooth(method = lm, se=FALSE) ggplot(mydata, aes(x = cits, y = res)) + geom_point() + geom_smooth(method = lm, se=FALSE) ggplot(mydata, aes(x = fitted, y = res)) + geom_point() + geom_smooth(method = lm, se=FALSE) When examining these plots, you dont want to see a relationship between the variability of the residuals and either the independent variables or the predicted values. If homoscedasticity is voilated, the standard errors of estimates will be incorrect. Thus, significance tests (p-value, confidence interval) are also incorrect. However, regression coefficients will still be correctly estimated. Use the ncvTest() function from the car package. This function computes a score test of the hypothesis of constant error variance against the alternative that the error variance changes with the level of the response (fitted values), or with a linear combination of predictors. The ncvTest() function performs the original version of Breusch-Pagan test. ncvTest(mymodel) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 0.09487541, Df = 1, p = 0.75807 Use the bptest() function from the lmtest package. This function offers studentized Breusch-Pagan test and is more robust than the orignial B-P test. bptest(mymodel) ## ## studentized Breusch-Pagan test ## ## data: mymodel ## BP = 0.1054, df = 2, p-value = 0.9487 The spreadLevelPlot() function from the car package creates plots for examining the possible dependence of spread on level, or an extension of these plots to the studentized residuals from linear models. It also suggests power transformation for improving homoscedasticity. spreadLevelPlot(mymodel) # plot studentized residuals vs. fitted values ## ## Suggested power transformation: 0.6806045 Homoscedasticity Remedies When the homoscedasticity assumption is not met, variables can be transformed to stabilize the variance. Natural Log Transformation can be used to reduce the skewness of variables and to stabilize the variance (values must be strictly positive to apply the log-function). newy = log(y) Box-Cox transformation transforms a variable using the power \\(\\lambda\\) tha tis mostly likely to normalize the variable. Use the powerTransform() function in the car package. When \\(\\lambda = 0\\), it is the log transformation. summary(powerTransform(mydata$salary)) ## bcPower Transformation to Normality ## Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd ## mydata$salary -0.2629 0 -1.4766 0.9507 ## ## Likelihood ratio test that transformation parameter is equal to 0 ## (log transformation) ## LRT df pval ## LR test, lambda = (0) 0.1808747 1 0.67062 ## ## Likelihood ratio test that no transformation is needed ## LRT df pval ## LR test, lambda = (1) 4.20349 1 0.040341 5.6.3 Normality How to check for Normality of residuals? Plot a histogram and the normal QQ plot of residuals Want the distribution of residuals to resemble the normal curve. mydata$fitted &lt;- fitted(mymodel) mydata$res &lt;- resid(mymodel) ggplot(mydata, aes(x=res)) + geom_histogram(aes(y=..density..), color = &quot;black&quot;, fill = &quot;white&quot;) + geom_density(alpha=.2, fill=&quot;#FF6666&quot;) Plot a histogram and the normal QQ plot of residuals Want the residuals (in circles) to be close to the straight line in the QQ plot. ggplot(mydata, aes(sample=res)) + stat_qq() + stat_qq_line(color = &quot;red&quot;) If normality is violated, standard errors are incorrect. Thus, significance tests (p-value, confidence interval) are also incorrect, especially for small samples (In large sample sizes, violation of this assumption is not a big problem). However, regression coefficients will still be correctly estimated. The Shapiro-Wilk test can also be used to check for normaility for small to moderate samples There a few other statistical tests for normaility: Kolmogorov-Smirnov (K-S), Anderson-Darling, DAgostino, Jarque-Bera. 5.6.4 Independence of Residuals A violation of this assumption typically occurs under 2 conditions: Clustering: When data are collected from groups or other clusters (e.g., people within different classes, people within different schools, people within different school districts, people who see different doctors, order of participation in a study, etc.). In this case, the residuals may be more similar within the clusters than between the clusters. Serial Dependency: When data are repeatedly collected from a single individual or the same sample of individuals over time (longitudinal studies), the residuals will often show serial dependency. Measures at adjacent times will tend to have more similar values. For clustering, plot residuals against the cluster variable. mydata$dept &lt;- as.factor(mydata$dept) # convert `dept` as a factor variable ggplot(mydata, aes(x=dept, y=res)) + geom_boxplot() Do the boxplots indicate much variability in the median value of thh residuals in each group? If there is clustering, the estimated regression coefficients are unbiased, but standard errors will typically be too small, leading to incorrect significance tests and confidence intervals. Dummy coded variables can be included as additional independent variables in the regression model to address clustering. Mulitivel modeling may be used as an alternative. For Serial Dependency, use Durbin-Watson Test. A value of 2 means the residuals are uncorrelated. A value less than 2 suggests positive autocorrelation; a value greater than 2 suggess negative autocorrelation. dwt(mymodel) ## lag Autocorrelation D-W Statistic p-value ## 1 -0.06087733 2.099968 0.67 ## Alternative hypothesis: rho != 0 5.6.5 Multicollinearity Multicollinearity: High correlations among some independent (predictor) variables which are included in the regression model. The more a predictor is correlated with other predictors, the less unique information it can contribute to the prediction of the dependent (criterion) variable. Exact Collinearity: Occurs when 1 independent variable has a correlation or a multiple correlation of 1.0 with the other independent variables. In this case, this independent variable cannot contribute any unique information that is not contained in the other independent variables. When exact collinearity occurs, there is no mathematically unique solution for the regression coefficients and regression is impossible. Example 1: X1 is weight of person in pounds; X2 is weight of person in kilograms Both variables contain the same information. Each is just a transformation of the other variable and will correlate perfectly. When running this regression, one of the variables will automatically be kicked out because they are perfectly correlated. \\({\\hat Y}=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}\\), where X1 and X2 are perfectly correlated (each X1 score is 2 points higher than its corresponding X2 score; X1 is a linear transformation of X2), and each of these predictors is correlated with the dependent variable at .967. Watch what happens: \\({{\\beta }_{1}}=\\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}=\\frac{0}{0}\\) \\({{\\beta }_{2}}=\\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{1-r_{12}^{2}}=\\frac{0}{0}\\) Example 2: X1 is score at time 1; X2 is score at time 2; X3 is difference between scores at time 1 and time 2 \\({\\hat Y}=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+{{b}_{3}}{{X}_{3}}\\) X3 can be perfectly predicted by X1 and X2. When running this regression, one of the variables would be kicked out. 5.6.5.1 measures of the degree of multicollinearity Squared correlation between two independent (predictor) variables. The closer the value is to 1.0, the more multicollinearity is indicated. With increasing numbers of predictor variables, this measure is more likely to miss substantial multicollinearity. Tolerance: A statistic used to determine how much the independent variables are linearly related to one another (multicollinear). It indicates the proportion of a variables variance not accounted for by other independent variables in the equation. A variable with very low tolerance contributes little information to a model, and can cause computational problems. It is calculated as 1 minus R squared for an independent variable when it is predicted by the other independent variables already included in the analysis: \\(1-R_{j}^{2}\\). If Tolerance is less than .10, severe multicollinearity for the corresponding variable exists. Some think this value is too lenient and recommend a cutoff value of .20 instead. Variance Inflation Factor: The reciprocal of the Tolerance: \\(\\frac{1}{\\text{Toleranc}{{\\text{e}}_{\\text{j}}}}\\). As the variance inflation factor increases, so does the variance of the regression coefficient, making it an unstable estimate. Large VIF values are an indicator of multicollinearity. VIF of 10 or higher indicates severe multicollinearity for the corresponding predictor variable. Some think this value is too lenient and instead recommend a cutoff value of 4. Condition Indices and Variance-Decomposition Proportions: These are results from a process known as singular value decomposition (SVD). Condition Number (CN) = \\(\\sqrt {\\frac{{{\\lambda _{\\max }}}}{{{\\lambda _{\\min }}}}}\\) Condition Index (CI) = \\(\\sqrt{\\frac{{{\\lambda }_{\\max }}}{{{\\lambda }_{i}}}}\\) Rule of thumb for CI: 0-10, weak dependency; 11-30, moderate dependency; &gt;30, strong dependency. CI is best used together with the variance-decomposition proportions. Variance-decomposition proportions are the proportions of variance of the intercept (a) and each of the regression coefficient (b) association with each CI. mydata &lt;- import(&quot;data/profs.sav&quot;) mymodel &lt;- lm(salary ~ time + cits, data = mydata) #library(car) vif(mymodel) ## time cits ## 1.161517 1.161517 5.6.5.2 problems of multicollinearity Unstable regression coefficients that are associated with large standard errors. \\({{t}_{1}}=\\frac{{{b}_{1}}-0}{S{{E}_{1}}}\\tilde{\\ }df=n-k-1\\) where \\(S{{E}_{1}}=\\frac{s{{d}_{res}}}{\\sqrt{\\sum{x_{1}^{2}(1-r_{12}^{2})}}}\\) where \\({sd_{res}}\\) is the standard deviation of the residuals, and the denominator is the square root of the product of the sum of squares of X scores and the variance not shared between X1 and X2, \\(1-{r_{12}^{2}}\\). Smaller standard errors represent better estimation of the true population parameters using our sample estimates of \\({{b}_{1}}\\) and \\({{b}_{2}}\\), meaning that they wont vary much from sample to sample. All other things being equal, as the correlation between X1 and X2 increases, the standard error increases, thus, decreasing our likelihood of a significant finding. Another problem is difficulty when interpreting regression coefficients due to the shared variance between X1 and X2. 5.7 Multiple Regression and Patterns of Association Complete Independence in Multiple Regression When the predictors are all independent (not correlated), the following relationship holds: \\({{R}^{2}}=r_{Y1}^{2}+r_{Y2}^{2}+\\cdots +r_{Yk}^{2}\\). The sum of the squared correlations between each independent variable and the criterion sum up to the multiple regression models \\(R^2\\). 5.7.1 Complete independence \\({{r}_{Y1}}=.5\\) \\({{r}_{Y2}}=.5\\) \\({{r}_{X1X2}}=0.0\\) Recall: \\({{R}^{2}}=\\frac{r_{Y1}^{2}+r_{Y2}^{2}-2{{r}_{Y1}}{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}\\) \\(a=r_{Y1}^{2}=sr_{1}^{2}=\\beta _{1}^{2}\\) and \\(b=r_{Y2}^{2}=sr_{2}^{2}=\\beta _{2}^{2}\\) Recall: \\(s{{r}_{1}}=\\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{\\sqrt{1-r_{12}^{2}}}\\) \\(s{{r}_{2}}=\\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{\\sqrt{1-r_{12}^{2}}}\\) \\({{\\beta }_{1}}=\\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}\\) \\({{\\beta }_{2}}=\\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{1-r_{12}^{2}}\\) In this case, the squared semi-partial correlation for each predictor is equal to the squared value of its correlation with the dependent variable and the squared value of its associated standardized regression coefficient. 5.7.2 Partial redundancy An indication of this occurrence is when \\({{r}_{Y1}}&gt;{{r}_{Y2}}{{r}_{12}}\\) and \\({{r}_{Y2}}&gt;{{r}_{Y1}}{{r}_{12}}\\). The semi-partial correlation and the standardized partial regression coefficient for each predictor will be smaller than its correlation with the dependent/criterion variable. 5.7.3 Complete redundancy This does not necessarily mean that X1 and X2 are perfectly correlated. It implies that adding X2 to a regression containing X1 does not increase \\({{R}^{2}}\\) at all. A predictor variable may correlate highly with the criterion, but this correlation becomes zero when controlling for the other predictor variable. 5.7.4 Suppression in multiple regression Suppression is present when either \\({{r}_{Y1}}&lt;{{r}_{Y2}}{{r}_{12}}\\) or \\({{r}_{Y2}}&lt;{{r}_{Y1}}{{r}_{12}}\\), or when the correlation between independent variables is negative and the correlations between each predictor and the criterion are positive. The standardized partial regression coefficient for each predictor will be larger than its correlation with the dependent/criterion variable and one of them may become negative. The relationship between the independent variables is hiding (suppressing) their real relationships with the dependent variable, which would be larger or possibly of opposite sign if the independent variables were not correlated. The inclusion of the suppressor in the regression equation removes (suppresses) the unwanted variance in X1, in effect, and enhances the relationship between X1 and Y. It removes the irrelevant variance that it shares with the independent variable and not with the dependent variable. If a variable with a positive correlation or a correlation of zero with the dependent variable has a significantly negative partial regression coefficient, it is a suppressor variable. 5.7.5 Summary of patterns of association Relationship Between Predictors Possible Effects on Statistics Complete Independence \\(r_{Y1}^2 = sr_1^2 = \\beta _1^2\\) and \\(r_{Y2}^2 = sr_2^2 = \\beta _2^2\\) Partial Redundancy \\(sr_1\\&amp;amp; {\\beta _1} &amp;lt; {r_{Y1}}\\) and \\(sr_2\\&amp;amp; {\\beta _2} &amp;lt; {r_{Y2}}\\); \\({r_{Y1}} &amp;gt; {r_{Y2}}{r_{12}}\\) and \\({r_{Y2}} &amp;gt; {r_{Y1}}{r_{12}}\\) Complete Redundancy \\(s{r_1}\\&amp;amp; {\\beta _1} \\approx 0\\) and \\(s{r_2}\\&amp;amp; {\\beta _2} &amp;gt; 0\\); OR \\(s{r_1}\\&amp;amp; {\\beta _1} &amp;gt; 0\\) and \\(s{r_2}\\&amp;amp; {\\beta _2} \\approx 0\\) and both predictors highly correlated with criterion, Suppression \\({\\beta _1} &amp;gt; {r_{Y1}}\\), and \\({\\beta _2} &amp;gt; {r_{Y2}}\\); \\({\\beta _1}\\) or \\({\\beta _2}\\) may become negative even if correlation with criterion is positive; correlation between predictors is negative and correlation between each predictor and criterion is positive; correlation between predictor and criterion is zero but has a significant negative partial regression coefficient. 5.7.6 Statistical paradoxes Suppression is a statistical paradox. The classical definition of suppression is that a potential covariate that is unrelated to the outcome variable (i.e. has a bivariate correlation of zero) increases the overall model fit within regression (as assessed by \\(R_2\\), for instance) when this covariate is added to the model. This seems counter-intuitive. Two other famous statistical paradoxes are: Simpsons Paradox: An association between two variables in a population emerges, disappears or reverses when the population is divided into subpopulations. Lords paradox: The relationship between a continuous outcome and a categorical variable being reversed when an additional continuous covariate is introduced to the analysis. These paradoxes are not problematic from the perspective of mathematics and probability theory but can be surprising for many people. Statistical modeling techniques such as structural equation modeling and multilevel modeling can help understand these paradoxes. To dig deeper, you probably want to read or take a course on causal inference. 5.8 Regression Diagnostics Outliers Diagnostics Outliers: One or more atypical points that do not fit with the data. case time pubs 1 3 18 2 6 3 3 3 2 4 8 17 5 9 11 6 60 6 7 16 38 8 10 48 9 2 9 10 5 22 11 5 30 12 6 21 13 7 10 14 11 27 15 18 37 Just skimming the data, case number 6 appears to be an outlier. Three characteristics of badly behaved data: - Leverage: How unusual is the case in terms of its values on the independent (predictor) variable? Discrepancy: Distance between predicted and observed values on the outcome variable. Influence: Reflects amount that regression coefficients would change if the outlier were removed from the data set. 5.8.1 Leverage Leverage: Reflects only the cases standing on the set of independent (predictor) variables. For each case, leverage tells us how far observed values for the case are from the mean values on the set of independent variables.  FYI: For one independent variable, leverage = \\({{h}_{i}}=\\frac{1}{n}+\\frac{{{({{X}_{i}}-\\bar{X})}^{2}}}{\\sum{{{x}^{2}}}}\\) where \\({{h}_{i}}\\) is the leverage for case i, n is the number of cases, \\({{X}_{i}}\\) is the score for case i on the predictor variable, \\(\\bar{X}\\) is the mean of X, and \\(\\sum{{{x}^{2}}}\\)is the sum over the n cases of the squared deviations of \\({{X}_{i}}\\) from the mean. For more than one independent variable, leverage = \\(\\mathbf{H}=\\mathbf{X}{{(\\mathbf{{X}&#39;X})}^{-\\mathbf{1}}}\\mathbf{{X}&#39;}\\) where X is the n x (k + 1) matrix of X values. These can range from 1/n to 1.  - Cases with leverage values greater than 2-3 times the average value are considered to have high leverage model &lt;- lm(pubs ~ time) hi &lt;- hatvalues(model) hi[(hi &gt; 3 * mean(hi))] ## 6 ## 0.9044069 5.8.2 Discrepancy Discrepancy: Extremity on the dependent variable. Distance between the predicted and observed values on the dependent variable. Use Studentized Residuals. The outlierTest() function from the car package tests the single largest residual for significance as an outlier. If it isnt significant, there are no outliers in the data. If it is significant, you must delete it and rerun the test to see if others are present. rstudent(model) # studentized residuals ## 1 2 3 4 5 6 7 8 9 10 11 ## -0.17131041 -1.28478010 -1.39891896 -0.21829507 -0.64134592 -3.29427276 1.37772714 2.35025411 -0.83472973 0.11829003 0.69147074 ## 12 13 14 15 ## 0.05234216 -0.72571300 0.49473073 1.30864113 # library(car) outlierTest(model) ## No Studentized residuals with Bonferroni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferroni p ## 6 -3.294273 0.0064084 0.096126 In large samples, cases with studentized deleted residuals greater than ± 3 or ± 4 are considered to have large discrepancy. In small samples, cases with studentized deleted residuals greater than ± 2 are considered to have large discrepancy. 5.8.3 Influence Influence: Combines information from measures of leverage and discrepancy to inform us how the regression equation would change if a case were removed from the data set. Cooks D: A measure of how much the residuals of all cases would change if a particular case were excluded from the calculation of the regression coefficients. FYI: Cooks \\({{D}_{i}}=\\frac{\\sum{{{({\\hat Y}-{{{{\\hat Y}}}_{(i)}})}^{2}}}}{(k+1){{MS}_{res}}}\\) where \\({\\hat Y}\\) is the predicted value of Y with all the cases included, \\({{{\\hat Y}&#39;}_{(i)}}\\) is the predicted value of Y with case i deleted, k is the number of independent variables, and \\({{MS}_{Res}}\\) is the Mean Square residual with all the cases included. Thus, it compares the predicted value of Y with case i included and deleted for all cases in the data set. These differences are squared and then summed. The denominator serves to standardize the value. Values range upward from its potential minimum value of zero. The value will always be positive. Case with Cooks D values greater than 1 or \\[4/(n - k - 1)\\] are considered high influential observations. Get Cooks D di &lt;- cooks.distance(model) summary(di) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000123 0.005616 0.021811 1.984229 0.072340 29.203927 di[di &gt;1] ## 6 ## 29.20393 di[di &gt; 4/(length(pubs)-1-1)] ## 6 ## 29.20393 Additional Influence Measures: Standardized difference in fit. Indicates how much the predicted value for case i would change if case i were deleted from the data set. Standardized DFBETA. Compares regression coefficients when case i is included versus not included in the data set. Use ?influence.measures to see more 5.8.4 Sources of Outliers and Remedies Sources of outliers: Error of execution of the research procedure (e.g., interviewer misread questions; experimenter delivered wrong treatment). Inaccurate measurement of the dependent variable (e.g., equipment failed). Errors in recording or keying of the data (e.g., wrote down wrong response; data entered into computer incorrectly). Errors in calculation of the measures (e.g., incorrectly add up number of responses). Nonattentive participants (e.g., participant fatigue, illness). Rare cases (e.g., outlying observations are correct; represent a valid but rare observation in the population). Remedies: Contaminated outlying data points (discussed in 1 through 5 above) are easy to correct. Correct, delete, or replace them as is appropriate. It is when you encounter rare cases it becomes more difficult. Option 1. Delete the outlier cases and reanalyze the remaining data. The regression coefficients typically wont change much, hopefully. In this case, researchers typically discuss findings with the outlier included, but do mention (sometimes in a footnote) the results with outliers deleted and the nature of the outliers. If the results do change drastically when deleting outlying cases, researchers may need to use other options. Option 2. Correct specification of the model needs to be ensured (e.g., does another variable need to be included to fit the data better? Is there a curvilinear relationship? Does an interaction term need to be included?). Option 3. Transformation of the data. Transforming variables can also sometimes help with violation of normality, linearity and homoscedasticity. Option 4. Robust regression procedures, such as Least Absolute Deviation estimation or Least Trimmed Squares estimation. 5.9 Curvilinear Regression Curvilinear regression analysis is the modeling of nonlinear relationships between predictor variables and the criterion. Linear regression refers to the regression models that take the form: \\(Y=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+...+{{b}_{k}}{{X}_{k}}\\) More formally, the regression model above is said to be linear in the parameters (or linear in the coefficients), where the parameters refer to the intercept and coefficients, \\(a\\), \\({{b}_{1}}\\), \\({{b}_{2}}\\), , \\({{b}_{k}}\\). If a regression equation is linear in the parameters, then the predicted Y value is a linear combination of the predictor variables. There are four broad classes of approaches to examining nonlinear relationships in multiple regression. Polynomial regression is traditional and commonly used in behavioral sciences. Second is the use of monotonic nonlinear transformations such as logarithms or exponents. Third is nonlinear regression in which the central point of the analysis is estimation of complex (nonlinear) relationships among variables that may be implied by theory. Fourth are nonparametric regression approaches. 5.9.1 Polynomial regression Power polynomials are a convenient method of fitting curves. In a polynomial regression equation, the predictor is raised to one or more powers. The highest power to which a predictor variable is raised is known as the degree or order of the polynomial equation. Linear: \\({\\hat Y}=a+{{b}_{1}}X\\) Quadratic: \\({\\hat Y}=a+{{b}_{1}}X+{{b}_{2}}{{X}^{2}}\\) Cubic: \\({\\hat Y}&#39;=a+{{b}_{1}}X+{{b}_{2}}{{X}^{2}}+{{b}_{3}}{{X}^{3}}\\) Quartic: \\({\\hat Y}=a+{{b}_{1}}X+{{b}_{2}}{{X}^{2}}+{{b}_{3}}{{X}^{3}}+{{b}_{4}}{{X}^{4}}\\) Quintic: \\({\\hat Y}=a+{{b}_{1}}X+{{b}_{2}}{{X}^{2}}+{{b}_{3}}{{X}^{3}}+{{b}_{4}}{{X}^{4}}+{{b}_{5}}{{X}^{5}}\\) The order, or degree of the polynomial indicates the number of bends in the regression curve. The highest possible degree that a given model may take is (g-1), where g is the number of distinct values of the predictor variable. The researcher is usually interested in identifying the highest-order model that fully describes the data. A regression model can be built that comprises of polynomial equations of increasing order until going further doesnt account for a significant amount of incremental variance in the criterion variable (i.e., adding a higher-order term doesnt contribute significantly to overall prediction above and beyond all lower order terms that are already in the model). However, from a research perspective, the nonlinear curve fitting with power polynomials should make substantive sense. Theory should guide the choice and for the most part theory in social sciences predicts quadratic, and at most cubic relationships. In addition, coefficients of higher order polynomials (above cubic) are difficulty to interpret. Center the (continuous) predictor variables. Lower order coefficients in higher order regression equations only have meaningful interpretation if the variable with which we are working has a meaningful zero. By centering ( \\(X - \\bar X\\)), zero on the predictor variable is its mean. DO NOT CENTER THE DEPENDENT VARIABLE. In order that higher order terms have meaning, all lower order terms must be included, since higher order terms are reflective of the specific level of curvature they represent only if all lower order terms are partialed out. Ploynomial regression is carried out as an ordinary hierarchical regression analysis. mydata$newtime &lt;- mydata$time - mean(mydata$time) mymodel2 &lt;- lm(salary ~ newtime + cits + I(newtime^2), data = mydata) summary(mymodel2) ## ## Call: ## lm(formula = salary ~ newtime + cits + I(newtime^2), data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14925.6 -4293.6 -527.9 3908.2 22528.9 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46350.421 2524.930 18.357 &lt; 2e-16 *** ## newtime 1085.629 298.989 3.631 0.000599 *** ## cits 212.495 57.155 3.718 0.000455 *** ## I(newtime^2) -4.579 36.862 -0.124 0.901573 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7102 on 58 degrees of freedom ## Multiple R-squared: 0.4909, Adjusted R-squared: 0.4646 ## F-statistic: 18.64 on 3 and 58 DF, p-value: 1.377e-08 What if the time variable was not centered? 5.10 Useful Functions and Symbols Commonly Used in R When Fitting Linear Models Table 5.2: Useful Functions in R When Fitting Linear Models Function Action summary() Displays detailed results for the fitted model coefficients() Lists the model parameters (intercept and slopes) for the fitted model confint() Provides confidence intervals for teh model parameters (95% by default) fitted() Lists the predicted values in a fitted model residuals() Lists the residual values in a fitted model anova() Generates an ANOVA table for a fitted model, or an ANOVA table comparing two or more fitted models vcov() Lists the covariance matrix for model parameters AIC() Prints Akaikes Information Criterion plot() Generates diagnostic plots for evaluating the fit of a model predict() Uses a fitted model to predict response values for a new dataset Table 5.3: Symbols Commonly Use in R Formulas Symbol Usage ~ Separates response variables on the left from the explanatory varialbes on the right. For example, a prediction of y from x, z, and w would be coded y ~ x + z + w. Separates predictor variables. : Denotes an interaction between predictor variables. A prediction of y from x, z, and the interaction between x and z would be coded y ~ x + z + x:z. A shortcut for denoting all possibel interactions. The code y ~ xzw expands to y ~ x + z + w + x:z + x:w + z:w + x:z:w. ^ Denotes interactions up to a specified degree. The code y ~ (x + z + w)^2 expands to y ~ x + z + w + x:z + x:w + z:w. . A placeholder for all other variables in the data frame except the dependent variable. For example, if a data frame contained the varibels x, y, z, and w, then the code y ~ . would expands to y ~ x + z + w. A minus sign removes a variable from teh equation. For example, y ~ (x + z + w)^2 - x:w expands to y ~ x + z + w + x:z + z:w. -1 Supresses the intercept. For example, the formula y ~ x -1 fits a regression of y on x, and forces the line through the origin at x=0. I() Elements within the parentheses are interpreted arithmetically. For example, y ~ x + (z + w)^2 would expand to y ~ x + z + w + z:w. In contrast, y ~ x + I(z + w)^2 would expand to y ~ x + h, where h is a new variable created by squaring the sum of z and w. function Mathematical functions cna be used in formulas. For example, log(y) ~ x + z + w would predict log(y) from x, z, and w 5.11 Interactions in Regression Interactions Among Continuous Predictors: Interactions represent an interplay among predictors that produces an effect on the outcome of Y that is different from the sum of the effects of the individual predictors. Consider how ability and motivation impact achievement in graduate school. The combined impact of ability and motivation on achievement equals the sum of their separate parts. Additive effects are indicated by separate independent variables in the regression equation: \\({\\hat Y}=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}\\) . Additive means that the regression of the criterion on one predictor is constant over all values of the other predictor. The whole equals the sum of their separate parts. What if the impact of ability and motivation as a whole is greater than the sum of their separate parts? When two predictors interact, the regression of Y on one of the predictors depends on or is conditional on the value of the other predictor. One alternative is that ability and motivation may interact such that graduate students with both high ability and high motivation achieve much more in graduate school than would be expected from the simple sum of the separate effects of ability and motivation. Another alternative is that ability and motivation compensate for one another in that students with high ability have less motivation to achieve and students with low ability have more motivation to achieve. We want to consider the effects of ability, motivation, and the interaction between ability and motivation on achievement. The interaction between \\({{X}_{1}}\\) and \\({{X}_{2}}\\) can be tested in multiple regression to determine the amount of variance in the criterion variable it accounts for over and above any additive combination of their separate effects: \\({\\hat Y}=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+{{b}_{3}}{{X}_{1}}{{X}_{2}}\\). The interaction term is simply the product of \\({{X}_{1}}\\) and \\({{X}_{2}}\\). a is the intercept (the predicted value of \\(Y\\) when \\({{X}_{1}}\\) and \\({{X}_{2}}\\) are zero); \\({{b}_{1}}\\) represent the unit change in \\(Y\\) with a 1-unit increase in \\({{X}_{1}}\\), holding other variable constant; \\({{b}_{2}}\\) represents the unit change in \\(Y\\) with a 1-unit increase in \\({{X}_{2}}\\), holding other variables constant; \\({{b}_{3}}\\) represents the interaction between \\({{X}_{1}}\\) and \\({{X}_{2}}\\), holding other variables constant. To Create an Interaction Term Center the predictor variables (always center the continuous predictor variables when an interaction term is to be included in the regression equation, unless a predictor variable has a meaningful zero point). Centering simply means subtracting the mean value of a predictor variable from each value of that predictor variable ( \\({{X}_{1}}-{{\\bar{X}}_{1}}\\)). .red[Do not center the dependent variable.] Multiply the centered variables \\({{X}_{1}}\\) and \\({{X}_{2}}\\), creating an interaction term for \\({{X}_{1}}\\) and \\({{X}_{2}}\\) to be included in the multiple regression equation.  Centering variables allows a meaningful interpretation of regression coefficents. When you center variables, those who are at the mean of a predictor variable get a value of zero on the predictor variable. For simple effects (e.g., \\({{X}_{1}}\\) alone), you interpret partial regression coefficients as the change in \\(Y\\) with a 1-unit increase in the predictor variable for people at the mean of the \\({{X}_{2}}\\) predictor variable: \\({Y}&#39;=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+{{b}_{3}}{{X}_{1}}{{X}_{2}}\\). Centering does not change the overall fitting of the model. Therefore, the F, and \\(R^2\\) values stay the same. 5.11.1 An example Regress Physical Endurance (minutes on treadmill test) on Age ( \\(X_1\\)) and Number of Years of Vigorous Physical Exercise ( \\(X_2\\)). endurance &lt;- import(&quot;data/endurance.sav&quot;) # str(endurance) attach(endurance) newage &lt;- scale(age, scale=FALSE) #&lt;&lt; newyears &lt;- scale(years, scale=FALSE) detach(endurance) mymodel1 &lt;- lm(endurance ~ newage + newyears + newage:newyears, data = endurance) mymodel2 &lt;- lm(endurance ~ newage*newyears, data = endurance) # alternative formula # mymodel2 &lt;- lm(endurance ~ I(age-49.18)*I(years-10.67), data = endurance) # use the `I()` symbol summary(mymodel1) ## ## Call: ## lm(formula = endurance ~ newage + newyears + newage:newyears, ## data = endurance) ## ## Residuals: ## Min 1Q Median 3Q Max ## -21.165 -6.939 0.269 6.300 21.299 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.88872 0.64662 40.037 &lt; 2e-16 *** ## newage -0.26169 0.06406 -4.085 6.01e-05 *** ## newyears 0.97272 0.13653 7.124 1.20e-11 *** ## newage:newyears 0.04724 0.01359 3.476 0.000604 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.7 on 241 degrees of freedom ## Multiple R-squared: 0.2061, Adjusted R-squared: 0.1962 ## F-statistic: 20.86 on 3 and 241 DF, p-value: 4.764e-12 The overall regression of Endurance on Age, Years, and Age*Years: \\({\\hat Y}=25.89-.262newage+.973newyears+.047newage*newyears\\). All of the partial regression coefficients are significantly different from zero, controlling for the other variables in the model. For every 1 year increase in age, there is an estimated loss of endurance of .26 minutes for people at the mean level of years of exercise in the sample. For every 1 year increase in years of vigorous exercise, there is an estimated increase in endurance of .97 minutes for people at the mean level of age in the sample. The interaction indicates that the decline in endurance with age depends on a history of rigorous exercise. To Interpret Interactions: Write the simple regression equation for the regression of the criterion on one predictor at a specific value of the other predictor variable, then plot these values. For example, write the simple regression equations for the regression of Physical Endurance on Age at three levels of Years of Rigorous Exercise. The three levels are typically 1) at the mean of the other predictor (when centered Years = 0); 2) at 1 standard deviation above the mean (when centered Years = 4.78); and 3) at 1 standard deviation below the mean (when centered Years = -4.78). psych::describe(endurance) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## case 1 245 124.99 72.48 124 124.86 91.92 1 250 249 0.02 -1.21 4.63 ## age 2 245 49.18 10.11 48 49.11 10.38 20 82 62 0.15 -0.08 0.65 ## years 3 245 10.67 4.78 11 10.56 4.45 0 26 26 0.27 0.23 0.31 ## endurance 4 245 26.53 10.82 27 26.39 10.38 0 55 55 0.11 -0.30 0.69 Multiple Regression Equation: \\({\\hat Y}=25.89-.262newage+.973newyears+.047newage*newyears\\) Rearrange the equation: \\({\\hat Y}=25.89+.973newyears+(-.262+.047newyears)*newage\\) For Low Years ( \\({{X}_{2}} = {newyear} = -4.78\\)): \\({\\hat Y}=25.89+.973(-4.78)+(-.262+.047(-4.78))*newage\\) \\({\\hat Y}=25.89-4.65-.487*newage =21.24-.487*newage\\) For Mean Years ( \\({{X}_{2}} = {newyear} = 0)\\)): \\({\\hat Y}=25.89+.973(0)+(-.262+.047(0))*newage = 25.89-.262*newage\\) For High Years ( \\({{X}_{2}} = {newyear} = 4.78\\)): \\({\\hat Y}=25.89+.973(4.78)+(-.262+.047(4.78))*newage\\) \\({\\hat Y}=25.89+4.65-.037*newage = 30.54-.037*newage\\) Low Years Exercise: \\({\\hat Y}=21.24-.487*newage\\) Mean Years Exercise: \\({\\hat Y}=25.89-.262*newage\\) High Years Exercise: \\({\\hat Y}=30.54-.037*newage\\) Solve for the predicted Y values at .red[low] years exercise for each age level (low age = -10.11; mean age = 0; high age = 10.11). Notice that the low to high levels represent 1 standard deviation below the mean (-10.11), the mean (0), and 1 standard deviation above the mean (10.11): Low Years/Low Age: \\({\\hat Y} =21.24-.487(-10.11)=26.16\\) Low Years/Mean Age: \\({\\hat Y}=21.24-.487(0)=21.24\\) Low Years/High Age: \\({\\hat Y}=21.24-.487(10.11)=16.32\\) Solve for the predicted Y values at .red[mean] level of years exercise for each age level: Mean Years/Low Age: \\({\\hat Y}=25.89-.262(-10.11)=28.54\\) Mean Years/Mean Age: \\({\\hat Y}=25.89-.262(0)=25.89\\) Mean Years/High Age: \\({\\hat Y}=25.89-.262(10.11)=23.24\\) Solve for the predicted Y values at .red[high] level of years exercise for each age level: High Years/Low Age: \\({\\hat Y}=30.54-.037(-10.11)=30.91\\) High Years/Mean Age: \\({\\hat Y}=30.54-.037(0)=30.54\\) High Years/High Age: \\({\\hat Y}=30.54-.037(10.11)=30.17\\) Age Low Mean High Low 26.16 21.24 16.32 Excercise Mean 28.54 25.89 23.24 High 30.91 30.54 30.17 This indicates that the decline in physical endurance across the life span is less dramatic for those with high number of years of rigorous exercise, whereas there appears to be a dramatic decline in physical endurance across the life span for those with low or average number of years of rigorous exercise. Does including the interaction improve prediction? attach(endurance) newage &lt;- scale(age, scale=FALSE) newyears &lt;- scale(years, scale=FALSE) detach(endurance) mymodel1 &lt;- lm(endurance ~ newage + newyears + newage:newyears, data = endurance) mymodel3 &lt;- lm(endurance ~ newage + newyears, data = endurance) anova(mymodel3, mymodel1) #using Type I Sum of Squares ## Analysis of Variance Table ## ## Model 1: endurance ~ newage + newyears ## Model 2: endurance ~ newage + newyears + newage:newyears ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 242 23810 ## 2 241 22674 1 1136.5 12.08 0.0006042 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Use the interact_plot() function in the interactions package. By default, all continuous variables not involved in the interaction are mean-centered. The centered argument can be used to overwrite the default. However, the response variable, pred, modx, and mod2 variables are never centered. mymodel &lt;- lm(endurance ~ age*years, data = endurance) interactions::interact_plot(mymodel, pred = age, modx = years, plot.points = TRUE, point.shape=TRUE) 5.12 Categorical Independent Variables 5.12.1 Binary independent variables Examples of nominal or categorical variables include: religion, treatment groups in experiments, region of country, ethnic group, occupation, diagnosis, or marital status. One Independent Variable with Two Categories: Regression of Salary on Gender (Females coded as 1 and males coded as 0). profs &lt;- import(&quot;data/profs.sav&quot;) case dept time pubs cits salary gender 4 2 8 17 34 61863 0 6 1 6 6 37 47034 0 7 2 16 38 48 66432 0 8 1 10 48 56 61100 0 10 2 5 22 29 47454 0 14 1 11 27 40 59677 0 15 1 18 37 61 61458 0 16 2 6 8 32 54528 0 18 2 7 6 69 56600 0 22 2 5 7 35 62895 0 23 1 7 6 18 53740 0 24 1 13 69 90 75822 0 25 1 5 11 60 56596 0 27 2 8 20 27 62091 0 30 1 13 27 56 74199 0 31 2 5 14 50 50729 0 32 1 3 23 25 70011 0 34 1 3 7 1 39652 0 35 1 9 19 69 68987 0 36 1 3 11 69 55579 0 37 1 9 31 27 54671 0 38 2 3 9 50 57704 0 42 1 11 12 54 60009 0 43 2 5 9 47 58632 0 44 1 1 6 29 38340 0 45 2 21 39 69 71219 0 48 2 16 50 55 83503 0 51 1 5 5 42 53650 0 55 1 4 19 83 74343 0 56 2 4 11 49 57710 0 57 1 5 13 14 52676 0 62 1 4 4 28 58582 0 1 2 3 18 50 51876 1 2 1 6 3 26 54511 1 3 2 3 2 50 53425 1 5 2 9 11 41 52926 1 9 1 2 9 19 41934 1 11 1 5 30 28 49832 1 12 2 6 21 31 47047 1 13 1 7 10 25 39115 1 17 1 9 13 36 60327 1 19 2 7 12 47 52542 1 20 1 3 29 29 50455 1 21 1 7 29 35 51647 1 26 2 8 9 30 55682 1 28 2 7 41 35 42162 1 29 2 2 3 14 52646 1 33 2 1 1 35 37939 1 39 2 4 12 32 44045 1 40 2 10 32 33 51122 1 41 1 1 26 45 47082 1 46 2 7 16 47 53712 1 47 2 5 12 43 54782 1 49 2 5 18 33 47212 1 50 2 4 16 28 52840 1 52 2 11 20 24 50931 1 53 1 16 50 31 66784 1 54 1 3 6 27 49751 1 58 1 6 3 36 41195 1 59 2 4 8 34 45662 1 60 2 8 11 70 47606 1 61 1 3 25 27 44301 1 cor.test(profs$gender, profs$salary) # point-biserial correlation ## ## Pearson&#39;s product-moment correlation ## ## data: profs$gender and profs$salary ## t = -4.6439, df = 60, p-value = 1.912e-05 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.6770144 -0.3033909 ## sample estimates: ## cor ## -0.5141948 mymodel &lt;- lm(salary ~ as.factor(gender), data = profs) #&lt;&lt; 0 for male, 1 for female on gender summary(mymodel) ## ## Call: ## lm(formula = salary ~ as.factor(gender), data = profs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -21268.9 -5045.2 98.5 3270.3 23894.1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 59609 1484 40.173 &lt; 2e-16 *** ## as.factor(gender)1 -9906 2133 -4.644 1.91e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8394 on 60 degrees of freedom ## Multiple R-squared: 0.2644, Adjusted R-squared: 0.2521 ## F-statistic: 21.57 on 1 and 60 DF, p-value: 1.912e-05 The regression coefficient associated with gender is statistically significantly different from zero (Not surprising because we already know the correlation coefficient is significantly different from zero). Because gender in this exampel only has two categories, the regression coefficient represents the difference in mean salaries for males and females. Thus, the t-test in this situation tests whether the mean salary for males is significantly different from the mean salary for females. Because females were coded as 1 and males coded as 0 (dummy coding), and the regression coefficient is negative, we know that females have significantly lower salaries than males. If the regression coefficient was positive, it would indicate that females have significantly higher salaries than males. When a group is coded as zero, it is the reference/comparison group. The males are the reference group in this situation. Notice the value of the constant (Y-intercept). Because males are the reference group, the constant is the mean salary for males. Regression Equation: \\(\\hat Y = a + b(gender)\\). What does a represent? What does b represent? Regression Equation: \\({\\hat Y} = 59608.94 - 9905.90*gender\\) For males (gender = 0): \\({\\hat Y} = 59608.94 - 9905.90*(0) = 59608.94\\) For females (gender = 1): \\({\\hat Y} = 59608.94 - 9905.90*(1) = 49703.04\\) To sum this up: The Point-Biserial correlation was computed to examine the relationship between gender and salary level, which indicated that a moderate and significant relationship exists, \\(r_{pb}(60) = -.514\\) , p &lt; .001. As indicated by the correlational analysis, gender was a significant predictor of professors salary, \\(t(60) = -4.64\\), p &lt; .001, accounting for approximately 25% of the variance in salary ( \\(R_{adj}^2 = .252\\)). Female professors make significantly less money than male professors. On average, females make about $9906 less than males (95% CI: -14173, -5639, \\(\\beta = -.514\\)). Alternatively, the regression model can be specified in R as below. . mymodel2 &lt;- lm(salary ~ gender, data = profs) summary(mymodel2) ## ## Call: ## lm(formula = salary ~ gender, data = profs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -21268.9 -5045.2 98.5 3270.3 23894.1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 59609 1484 40.173 &lt; 2e-16 *** ## gender -9906 2133 -4.644 1.91e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8394 on 60 degrees of freedom ## Multiple R-squared: 0.2644, Adjusted R-squared: 0.2521 ## F-statistic: 21.57 on 1 and 60 DF, p-value: 1.912e-05 5.12.2 Categorical independent variables with more than two categories What if a variable has more than two categories? When a variable has more than two categories, it must be coded to provide meaningful results in the regression analysis. To code a variable with more than two categories or groups, g  1 variables must be created and used in the regression equation (where g = the number of categories or groups). For example, if we were interested in the effects of different religious backgrounds (Catholic, Protestant, Jewish, Other) on attitudes toward abortion, we would need to create 3 code variables (g  1 = 4  1 = 3). 5.12.3 An example Effects of religion on attitude toward abortion religion &lt;- import(&quot;data/religion.sav&quot;) case religion ata 1 C 61 2 O 78 3 P 47 4 C 65 5 C 45 6 O 106 7 P 120 8 C 49 9 O 45 10 O 62 11 C 79 12 O 54 13 P 140 14 C 52 15 P 88 16 C 70 17 C 56 18 J 124 19 O 98 20 C 69 21 P 56 22 J 135 23 P 64 24 P 130 25 J 74 26 O 58 27 P 116 28 O 60 29 J 84 30 P 68 31 P 90 32 P 112 33 J 94 34 P 80 35 J 110 36 P 102 table(religion$religion) ## ## C J O P ## 9 6 8 13 5.12.3.1 Dummy Coding One group is designated as the reference group and is assigned a value of 0 for every code variable. The reference group should serve as a useful comparison (e.g., a control group; the group expected to score the highest or lowest on Y; a standard treatment). The reference group should be well defined and not a catch all category (e.g., Other). The reference group should not have a very small sample size compared to the other groups. Table 5.4: Protestant as Reference Group: Code Variable Religion Catholic Jewish Other Protestant 0 0 0 Catholic 1 0 0 Jewish 0 1 0 Other 0 0 1 Catholic represents Catholic versus non-Catholic; Jewish represnts Jewish versus non-Jewish; and Other represtns other versus non-other (Catholic, Protestant, and Jewish combined) \\(\\hat Y = a + {b_1}Catholic + {b_2}Jewish + {b_3}Other\\) In R, dummy coding is called treatment constrasts. Its the default coding with the first group C as the reference group. religion$group &lt;- factor(religion$religion) contrasts(religion$group) # Catholic is the reference group ## J O P ## C 0 0 0 ## J 1 0 0 ## O 0 1 0 ## P 0 0 1 mymodel &lt;- lm(ata ~ group, data=religion) summary(mymodel)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 60.666667 7.806057 7.771743 7.301349e-09 ## groupJ 42.833333 12.342460 3.470405 1.508602e-03 ## groupO 9.458333 11.379186 0.831196 4.120223e-01 ## groupP 32.641026 10.154800 3.214344 2.982232e-03 coef(mymodel) ## (Intercept) groupJ groupO groupP ## 60.666667 42.833333 9.458333 32.641026 Manually setting the contrasts Jewish &lt;- c(0, 1, 0, 0) Other &lt;- c(0, 0, 1, 0) Protestant &lt;- c(0, 0, 0, 1) contrasts(religion$group) &lt;- cbind(Jewish, Other, Protestant) #add a `contrasts` attribute to `religion$group` religion$group ## [1] C O P C C O P C O O C O P C P C C J O C P J P P J O P O J P P P J P J P ## attr(,&quot;contrasts&quot;) ## Jewish Other Protestant ## C 0 0 0 ## J 1 0 0 ## O 0 1 0 ## P 0 0 1 ## Levels: C J O P mymodel2 &lt;- lm(ata ~ group, data=religion) summary(mymodel2)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 60.666667 7.806057 7.771743 7.301349e-09 ## groupJewish 42.833333 12.342460 3.470405 1.508602e-03 ## groupOther 9.458333 11.379186 0.831196 4.120223e-01 ## groupProtestant 32.641026 10.154800 3.214344 2.982232e-03 Change the reference group for dummy coding. contrasts(religion$group) &lt;-contr.treatment(n=4, base=4) # &quot;religion&quot; has four groups: C, J, O, P contrasts(religion$group) # Protestant is the reference group ## 1 2 3 ## C 1 0 0 ## J 0 1 0 ## O 0 0 1 ## P 0 0 0 mymodel2 &lt;- lm(ata ~ group, data=religion) summary(mymodel2)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 93.30769 6.495032 14.3660093 1.664683e-15 ## group1 -32.64103 10.154800 -3.2143444 2.982232e-03 ## group2 10.19231 11.557994 0.8818405 3.844393e-01 ## group3 -23.18269 10.523155 -2.2030173 3.491209e-02 Change the reference group for dummy coding. Manually setting the contrasts. Catholic &lt;- c(1, 0, 0, 0) Jewish &lt;- c(0, 1, 0, 0) Other &lt;- c(0, 0, 1, 0) contrasts(religion$group) &lt;- cbind(Catholic, Jewish, Other) #&lt;&lt; religion$group ## [1] C O P C C O P C O O C O P C P C C J O C P J P P J O P O J P P P J P J P ## attr(,&quot;contrasts&quot;) ## Catholic Jewish Other ## C 1 0 0 ## J 0 1 0 ## O 0 0 1 ## P 0 0 0 ## Levels: C J O P mymodel3 &lt;- lm(ata ~ group, data=religion) summary(mymodel3)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 93.30769 6.495032 14.3660093 1.664683e-15 ## groupCatholic -32.64103 10.154800 -3.2143444 2.982232e-03 ## groupJewish 10.19231 11.557994 0.8818405 3.844393e-01 ## groupOther -23.18269 10.523155 -2.2030173 3.491209e-02 Regression Equation: \\(\\hat Y = 93.31 - 32.64Catholic + 10.19Jewish - 23.18Other\\) Protestant: \\(\\hat Y = 93.31 - 32.64(0) + 10.19(0) - 23.18(0) = 93.31\\) Catholic: \\(\\hat Y = 93.31 - 32.64(1) + 10.19(0) - 23.18(0) = 60.67\\) Jewish: \\(\\hat Y = 93.31 - 32.64(0) + 10.19(1) - 23.18(0) = 103.50\\) Other: \\(\\hat Y = 93.31 - 32.64(0) + 10.19(0) - 23.18(1) = 70.13\\) \\(\\hat Y = a + {b_1}Catholic + {b_2}Jewish + {b_3}Other\\), What do \\(a\\), \\(b_1\\), \\(b_2\\), \\(b_3\\), and \\(b_4\\) represent? Sum this up: Multiple regression analysis was used to determine how well religion explains attitudes toward abortion. Religion was categorized as Protestant, Catholic, Jewish, or Other. Dummy-coding was used, creating three code variables (viz., Catholic, Jewish, and Other). Protestant was coded as the reference group. The analysis indicated that religion accounted for approximately 29% of the variance in attitudes toward abortion \\((R_{adj}^{2}=.29)\\), F(3, 32) = 5.87, p &lt; .01. There was a significant difference between Catholic and Protestant attitudes toward abortion, t(32) = -3.21, p &lt; .01. On average, Catholic attitudes toward abortion were about 33 points less favorable than Protestant attitudes toward abortion (95% CI: -53.33; -11.96). There was no significant difference between Jewish and Protestant attitudes toward abortion, t(32) = .882, p &gt; .05. There was a significant difference between Other and Protestant attitudes toward abortion, t(32) = -2.20, p &lt; .05. On average, Other attitudes toward abortion were about 23 points less favorable than Protestant attitudes toward abortion (95% CI: -44.62; -1.75; \\(\\beta = -.351\\)). Adding Other Variables to the Dummy Coded Model: \\(\\hat Y = a + {b_1}Catholic + {b_2}Jewish + {b_3}Other + {b_4}Gender + {b_5}Income\\) The dummy variables Catholic, Jewish, and Other are the same as before. Gender is 0 for male and 1 for female. Income is a continuous variable. .red[What does each of the regression coefficients represent?] 5.12.3.2 Effect Coding The reference point is the unweighted mean of all of the group means, which is the intercept. The unstandardized regression coefficients represent the difference between each groups mean and the unweighted mean. You must select a base group (a group for which comparisons with the mean are of least interest) and assign that group a value of -1. The results of multiple regression do not directly inform us about this base group (e.g., control) but may be obtained through subtraction. Useful with experimental designs. Table 5.5: Effect Coding Code Variable Religion Catholic Jewish Other Protestant -1 -1 -1 Catholic 1 0 0 Jewish 0 1 0 Other 0 0 1 Regression Equation: \\(\\hat Y = 81.90 - 21.23Catholic + 21.6Jewish - 11.78Other\\) The intercept is equal to the grand mean of the group means. Each regression coefficient is equal to the difference between the respective group mean and the grand mean of group means Catholic: 60.67  81.90 = -21.23 Jewish: 103.50  81.90 = 21.6 Other: 70.12  81.90 = -11.78 # Obtain the mean of `ata` by religion group aggregate(religion$ata, by = list(religion$group), FUN = mean) ## Group.1 x ## 1 C 60.66667 ## 2 J 103.50000 ## 3 O 70.12500 ## 4 P 93.30769 In R, effect coding is called sum contrasts (i.e., contrasts sum up to zero). Use the contr.sum function in the memisc package. # library(memisc) contrasts(religion$group) &lt;- memisc::contr.sum(n=4, base=4) # &quot;religion&quot; has four groups: C, J, O, P contrasts(religion$group) # Protestant is the reference group ## 1 2 3 ## C 1 0 0 ## J 0 1 0 ## O 0 0 1 ## P -1 -1 -1 mymodel4 &lt;- lm(ata ~ group, data=religion) summary(mymodel4)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 81.89984 4.054882 20.197835 8.897761e-20 ## group1 -21.23317 6.849039 -3.100168 4.015602e-03 ## group2 21.60016 7.883081 2.740066 9.961199e-03 ## group3 -11.77484 7.121639 -1.653389 1.080306e-01 Manually setting the sum contrasts Catholic &lt;- c(1, 0, 0, -1) Jewish &lt;- c(0, 1, 0, -1) Other &lt;- c(0, 0, 1, -1) contrasts(religion$group) &lt;- cbind(Catholic, Jewish, Other) religion$group ## [1] C O P C C O P C O O C O P C P C C J O C P J P P J O P O J P P P J P J P ## attr(,&quot;contrasts&quot;) ## Catholic Jewish Other ## C 1 0 0 ## J 0 1 0 ## O 0 0 1 ## P -1 -1 -1 ## Levels: C J O P mymodel5 &lt;- lm(ata ~ group, data=religion) summary(mymodel5)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 81.89984 4.054882 20.197835 8.897761e-20 ## groupCatholic -21.23317 6.849039 -3.100168 4.015602e-03 ## groupJewish 21.60016 7.883081 2.740066 9.961199e-03 ## groupOther -11.77484 7.121639 -1.653389 1.080306e-01 5.13 Interactions Between Categorical and Continuous Variables Center the continuous predictor variables and multiply the centered continuous variable by the categorical variables. For example, if we wanted to regress Endurance on Age, Gender (male = 0 vs. female =1), and Age*Gender, we would center the age variable and multiply that centered age variable by the gender variable: \\(\\hat Y = a + {b_1}Age + {b_2}Gender + {b_3}Age*Gender\\) If we wanted to regress Endurance on Age, Marital Status (Single, Married, Divorced/Separated, Widowed), and Age*Marital Status, we would first create 3 (g-1) dummy coded variables, center the age variable, and multiply the age variable by each of the three dummy coded variables: \\[\\begin{array}{l} \\hat Y = a + {b_1}Age + {b_2}Single + {b_3}Divorced/Separated + {b_4}Widowed + \\\\ {b_5}Age*Single + {b_6}Age*Divorced/Separated + {b_7}Age*Widowed \\end{array}\\] 5.14 An Open Book on Regression Handbook of Regression Modeling in People Analytics: With Examples in R and Python 5.15 Create APA Tables with apaTables Package apa.cor.table(iris, filename=&quot;output/Table1_APA.doc&quot;, table.number = 1) # filename must end in .rtf or .doc only) reg.model &lt;- lm(sales ~ adverts + airplay, data = album) # The &quot;album&quot; dataset comes with the &quot;apaTables&quot; package apa.reg.table(reg.model, filename = &quot;output/Table2_APA.doc&quot;, table.number = 2) For more examples, check apaTables vignettes "],["binary-logistic-regression.html", "Chapter 6 Binary Logistic Regression 6.1 Some Definitions 6.2 Logarithm Rules 6.3 Logistic Regression Equation 6.4 Run Logistic Regression 6.5 Logistic Regression Coefficients 6.6 Similarities and Differences Between Binary Logistic Regression and OLS Regression 6.7 Empirical Example", " Chapter 6 Binary Logistic Regression library(rio); library(ggplot2); library(QuantPsyc); library(psych); library(car); library(reshape) Binary logistic regression is a form of regression which is used when the DV is a dichotomy and the IVs are of any type. It belongs to generalized linear models. For a binary DV, we have two possible outcomes. Lets take a look at the UCBAdmissions dataset built into R. # View(UCBAdmissions) class(UCBAdmissions) # the dataset is a table ## [1] &quot;table&quot; dat &lt;- as.data.frame(UCBAdmissions) # convert the table to a data frame class(dat) ## [1] &quot;data.frame&quot; dat &lt;- reshape::untable(df = dat[, !names(dat) %in% c(&quot;Freq&quot;)], num=dat$Freq) # View(dat) Lets answer the question: Is there a relationship between gender and admission? Alternatively, do males have a higher chance of being admitted than females? For now, lets also recode the variables Admit and Gender to be numeric variables with 0s and 1s. For the Admit variable, Admitted = 1, Rejected = 0. For the Gender varable, 1 = Female, 0 = Male. contingency_table1 &lt;- table(dat$Gender, dat$Admit) # contingency table. row is Gender; column is Admit addmargins(contingency_table1) prop.table(contingency_table1) # cell percentages prop.table(contingency_table1, 1) # row percentages prop.table(contingency_table1, 2) # column percentages ## ## Admitted Rejected Sum ## Male 1198 1493 2691 ## Female 557 1278 1835 ## Sum 1755 2771 4526 ## ## Admitted Rejected ## Male 0.2646929 0.3298719 ## Female 0.1230667 0.2823685 ## ## Admitted Rejected ## Male 0.4451877 0.5548123 ## Female 0.3035422 0.6964578 ## ## Admitted Rejected ## Male 0.6826211 0.5387947 ## Female 0.3173789 0.4612053 dat$Admit &lt;- ifelse(dat$Admit==&quot;Admitted&quot;, 1, 0) dat$Gender &lt;- ifelse(dat$Gender==&quot;Female&quot;, 1, 0) contingency_table2 &lt;- table(dat$Gender, dat$Admit) # contingency table. row is Gender; column is Admit addmargins(contingency_table2) ggplot(dat, aes(x=Gender, y=Admit, color = Gender)) + geom_jitter(width=0, height=0.1) + geom_smooth(method = &quot;lm&quot;) + theme(legend.position = &quot;none&quot;) ## ## 0 1 Sum ## 0 1493 1198 2691 ## 1 1278 557 1835 ## Sum 2771 1755 4526 6.1 Some Definitions Probability of an Event Event = Being admitted to college Probability of the event P(Admitted) = 0.6 Odds of an Event Odds is the relative chance of an event Odds of being admitted = \\(\\frac{{P(Admitted)}}{{1 - P(Admitted)}} = \\frac{{.6}}{{.4}} = 1.5\\) If you know the probability of an event, you can get the odds and vice versa. Logit = Log-Ods of an Event Logit is the logarithm of the odds. Logit = log(odds) Odds of being admitted = \\(\\frac{{P(Admitted)}}{{1 - P(Admitted)}} = \\frac{{.6}}{{.4}} = 1.5\\) Logit of being admitted = \\({\\log _e}(Odds(Admitted)) = {\\log _e}(1.5) = .405\\) If you know the one of the three: probability, odds, and logit, you can get the values for the other two. 6.2 Logarithm Rules \\[{\\log _b}(XY) = {\\log _b}(X) + {\\log _b}(Y)\\] \\[{\\log _b}(\\frac{X}{Y}) = {\\log _b}(X) - {\\log _b}(Y)\\] \\[{\\log _b}({X^k}) = k{\\log _b}(X)\\] \\[{\\log _b}(1) = 0\\] \\[{\\log _b}(b) = 1\\] \\[{\\log _b}({b^k}) = k\\] \\[{b^{{{\\log }_b}(k)}} = k\\] Given \\({\\log _e}(A) = 1.5\\), what is A? \\(A = {e^{{{\\log }_e}(A)}} = {e^{1.5}} = {(2.718282)^{1.5}}\\) where \\(e\\) is a mathematical constant often called Eulers (pronounced Oiler) number. exp(1.5) ## [1] 4.481689 Given logit = 3, what is the odds? logit = \\({\\log _e}(Odds) = 3\\) \\(Odds = {e^3}\\) exp(3) ## [1] 20.08554 6.3 Logistic Regression Equation Regular OLS Regression \\[Y = a + bX + e\\] \\[\\hat Y = a + bX\\] Logistic Regression \\[{\\rm{logit}} = a + bX\\] \\[Odds = {e^{(a + bX)}}\\] \\[P(Y = 1) = \\frac{{{e^{a + bX}}}}{{1 + {e^{a + bX}}}} = \\frac{1}{{1 + {e^{ - (a + bX)}}}}\\] 6.4 Run Logistic Regression model &lt;- glm(Admit ~ Gender, family = binomial, data = dat) summary(model) ## ## Call: ## glm(formula = Admit ~ Gender, family = binomial, data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.0855 -1.0855 -0.8506 1.2722 1.5442 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.22013 0.03879 -5.675 1.38e-08 *** ## Gender -0.61035 0.06389 -9.553 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 6044.3 on 4525 degrees of freedom ## Residual deviance: 5950.9 on 4524 degrees of freedom ## AIC: 5954.9 ## ## Number of Fisher Scoring iterations: 4 dat$prob &lt;- fitted(model) head(dat) ## Admit Gender Dept prob ## 1 1 0 A 0.4451877 ## 1.1 1 0 A 0.4451877 ## 1.2 1 0 A 0.4451877 ## 1.3 1 0 A 0.4451877 ## 1.4 1 0 A 0.4451877 ## 1.5 1 0 A 0.4451877 6.5 Logistic Regression Coefficients \\({\\log _e}(\\frac{{P(Admitted)}}{{1 - P(Admitted)}}) = - 0.22 - 0.61*Gender\\) The outcome is the logit! Which group (male vs. female) has a higher logit value? How to interprete? The logit of the female group is 0.61 lower than the logit of the male group. What does this mean? \\(- 0.61 = {\\rm{logit}}(Female) - {\\rm{logit}}(Male)\\) \\({\\rm{ }} = {\\log _e}[Odds(Female)] - {\\log _e}[Odds(Male)]\\) \\({\\rm{ }} = {\\log _e}[\\frac{{Odds(Female)}}{{Odds(Male)}}]\\) Therefore, \\(\\frac{{Odds(Female)}}{{Odds(Male)}} = \\exp (- 0.61) = 0.54\\) This is called Odds Ratio. Odds Ratio is the ratio of odds of a comparison/treatment group to that of the reference/control group. Note: For the UCBAdmissions data, if you conduct a binary logistic regression model for each department separately, you may reach different conclusions regarding gender differences in college admissions. Google .blue[Simpsons paradox] to learn more. 6.6 Similarities and Differences Between Binary Logistic Regression and OLS Regression Similarities IVs can be continuous, categorical, or a combination of continuous and categorical variables. Many of the essential concepts in OLS regression still apply to logistic regression. Model comparison and selection Assumptions (Independence of Errors, Linearity, Multicollinearity; but NOT Normality) Diagnostics (Leverage, Discrepancy, Influence) Differences DV is categorical in logistic regression The DV in logistic regression follows a Binomial distribution for which you can specify a link function (logit and probit are two commonly used link functions) The estimation method for logistic regression is maximum likelihood, rather than OLS. 6.7 Empirical Example The Framingham Heart Study DV : Coronary Heart Disease (TenYearCHD, yes=1; no=0) status was collected 10 years after the first exam. IVs were collected in the first data collection Demographic factors male(male = 1, female = 0) age (in years) education (Some high school=1, high school/GED=2; some college/vocational school=3; college=4) Behavioral risk factors currentSmoker (smoker=1, non smoker=0) cigsPerDay Medical history risk factors BPmeds (On blood pressure medication at time of first exam) prevalentStroker (Previously had a stroke) prevalentHyp (Current hypertension/high blood pressure) diabetes (currently has diabetes) Risk factors from first examination totChol (Total cholesterol, mg/dL) sysBP (Systolic blood pressure) diaBP (Diastolic blood pressure) BMI (Body Mass Index, weight (kg)/height (m^2)) heartRate (Heart rate, beats/minute) glucose (Blood glucose level, mg/dL) dat &lt;- import(&quot;data/framingham.csv&quot;) model &lt;- glm(TenYearCHD ~ age, family = binomial, data = dat) summary(model) ## ## Call: ## glm(formula = TenYearCHD ~ age, family = binomial, data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.0386 -0.6261 -0.4580 -0.3695 2.4493 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.561090 0.283746 -19.60 &lt;2e-16 *** ## age 0.074650 0.005265 14.18 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3612.2 on 4239 degrees of freedom ## Residual deviance: 3396.6 on 4238 degrees of freedom ## AIC: 3400.6 ## ## Number of Fisher Scoring iterations: 5 Regression Coefficient Odds Ratios and Confidence Intervals exp(model$coefficients) ## (Intercept) age ## 0.003844584 1.077507075 exp(confint(model)) ## 2.5 % 97.5 % ## (Intercept) 0.002190705 0.006664767 ## age 1.066517962 1.088765650 One year increase in age is related to 0.075 increase in the logit of having CHD Given logit = 0.075, we obtain exp(0.055) = 1.08 One year increase in age changes the odds of CHD from 1 to 1.08 To Obtain Probabilities dat$prob &lt;- fitted(model) #&lt;&lt; df_low_prob &lt;- dat[dat$age &lt; 40, ] df_mid_prob &lt;- dat[dat$age &gt;45 &amp; dat$age &lt;55, ] df_high_prob &lt;- dat[dat$age &gt;65,] ggplot(dat, aes(x=age, y=prob)) + geom_point() + geom_smooth(data = df_low_prob, method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) + geom_smooth(data = df_mid_prob, method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) + geom_smooth(data = df_high_prob, method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) The relationship between age and CHD is positive but not linear. 6.7.1 Assess the model - model chi-square How much better does the model predict the outcome variable, compared to a baseline model? log-likelihood of the model. Analogous to the residual sum of squares in OLS regression deviance = -2 X log-likelihood. It has a chi-square distribution. model chi-square statistic. Difference between the deviance of the baseline model and the deviance of the current model. # summary(model) model.chisq &lt;- model$null.deviance - model$deviance chisq.df &lt;- model$df.null - model$df.residual (chisq.prob &lt;- 1 - pchisq(model.chisq, chisq.df)) # right tailed ## [1] 0 6.7.2 Assess the model - \\({R^2}\\) logisticPseudoR2s &lt;- function(LogModel) { dev &lt;- LogModel$deviance nullDev &lt;- LogModel$null.deviance modelN &lt;- length(LogModel$fitted.values) R.1 &lt;- 1-dev/nullDev R.cs &lt;- 1-exp(-(nullDev-dev)/modelN) R.n &lt;- R.cs/(1-(exp(-(nullDev/modelN)))) cat(&quot;PseudoR^2 for logistic regression\\n&quot;) cat(&quot;Hosmer &amp; Lemeshow R^2: &quot;, round(R.1, 3), &quot;\\n&quot;) cat(&quot;Cox &amp; Snell R^2: &quot;, round(R.cs, 3), &quot;\\n&quot;) cat(&quot;Naelkerke R^2: &quot;, round(R.n, 3), &quot;\\n&quot;) } logisticPseudoR2s(model) ## PseudoR^2 for logistic regression ## Hosmer &amp; Lemeshow R^2: 0.06 ## Cox &amp; Snell R^2: 0.05 ## Naelkerke R^2: 0.086 6.7.3 Diagnostics dat$leverage &lt;- hatvalues(model) # leverage dat$studentized.residuals &lt;- rstudent(model) # discrepancy dat$cooks.d &lt;- cooks.distance(model) # influence dat$leverage[(dat$leverage &gt; 3 * mean(dat$leverage))] ## [1] 0.001661901 0.001459850 0.001459850 0.001661901 0.001459850 0.001459850 0.001661901 0.001661901 0.001459850 0.001661901 0.001880347 ## [12] 0.001661901 0.001661901 0.001880347 0.001459850 0.001459850 0.001661901 0.001459850 0.001661901 0.001661901 0.001661901 0.001459850 ## [23] 0.001880347 0.001459850 0.001459850 0.001459850 0.001661901 0.001661901 0.001661901 0.001459850 0.001661901 0.001661901 0.001880347 ## [34] 0.001661901 0.001459850 0.001459850 0.001880347 0.001459850 0.001459850 0.001661901 0.001661901 0.001661901 0.001661901 0.001661901 ## [45] 0.001459850 0.002362781 0.001459850 0.001459850 0.001661901 0.001459850 0.001459850 0.001661901 0.001459850 0.001661901 0.001880347 ## [56] 0.001459850 0.001661901 0.001459850 0.001459850 0.001459850 0.001459850 0.001459850 0.001661901 0.001661901 0.001880347 0.001661901 ## [67] 0.001661901 0.001459850 0.001880347 0.001661901 0.001880347 0.001661901 0.001661901 0.001880347 0.001661901 0.001661901 0.001661901 ## [78] 0.001880347 0.001459850 0.002114350 0.001661901 0.002362781 0.001661901 0.001459850 0.002114350 0.002114350 0.001459850 0.001459850 ## [89] 0.001880347 0.001459850 0.001661901 0.001880347 0.001661901 0.001459850 0.001880347 0.002114350 0.001661901 0.001661901 0.001880347 ## [100] 0.001880347 0.001661901 0.001880347 0.001661901 0.001459850 0.001459850 0.002114350 0.002114350 0.001661901 0.002114350 0.001880347 dat$studentized.residuals[abs(dat$studentized.residuals) &gt;3] ## numeric(0) dat$cooks.d[dat$cooks.d&gt;1] ## numeric(0) dat$studentized.residuals[(abs(dat$studentized.residuals) &gt; 3)] ## numeric(0) outlierTest(model) ## No Studentized residuals with Bonferroni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferroni p ## 782 2.451316 0.014234 NA dat$cooks.d[dat$cooks.d &gt;1] ## numeric(0) 6.7.4 Assumptions - Linearity dat$log.age.Int &lt;- log(dat$age)*dat$age #&lt;&lt; model.linearity &lt;- glm(TenYearCHD ~ age + dat$log.age.Int, data = dat, family = binomial()) #&lt;&lt; summary(model.linearity) ## ## Call: ## glm(formula = TenYearCHD ~ age + dat$log.age.Int, family = binomial(), ## data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.9096 -0.6538 -0.4639 -0.3403 2.5935 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -13.45767 3.46917 -3.879 0.000105 *** ## age 0.83681 0.33290 2.514 0.011947 * ## dat$log.age.Int -0.15396 0.06721 -2.291 0.021975 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3612.2 on 4239 degrees of freedom ## Residual deviance: 3391.2 on 4237 degrees of freedom ## AIC: 3397.2 ## ## Number of Fisher Scoring iterations: 5 6.7.5 Assumptions - Independence of errors dwt(model) ## lag Autocorrelation D-W Statistic p-value ## 1 0.02687873 1.946162 0.074 ## Alternative hypothesis: rho != 0 6.7.6 Use more predictors dat$edu.c &lt;- factor(dat$education) levels(dat$edu.c) &lt;- c(&quot;some high school&quot;, &quot;high school/GED&quot;, &quot;some college/vocational&quot;, &quot;college&quot;) model2 &lt;- glm(TenYearCHD ~ age + male + sysBP + edu.c, family = binomial, data = dat) summary(model2) ## ## Call: ## glm(formula = TenYearCHD ~ age + male + sysBP + edu.c, family = binomial, ## data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5951 -0.5995 -0.4423 -0.3124 2.8506 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.427112 0.379871 -19.552 &lt; 2e-16 *** ## age 0.058923 0.005893 9.999 &lt; 2e-16 *** ## male 0.612284 0.093217 6.568 5.09e-11 *** ## sysBP 0.017947 0.002002 8.967 &lt; 2e-16 *** ## edu.chigh school/GED -0.179544 0.114138 -1.573 0.116 ## edu.csome college/vocational -0.100398 0.136901 -0.733 0.463 ## edu.ccollege -0.007083 0.151157 -0.047 0.963 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3522.6 on 4134 degrees of freedom ## Residual deviance: 3186.8 on 4128 degrees of freedom ## (105 observations deleted due to missingness) ## AIC: 3200.8 ## ## Number of Fisher Scoring iterations: 5 6.7.7 Check Multicollinearity Among IVs vif(model2) ## GVIF Df GVIF^(1/(2*Df)) ## age 1.167782 1 1.080640 ## male 1.054028 1 1.026659 ## sysBP 1.144721 1 1.069916 ## edu.c 1.077856 3 1.012574 6.7.8 Interprete results Regression Coefficients Odds Ratios and Confidence Intervals exp(model2$coefficients) ## (Intercept) age male sysBP ## 0.0005949034 1.0606932534 1.8446390825 1.0181093723 ## edu.chigh school/GED edu.csome college/vocational edu.ccollege ## 0.8356510686 0.9044777525 0.9929417559 exp(confint(model2)) ## 2.5 % 97.5 % ## (Intercept) 0.0002801087 0.001242352 ## age 1.0485587804 1.073069542 ## male 1.5372823946 2.215676632 ## sysBP 1.0141309942 1.022122846 ## edu.chigh school/GED 0.6670598320 1.043750212 ## edu.csome college/vocational 0.6888360992 1.178746245 ## edu.ccollege 0.7343253293 1.329077617 Coefficient for male. A male is more likely to have CHD than a female adjusting for the effects of other variables. The odds of having CHD for a male is predicted to be 1.85 ( \\({e^{0.612}}\\)) times the odds of of having CHD for a female, holding the other variables constant. Coefficient for age. The older the person grew, the more likely the person would have CHD. The odds of having CHD is predicted to increase by a factor of 1.06 ( \\({e^{0.0589}}\\)) with one year increase in age, holding the other variables constant. Coefficients for the coded dummy variables for education. None of them was statistically significant. Education level was not statistically significantly associated with the odds of having CHD, holding the other variables constant. If they were statistically significant, a person who finished high school is less likely to have CHD than a person who only had some high school eduation, holding the other variables constant. The odds of having CHD for a person who finished high school is predicted to decrease by a factor of 0.835( \\({e^{-.180}}\\)) than a person who only had some high school education, holding the other variables constant. The coefficients for the dummy variables compare one groups odds of having CHD with the odds of the reference group (i.e., those with some high school education). "],["analysis-of-variance.html", "Chapter 7 Analysis of Variance 7.1 Historical Background 7.2 The General Linear Model (GLM) 7.3 One-Way ANOVA (for Single Factor Designs) 7.4 ANOVA Assumptions 7.5 One-way ANOVA Using GLM approach 7.6 Factorial Analysis of Variance (Factorial ANOVA) 7.7 Factorial ANOVA as Multiple Regression 7.8 A Blog on ANOVA in R", " Chapter 7 Analysis of Variance library(rio); library(ggplot2); library(QuantPsyc); library(psych); library(car); library(interactions); library(memisc); library(multcomp) 7.1 Historical Background Regression models and ANOVA developed in different research areas and address different questions Regression models Emerged in biology and psychology at the end of the 19th century as scientists studied the correlation of humans attributes and characteristics. For example: Sir Francis Galton described the regression to the mean phenomenon while studying the height of parents and their adult children (i.e., short parents children tended to be shorter than the average but they tended to be taller than their parents). Aim: Explaining/predicting/forecasting data and determining the relative contribution of each independent variable to the prediction. Regression models and ANOVA developed in different research areas and address different questions Analysis of Variance (ANOVA) Emerged in agricultural sciences in the early 20th century. William S. Gosset (pseudonym: Student) proposed the t-test for the comparison of means of two (experimental) groups. Sir Ronald A. Fisher proposed ANOVA to compare means from any number of experimental conditions without increase in Type I error. Aim: Evaluate whether mean scores of a dependent variable significantly differ across (experimental) conditions. ANOVA is the ideal technique for many psychological areas. 7.2 The General Linear Model (GLM) Many statistic textbooks in education and social sciences describe regression models and ANOVA as distinct and independent statistical concepts. The General Linear Model (GLM) is the overarching method that includes regression and ANOVA. The GLM conception is that observed data can be described in terms of a particular model and some error, \\(data = model + error\\) where the model term represents the hypothesis of interest; and the error term describes other (unconsidered) influences. The relative size of model and error components can be used to judge how well the model describes the data. General refers to the fact that GLM can be used to analyze continuous independent variables (as in regression analysis) and categorical independent variables (e.g., groups; as in ANOVA). .red[Linear] means that linearity is assumed in terms of model parameters (not necessarily in terms of variables) Examples of linear models: \\({Y_i} = \\alpha + \\beta {X_i} + {\\varepsilon _i}\\) \\({Y_i} = \\alpha + \\beta X_i^2 + {\\varepsilon _i}\\) 7.3 One-Way ANOVA (for Single Factor Designs) It answer the question: Are there significant differences between mean scores of a dependent variable across a number of groups (e.g., experimental conditions)? True Experiment An experiment that enables a researcher to test a hypothesized relationship between an independent variable and a dependent variable by manipulating the independent variable. True experiments are characterized by: manipulation by the researcher of one or more independent variables use of controls such as the random assignment of participants to experimental conditions to minimize the effects of nuisance variables careful observation or measurement of one or more dependent variables [1 and 2 are necessary for a true experiment] Quasi-Experiment A study in which at least one independent variable is manipulated, but participants are not randomly assigned to groups. Some studies do not meet the requirements of a true experiment. Due to ethical reasons or other relevant reasons, a researcher cannot always conduct a true experiment. Example: If you want to know the effect of attending half-day daycare vs. attending full-day daycare, it is probably unlikely to randomly assign participants (e.g., parents may need to work so half-day daycare is not an option). Organismic Variable (also called Subject Variables) They refer to already-existing characteristics of the subjects participating in the study (e.g., gender, age, intelligence, physical or psychiatric disorder, any personality attribute you can name). When using organismic variables in a study, the researcher cannot manipulate them directly but must select participants for the different conditions by virtue of the characteristics they already have. Other examples are Race/Ethnicity, ADHD, Introversion/Extroversion, Depression, SES 7.3.1 Statistical hypotheses Null Hypothesis: \\({H_0}:{\\mu _1} = {\\mu _2} = {\\mu _3} = ... = {\\mu _k}\\) (The group means are equal in the population, where k is the number of groups) Alternative Hypothesis: At least two of the \\({\\mu _j}\\) (group means) are different in the population. 7.3.2 Basic idea of ANOVA An F -statistic is used to determine whether significant differences in means exist. In traditional ANOVA, F -statistic is the ratio of between group variance and within group variance. The distribution of the F-statistics (with known degrees of freedom) is known, when the null hypothesis is correct. The probability that the observed (or more extreme) differences are due to chance (i.e., sampling variation) can be determined by comparing the distribution of F-statistics under \\({H_0}\\) and the observed F-statistic. If this probability is sufficiently small, then the null hypothesis is rejected. Why not simply compare the group means two at a time using the t test for independent samples? Why not compare the following?: 1. \\({\\mu _1} = {\\mu _2}\\) 2. \\({\\mu _1} = {\\mu _3}\\) 3. \\({\\mu _2} = {\\mu _3}\\) Besides the extra time that it takes, especially as the number of groups (k) increase, the reason we do no conduct several t tests is because of TYPE I ERROR INFLATION. TYPE I ERROR INFLATION When we set alpha ( \\(\\alpha\\)) equal to .05, we are willing to risk being wrong 5% of the time when we reject the null hypothesis ( \\({H_0}\\)). With just two group means to compare, there is just 1 t statistic calculation. We would compare our observed/obtained t value with the critical t value to see if our observed/obtained t value fell in the critical region for rejecting the null ( \\({H_0}\\)). With alpha set at .05, the critical t value was originally determined by taking the sampling distribution of t for the appropriate df and locating the t value such that the proportion of the total number of t values that were equal to or more extreme than it equaled .05. That is, if we were randomly sampling one t score from the t distribution, the probability it would be greater than or equal to the critical t value is .05. Now what happens when we do an experiment involving many t comparisons, such as the 3 described above? We are no longer sampling 1 t value from the t distribution, but 3. The probability of getting t values greater than the critical t obviously goes up. It is no longer equal to .05. The probability of making a Type I error has gone up as a result of doing an experiment with many groups and analyzing the data with more than one comparison. Overall Alpha for a set of tests is defined as the maximum risk one is willing to take in making at least one false rejection when the null is true (i.e., all the population means are equal in One-Way ANOVA), resulting in a Type I error. Overall \\(\\alpha \\approx r\\alpha\\) (Its actually equal to \\(1-{(1 - \\alpha )^3}\\)), where r is the number of tests being done. Number of Groups Number of \\(t\\) tests [\\(k(k-1)/2\\)] Approximate Overall \\(\\alpha\\) [$ r$] 3 3(3-1)/2 = 3 3 * 5 = .15 4 4(4-1)/2 = 6 6 * .05 = .30 5 5(5-1)/2 = 10 10 * .05 = .50 6 6(6-1)/2 = 15 15 * .05 = .75 With 3 tests, we have an approximate 15% chance of making at least 1 Type I error. The risk increases as the number of groups increase. 7.3.3 Empirical example Reading achievement of 15 children for three experimental training conditions (A = control, B = non-parental training, and C = parental training). Condition A Condition B Condition C 40.00 48.00 44.00 33.00 43.00 46.00 34.00 51.00 42.00 35.00 46.00 54.00 44.00 38.00 43.00 Mean 37.20 45.20 45.80 Grand Mean: SD 4.66 4.97 4.82 42.73 Variance Partitioning (Its all about the Variance). Traditinal ANOVA partitions the total variation of observed data into between-group and within-group components. Total Sum of Squares: Equals the sum of squared deviations of individual data points around the overall mean (i.e., the grand mean) and serves as an indicator for the total variance of the dependent variable. Between-group Sum of Squares: Equals the sum of squared deviations of the group means around the overall mean and serves as an indicator for the variance associated with group membership. Within-group Sum of Squares: Equals the sum of squared deviations of individual data points around the group means and serves as an indicator for the unexplained variances existing within groups. Table 7.1: ANOVA Table for One-Way ANOVA Source of Variaton Sum of Squares Degrees of Freedom Mean Squares \\(F\\) Statistic Between-group (model) \\(SS_b = \\sum\\limits_{j = 1}^k {{n_j}{{({\\mu _j} - \\mu )}^2}}\\) \\(df = k-1\\) \\[MS _b = {SS_b} \\over {k - 1}\\] \\(F = {MS _b} \\over {MS _w}\\) Within-group (error) \\(SS_w = \\sum\\limits_{j = 1}^k {\\sum\\limits_{i = 1}^{n _j} {{({y _ij} - \\mu _j)}^2}}\\) \\(df = N-k\\) \\[MS _w = {SS _w} \\over {N-k}\\] Total \\(SS_t = \\sum\\limits_{j = 1}^k {\\sum\\limits_{i=1}^{n _j} {{({y _ij} - \\mu)}^2}}\\) \\(df = N-1\\) The total sum of squares equals the between-group sum of squares plus the within-group sum of squares, i.e., \\(S{S_t} = S{S_b} + S{S_w}\\) Dividing these sums of squares by their respective degrees of freedom (df) makes these statistics more accurate estimates of the variability they represent in the population. The resulting value is referred to as the Mean Square or MS because it represents the mean of a particular sum of squares, which will be an unbiased estimate of the population variance. Using \\(M{S_b}\\) and \\(M{S_w}\\), we can caluate the F ratio to test the null hypothesis that the group means are equal. \\(F = \\frac{{M{S_b}}}{{M{S_w}}}\\) The coefficient of determination is defined as \\({R^2} = \\frac{{S{S_b}}}{{S{S_t}}}\\) reading &lt;- import(&quot;data/reading.txt&quot;) head(reading) ## group reading ## 1 1 40 ## 2 1 33 ## 3 1 34 ## 4 1 35 ## 5 1 44 ## 6 2 48 reading$group.f &lt;- as.factor(reading$group) levels(reading$group.f) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) mymodel &lt;- aov(reading ~ group.f, data=reading) summary(mymodel) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group.f 2 230.5 115.3 4.968 0.0268 * ## Residuals 12 278.4 23.2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.3.4 Pairwise comparisons Note that a significant F-test simply means that there are significant differences somewhere among means. No statements are made concerning which means differ. Planned or post-hoc comparisons are necessary to locate the source of a significant F-test. Planned Comparisons are those comparisons that are planned in advance of data collection. For example, in an experiment with two treatment groups and one control group (see empirical example), one may only want to compare the treatment groups with the control group (without comparing the treatment groups). Post-hoc comparisons are those comparisons withouth specific a priori theory (rather exploratory approach). 7.3.5 Planned comparisons Each mean that reflects the planned hypothesis is weighted (i.e., numbers such as 0, 1, 1/2, etc). At least two of the means have to have nonzero weights. Groups with a weight of zero are left out of the comparison. Means that are contrasted with each other are assigned weights with opposite signs. The weights must sum to zero, i.e., \\(\\sum {{w_i} = 0}\\) Constrasts Group A: Control Group B: Non-parental Training Group C: Parental Training \\(\\sum{w _i}\\) control vs. average of training 1 -0.5 0 -0.5 non-parental vs. parental training 0 -1.0 0 1.0 contraol vs non-parental training 1 -1.0 0 0.0 control vs. parental training 1 0.0 0 -1.0 c1 &lt;- c(1, -1/2, -1/2) # compares control vs. mean of both treatments groups c2 &lt;- c(0, -1, 1) # compares means of treatment groups contrasts(reading$group.f) &lt;- cbind(c1, c2) # add a &quot;contrasts&quot; attribute to &quot;reading$group.f&quot; mymodel2 &lt;- model.c &lt;- aov(reading ~ group.f, data=reading) summary(mymodel2, split = list(group.f = list(&quot;Control vs Treatment&quot; = 1, &quot;Treatment1 vs Treatment2&quot; = 2))) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group.f 2 230.5 115.3 4.968 0.02679 * ## group.f: Control vs Treatment 1 229.6 229.6 9.898 0.00844 ** ## group.f: Treatment1 vs Treatment2 1 0.9 0.9 0.039 0.84716 ## Residuals 12 278.4 23.2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.3.6 Post-hoc pairwise comparisons For example, for three groups one obtains three pairwise comparisons: A vs. B, A vs. C, and B vs. C The probability of not committing a Type I Error in three comparisons is \\({(1 - \\alpha )^3}\\) with \\(\\alpha\\) being the nominal significance level. The probability that at least one of the three comparisons commits a Type I Error is \\(1-{(1 - \\alpha )^3}\\). E.g., using \\(\\alpha = 0.05\\), we get an actual \\(\\alpha\\) of 0.143. Thus, when more than two groups are compared, a correction for multiple comparisons must be used to avoid Type I Error inflation. 7.3.6.1 Bonferroni correction Simplest method to adjust the nominal significance level. Given a significance level of \\(\\alpha\\), the p-value of each of k tests must be smaller than \\(\\alpha /k\\). For example, if k = 3 and \\(\\alpha = 0.05\\): \\(\\alpha * = \\frac{{0.05}}{3} = 0.017\\) pairwise.t.test(reading$reading, reading$group.f, p.adjust=&quot;bonferroni&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: reading$reading and reading$group.f ## ## A B ## B 0.066 - ## C 0.046 1.000 ## ## P value adjustment method: bonferroni 7.3.6.2 Tukeys Honest Significant Difference (HSD) One of the most commonly used multiple comparison procedures. Performs all possible pairwise contrasts, where \\({H_0}:{\\mu _i} = {\\mu _j}\\) for all \\(i \\ne j\\). One of the most conservative procedures: Thus, low probability of Type I Error and less power. mymodel3 &lt;-aov(reading ~ group.f, data = reading) TukeyHSD(mymodel3) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = reading ~ group.f, data = reading) ## ## $group.f ## diff lwr upr p adj ## B-A 8.0 -0.1271377 16.127138 0.0537998 ## C-A 8.6 0.4728623 16.727138 0.0380034 ## C-B 0.6 -7.5271377 8.727138 0.9788785 7.4 ANOVA Assumptions Independence of observations: Observations are assumed to be randomly sampled from a pre-defined target population. Each participants score on the dependent variable is not affected by other participants in the same treatment group Normality: The dependent variable (or the residuals) are assumed to be normally distributed in each sub-population (i.e., in each experimental cell). Visual displays of the data for each group (e.g., with histograms, QQ plots); Descriptive statistics (e.g., skewness and kurtosis); Use the Shapiro-Wilk test Variance homogeneity: The variance is assumed to be the same for each sub-population. The F test performs acceptably when this assumption is violated if groups are equal in size or if the number in the largest group/the number in the smallest group &lt; 1.5. It becomes a problem when group sizes are extremely unequal (largest/smallest &gt; 1.5) and the assumption is violated. If larger variances are associated with small groups, the F test is liberal, meaning that the null is rejected falsely too often (i.e., greater than 5% of the time; actual \\(\\alpha\\) &gt; nominal \\(\\alpha\\)). If larger variances are associated with large groups, the F test is conservative, meaning that the null is rejected falsely much less than 5% of the time (actual \\(\\alpha\\) &lt; nominal \\(\\alpha\\)). While this may not appear to be a problem, you tend to lose power with smaller alpha levels. In R, use the bartlett.test function from the stats package or the leveneTest function from the car package to test the homogeneity assumption. The null hypothesis for Levenes test and the Bartletts test is that the variances in different groups are equal. You DO NOT want to reject the null hypothesis. Bartletts test bartlett.test(reading ~ group.f, data = reading) ## ## Bartlett test of homogeneity of variances ## ## data: reading by group.f ## Bartlett&#39;s K-squared = 0.015081, df = 2, p-value = 0.9925 Levenes test leveneTest(reading ~ group.f, data = reading) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0.0363 0.9645 ## 12 What to do when the homogeneity of variance assumption is violated? Run an ANOVA that does not assume equal variances, such as the Kruskal-Wallis test. The Kruskal-Wallis test is a nonparametric test that is used with an independent groups design employing k samples. It is used as a substitute for the parametric one-way ANOVA when the assumptions of that test are seriously violated. The Kruskal-Wallis test does not assume normality, nor homogeneity of variance, as does the parametric ANOVA, and requires only ordinal scaling of the dependent variable (ranked data for the dependent variable). It is used when violations of normality and/or homogeneity of variance are extreme, or when interval or ratio scaling are not met by the data. Use a more stringent alpha level (e.g., .01, .001) Conduct a variance stabilizing transformation (e.g., square root or natural log) to the dependent variable 7.4.1 Robustness Robustness refers to the insensitivity of a statistical procedure against violations of model assumptions. Nominal alpha (level of significance) is the level set by the experimenter and is the percent of time one is rejecting falsely when the null hypothesis is true and all assumptions are met. Actual alpha is the percent of time one is rejecting falsely if 1 or more of the assumptions is violated. A test is robust if the actual alpha is very close to the nominal alpha. Robustness refers to the insensitivity of a statistical procedure against violations of model assumptions. Type I Error Robustness: A statistical procedure is said to be robust, if the procedure is able to protect the nominal significance level (e.g., \\(\\alpha = 0.05\\)). The ANOVA F -test is Type I Error robust against violations of the normality and variance homogeneity assumption (in particular, in balanced designs, i.e., equal sample sizes across conditions). However, Wilcox (1995) cautions that skewed distributions or distributions with high kurtosis lower the power of the ANOVA F -test to detect true mean differences. Wilcox (2012) suggests modern statistical techniques (e.g., bootstrapping) and Conover and Iman (1981) suggest rank transformations as potential remedies. 7.5 One-way ANOVA Using GLM approach The model underlying the (traditional) ANOVA can be written as: \\({y_i} = \\mu + {\\tau _j} + {\\varepsilon _{i}}\\) where \\(\\mu\\) is a constant representing the DV free of any group effects, \\(\\tau _j\\) is the effect of the jth group, and \\({\\varepsilon _i}\\) is the individual error The ANOVA model can also be written as a multiple regression model: \\({y_i} = \\alpha + \\sum\\limits_j^{k - 1} {{\\beta _j}{x_{ij}} + } {\\varepsilon _i}\\) where \\(\\alpha\\) is a constant representing the DV free of any group effects; \\(\\beta_j\\)s are regression coefficients for the coded variables \\(x_{ij}\\); and {_i} is the individual error. We have used the GLM approached before when we had regression models with categorical predictors. With k groups, we only need (k-1) coded variables. Dummy coding and effect coding are often used coding schemes. Choose a reference group for coding ANOVA is a special case of multiple linear regression! Dummy Coding In R, dummy coding is called treatment contrasts (this is the default). the first group is used as the reference group by default. reading &lt;- import(&quot;data/reading.txt&quot;) reading$group.f &lt;- as.factor(reading$group) levels(reading$group.f) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) contrasts(reading$group.f) # group A is the reference group ## B C ## A 0 0 ## B 1 0 ## C 0 1 mymodel4 &lt;- lm(reading ~ group.f, data=reading) #&lt;&lt; summary(mymodel4)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2 2.154066 17.269666 7.682923e-10 ## group.fB 8.0 3.046309 2.626129 2.213323e-02 ## group.fC 8.6 3.046309 2.823088 1.537161e-02 \\(y\\) \\(\\beta _0\\) \\(x_B\\) \\(x_C\\) 40 1 0 0 33 1 0 0 34 1 0 0 35 1 0 0 44 1 0 0 48 1 1 0 43 1 1 0 51 1 1 0 46 1 1 0 38 1 1 0 44 1 0 1 46 1 0 1 42 1 0 1 54 1 0 1 43 1 0 1 The reference group can be changed. contrasts(reading$group.f) &lt;- contr.treatment(n = 3, base = 2) #&lt;&lt; contrasts(reading$group.f) # 2nd group is the reference group ## 1 3 ## A 1 0 ## B 0 0 ## C 0 1 mymodel5 &lt;- lm(reading ~ group.f, data=reading) summary(mymodel5)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 45.2 2.154066 20.9835732 7.957511e-11 ## group.f1 -8.0 3.046309 -2.6261287 2.213323e-02 ## group.f3 0.6 3.046309 0.1969596 8.471556e-01 coef(mymodel5) ## (Intercept) group.f1 group.f3 ## 45.2 -8.0 0.6 Effect Coding In R, effect coding is called sum contrasts (i.e., contrasts sum up to zero) # library(memisc) # use the modified version of the contr.sum() function from the &quot;memisc&quot; group contrasts(reading$group.f) &lt;- contr.sum(n = 3, base = 1) # 1st group is the reference group contrasts(reading$group.f) ## 2 3 ## A -1 -1 ## B 1 0 ## C 0 1 mymodel6 &lt;- lm(reading ~ group.f, data=reading) summary(mymodel6)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.733333 1.243651 34.361207 2.350226e-13 ## group.f2 2.466667 1.758787 1.402481 1.861089e-01 ## group.f3 3.066667 1.758787 1.743626 1.067619e-01 coef(mymodel6) ## (Intercept) group.f2 group.f3 ## 42.733333 2.466667 3.066667 \\(y\\) \\(\\beta _0\\) \\(x_B\\) \\(x_C\\) 40 1 -1 -1 33 1 -1 -1 34 1 -1 -1 35 1 -1 -1 44 1 -1 -1 48 1 1 0 43 1 1 0 51 1 1 0 46 1 1 0 38 1 1 0 44 1 0 1 46 1 0 1 42 1 0 1 54 1 0 1 43 1 0 1 contrasts(reading$group.f) &lt;- contr.sum(n = 3, base = 2) # 2nd group is the reference group contrasts(reading$group.f) ## 1 3 ## A 1 0 ## B -1 -1 ## C 0 1 mymodel7 &lt;- lm(reading ~ group.f, data=reading) summary(mymodel7)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.733333 1.243651 34.361207 2.350226e-13 ## group.f1 -5.533333 1.758787 -3.146107 8.435697e-03 ## group.f3 3.066667 1.758787 1.743626 1.067619e-01 coef(mymodel7) ## (Intercept) group.f1 group.f3 ## 42.733333 -5.533333 3.066667 Use the function anova() to get the traditional ANOVA table. anova(mymodel4) ## Analysis of Variance Table ## ## Response: reading ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group.f 2 230.53 115.27 4.9684 0.02679 * ## Residuals 12 278.40 23.20 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the coding scheme does not affect the ANOVA statistics. anova(mymodel5); anova(mymodel6); anova(mymodel7) 7.6 Factorial Analysis of Variance (Factorial ANOVA) Factorial ANOVA is used when you have two or more categorical independent variables. For exmaple: Is the treatment effect (3 treatments) different for females than for males? [2 (gender) x 3 (treatment) factorial design] Is teaching method (4 methods) more effective for high- than for low-ability students? [2 (ability level) x 4 (teaching method) factorial design] Is the gender difference on some variable (e.g., math achievement) greater for certain nationalities (American vs. Indonesian) than for others? [2 (gender) x 2 (nationality) factorial design You can use this design without treatment variables. For example, your design could consist of demographic variables only (age by gender; age by ethnicity). Factorial Design is a good thing (for 3 reasons). A two-way ANOVA allows us to examine the joint or interactive effect of the two independent variables on the dependent variable. Factorial designs can lead to more powerful tests by reducing error (within cell) variance. If performance on the dependent variable is related to the individual difference characteristic (the blocking variable; e.g., gender, ethnicity, age), then the reduction can be substantial. Economy of participants. We only need half the participants for a two-way ANOVA as would be needed for two one-way ANOVAs with the same number of levels for each factor. A two-way ANOVA allows us to examine the joint or interactive effect of the two independent variables on the dependent variable. Interaction Definitions An interaction is present when the effects of one independent variable on performance change at different levels of the second independent variable. An interaction is present when the pattern of differences associated with an independent variable changes at different levels of the other independent variable. An interaction is present when the simple effects of one independent variable are not the same at all levels of the second independent variable. An interaction is present when the main effect of one independent variable is not representative of the simple effects of that variable. An interaction is present when the differences between the cell means representing the effect of factor A at one level of factor B do not equal the corresponding differences at another level of factor B. An interaction is present when the effects of one of the independent variables depend on the levels of the other independent variable. An interaction is present when one of the effects of one independent variable is not constant (not the same) at all levels of the other independent variable. Example 1: The degree of superiority changes, but one subgroup always does better than another. T1 T2 T3 mean High Ability 85.00000 80.00000 76.00000 80.33333 Low Ability 60.00000 63.00000 68.00000 63.66667 mean 72.50000 71.50000 72.00000 72.00000 Example 2: The superiority reverses; one treatment is best with one group, but another treatment is better for a different group. Contract for Grade Teacher Controlled mean Internal 50.520 38.010 44.265 External 36.330 46.220 41.275 mean 43.425 42.115 42.770 Factorial designs can lead to more powerful tests by reducing error (within cell) variance. If performance on the dependent variable is related to the individual difference characteristic (the blocking variable; e.g., gender, ethnicity, age), then the reduction can be substantial. Treatment Only (variances are inside parentheses) T1 T2 18, 19, 21, 20, 22, 11, 12, 11, 13, 14 (18.77) 17, 16, 16, 18, 15, 9, 9, 11, 8, 7 (17.6) Treatment X Gender (variances are inside parentheses) \\(T_1\\) \\(T_2\\) Males 18, 19, 21, 20, 22 (2.5) 17, 16, 16, 18, 15 (1.3) Females 11, 12, 11, 13, 14 (1.7) 9, 9, 11, 8, 7 (2.2) Economy of participants. We only need half the participants for a two-way ANOVA as would be needed for two one-way ANOVAs with the same number of levels for each factor. Drug X Drug Y DrugZ \\(n = 30\\) \\(n = 30\\) \\(n=30\\) Biofeedback \\(n=45\\) Control \\(n = 45\\) DrugX Drug Y Drug Z Total Biofeedback \\(n=15\\) \\(n=15\\) \\(n=15\\) \\(n=45\\) Control \\(n=15\\) \\(n=15\\) \\(n=15\\) \\(n=45\\) Total \\(n=30\\) \\(n=30\\) \\(n=30\\) 7.6.1 Numerical calcuations for two-way ANOVA We had between and within variation in the one-way design. In the two-way design, we will have four sources of variation: Variation due to Factor A (1 independent variable) Variation due to Factor B (1 independent variable) Variation due to the interactive effect of A and B (interaction between two independent variables) Within cell (error) variance Earlier Example: Reading achievement of 15 children for three experimental training conditions (A = control, B = non-parental training, C = parental training) Lets call the treatment Factor A. We also want to consider the Gender Factor (Factor B). The mean of each cell and the marginal means look like: Factor A Control Non-parental training Parental training Boys \\(\\mu _{11}\\) \\(\\mu _{12}\\) \\(\\mu _{13}\\) \\(\\mu _{1.}\\) Girls \\(\\mu _{21}\\) \\(\\mu _{22}\\) \\(\\mu _{23}\\) \\(\\mu _{2.}\\) \\(\\mu _{.1}\\) \\(\\mu _{.2}\\) \\(\\mu _{.3}\\) \\(\\mu _{..}\\) Two-Way ANOVA allows two factors to be included in the same model (J X K groups). A 3 X 2 ANOVA means there are three levels of Factor A and two levels of Factor B. In a two-way ANOVA, there are two main effects (the effect of one factor that is averaged across the levels of all other factors) and an interaction effect: Main Effect for A: \\({{H}_{0}}:{{\\mu }_{.1}}={{\\mu }_{.2}}=\\cdots ={{\\mu }_{.J}}\\) (population column means are equal, where J = number of levels for Factor A). Is there a significant difference between training groups, holding gender constant? Main Effect for B: \\({{H}_{0}}:{{\\mu }_{1.}}={{\\mu }_{2.}}=\\cdots ={{\\mu }_{K.}}\\) (population row means are equal, where K = number of levels for Factor B). Is there a significant gender difference, holding training conditions constant? Interaction Effect (Factor A X Factor B): ): \\({{H}_{0}}:{{\\varphi }_{ij}}=0\\) for each i and j, where \\({{\\varphi }_{ij}}\\) represents the cell interaction effect (the part of the cell mean that cannot be accounted for by the overall effect/grand mean and by main effects A and B). Are the differences between boys and girls different across different training conditions? Alternatively, are training effects different for boys and girls? 7.6.2 Partitioning the total sum of squares \\[{y_{ijk}} = \\mu + {\\alpha _j} + {\\beta _k} + {(\\alpha \\beta )_{jk}} + {\\varepsilon _{ijk}}\\] where - \\(\\mu = {\\Sigma _j}{\\Sigma _k}{\\mu _{jk}}/JK\\) is the grand mean \\({\\alpha _j} = {\\mu _{.j}} - \\mu\\) is the effect of the jth group of Factor A. The marginal mean \\(mu _{.j}\\) is defined as \\({\\mu _{.j}} = {\\Sigma _k}{\\mu _{jk}}/K\\) where K is the number of levels of Factor B. \\({\\beta _k} = {\\mu _{k.}} - \\mu\\) is the effect of the kth gorup of Factor B. The marignal mean \\(mu _{k.}\\) is defined as \\({\\mu _{k.}} = {\\Sigma _j}{\\mu _{jk}}/J\\) where J is the number of levels of Factor A. \\({(\\alpha \\beta )_{jk}} = {\\mu _{jk}} - (\\mu + {\\alpha _j} + {\\beta _k})\\) is the interaction effect, where \\(\\mu _{jk}\\) is the cell mean. An interaction effect is teh extent to which each cell mean differs from the additive pattern of the main effects. \\({\\varepsilon _{ijk}}\\) is the error term which is assumed to be normally distributed (i.e., \\({\\varepsilon _{ijk}} \\sim N(0,\\sigma _\\varepsilon ^2)\\)) \\[{y_{ijk}} = \\mu + {\\alpha _j} + {\\beta _k} + {(\\alpha \\beta )_{jk}} + {\\varepsilon _{ijk}}\\] \\[SS{Q_y} = SS{Q_A} + SS{Q_B} + SS{Q_{AB}} + SS{Q_E}\\] Table 7.2: ANOVA Table for Two-Way ANOVA Source of Variaton Sum of Squares Degrees of Freedom Mean Squares \\(F\\) Statistic Factor A \\(SSQ_A = {nK}\\sum\\limits_{j = 1}^J {{({\\mu _j} - \\mu)}^2}\\) \\(J-1\\) \\(MS _A = {{SSQ_A} \\over {J - 1}}\\) \\(F = {{MS _A} \\over {MS _E}}\\) Factor B \\(SSQ_B = {nJ}\\sum\\limits_{k = 1}^K {{({\\mu _k} - \\mu)}^2}\\) \\(K-1\\) \\(MS _B = {{SSQ _B} \\over {K-1}}\\) \\(F = {{MS _B} \\over {MS_E}}\\) Interaction (A \\(\\times\\) B \\(SSQ_{AB} = n\\sum\\limits_{j}^J {\\sum\\limits_{k}^K {{({\\mu _{jk}} - {\\mu _j} - {\\mu _k} + {\\mu})}^2}}\\) \\((J-1)(K-1)\\) \\(MS _{AB} = {{SSQ _{AB}} \\over {(J-1)(K-1)}}\\) \\(F = {{MS _{AB}} \\over {MS_E}}\\) Error \\(SSQ_E = \\sum\\limits_{i}^{n} {\\sum\\limits_{j}^J {\\sum\\limits_{k}^K {{({\\mu _{ijk}} - {\\mu _jk})}^2}}}\\) \\(JK(n-1)\\) \\(MS _B = {{SSQ _E} \\over {JK(n-1)}}\\) Total \\(SSQ_y = \\sum\\limits_{i}^{n} {\\sum\\limits_{j}^J {\\sum\\limits_{k}^K {{({\\mu _{ijk}} - {\\mu})}^2}}}\\) \\(nJK-1\\) Note that \\[R^2 = {{SSQ_A + SSQ_B + SSQ_AB} \\over {SSQ_y}}\\] 7.6.3 Empirical example - Two-way ANOVA Condition A Condition B Condition C 40 48 44 Boys 33 43 46 34 51 42 44 38 43 42 43 43 Girls 35 66 41 41 57 39 37 36 45 dat &lt;- import(&quot;data/reading2.txt&quot;) dat$training &lt;- as.factor(dat$training) dat$gender &lt;- as.factor(dat$gender) head(dat) ## reading training gender ## 1 40 A boy ## 2 33 A boy ## 3 34 A boy ## 4 44 A boy ## 5 48 B boy ## 6 43 B boy Means of main effects aggregate(reading ~ training, data = dat, mean, na.rm = TRUE) aggregate(reading ~ gender, data = dat, mean, na.rm = TRUE) ## training reading ## 1 A 38.250 ## 2 B 47.750 ## 3 C 42.875 ## gender reading ## 1 boy 42.16667 ## 2 girl 43.75000 Means of interaction effects aggregate(reading ~ training + gender, data = dat, mean, na.rm = TRUE) ## training gender reading ## 1 A boy 37.75 ## 2 B boy 45.00 ## 3 C boy 43.75 ## 4 A girl 38.75 ## 5 B girl 50.50 ## 6 C girl 42.00 Two-Way ANOVA Model model &lt;- aov(reading ~ training * gender, data = dat) summary(model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## training 2 361.1 180.54 4.118 0.0337 * ## gender 1 15.0 15.04 0.343 0.5653 ## training:gender 2 53.6 26.79 0.611 0.5537 ## Residuals 18 789.2 43.85 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 M &lt;- tapply(dat$reading, list(dat$gender, dat$training), mean) df &lt;- data.frame( treatment=factor(c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;)), gender=factor(c(&quot;boy&quot;, &quot;girl&quot;,&quot;boy&quot;, &quot;girl&quot;,&quot;boy&quot;, &quot;girl&quot;)), score=as.vector(M) ) ggplot(data =df, aes(x=treatment, y=score, group = gender, color=gender)) + geom_point() + ggtitle(&quot;Achievement by Treatment and Gender&quot;) + xlab(&quot;Treatment Group&quot;) + ylab(&quot;Achievement&quot;) + geom_line(aes(linetype = gender), size = 1) + theme(legend.title=element_blank()) dat$int &lt;- interaction(dat$training, dat$gender) ggplot(dat, aes(y=reading, x=int, color=gender)) + geom_boxplot() Interpret model results. When interpreting results of models with interaction terms, use Type III Sums of Squares. model &lt;- aov(reading ~ training * gender, data = dat) summary(model) # anova(model) Anova(model, type = &quot;III&quot;) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## training 2 361.1 180.54 4.118 0.0337 * ## gender 1 15.0 15.04 0.343 0.5653 ## training:gender 2 53.6 26.79 0.611 0.5537 ## Residuals 18 789.2 43.85 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Anova Table (Type III tests) ## ## Response: reading ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 5700.3 1 130.0025 1.144e-09 *** ## training 120.2 2 1.3703 0.2793 ## gender 2.0 1 0.0456 0.8333 ## training:gender 53.6 2 0.6110 0.5537 ## Residuals 789.2 18 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Main effect of training: F(2,18) = 1.37, p = 0.28. We fail to reject the null hypothesis that there are no differences among the training groups.That is, there are no statistically significant differences among the training groups in reading. Main effect of gender: F(1, 18) = 0.05, p = .83. We fail to reject the null hypothesis that boys and girls have the same reading scores. That is, there is no statistically significant gender difference in reading. Interaction effect: F(2, 18) = 0.61, p = 0.55. We fail to reject the null hypothesis that there is no interaction between training and gender. That is, there is no statistically significant interaction between training and gender. 7.6.4 Assumptions for factorial ANOVA (see 7.4) Use leveneTest to test variance homogeneity. Levenes test leveneTest(reading ~ training * gender, data = dat) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 5 8.554 0.000272 *** ## 18 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What to do when the homogeneity of variance assumption is violated? Run an ANOVA that does not assume equal variances, such as the Kruskal-Wallis test. The Kruskal-Wallis test is a nonparametric test that is used with an independent groups design employing k samples. It is used as a substitute for the parametric one-way ANOVA when the assumptions of that test are seriously violated. The Kruskal-Wallis test does not assume normality, nor homogeneity of variance, as does the parametric ANOVA, and requires only ordinal scaling of the dependent variable (ranked data for the dependent variable). It is used when violations of normality and/or homogeneity of variance are extreme, or when interval or ratio scaling are not met by the data. Use a more stringent alpha level (e.g., .01, .001) Conduct a variance stabilizing transformation (e.g., square root or natural log) to the dependent variable 7.6.5 Contrasts using the multcomp package One-Way ANOVA using pairwise contrasts (so-called Tukey contrasts) #library(multcomp) model &lt;- aov(reading ~ -1 + training, data = dat) # no intercept to obtain cell means coef(model) ## trainingA trainingB trainingC ## 38.250 47.750 42.875 # TukeyHSD(model) out &lt;- glht(model, linfct=mcp(training=&quot;Tukey&quot;)) # mean contrasts; mcp - multiple comparison procedures out ## ## General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Linear Hypotheses: ## Estimate ## B - A == 0 9.500 ## C - A == 0 4.625 ## C - B == 0 -4.875 confint(out) ## ## Simultaneous Confidence Intervals ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: aov(formula = reading ~ -1 + training, data = dat) ## ## Quantile = 2.5217 ## 95% family-wise confidence level ## ## ## Linear Hypotheses: ## Estimate lwr upr ## B - A == 0 9.5000 1.4414 17.5586 ## C - A == 0 4.6250 -3.4336 12.6836 ## C - B == 0 -4.8750 -12.9336 3.1836 One-Way ANOVA using user-defined contrasts. K &lt;- rbind(c(1,-1/2, -1/2), # control vs. mean of the two treatment groups c(0, -1, 1)) # compares the two treatment groups rownames(K) &lt;- c(&quot;control vs treatments&quot;, &quot;treatment 1 vs treatment 2&quot;) colnames(K) &lt;- names(coef(model)) # K out &lt;- glht(model, linfct = K) out ## ## General Linear Hypotheses ## ## Linear Hypotheses: ## Estimate ## control vs treatments == 0 -7.062 ## treatment 1 vs treatment 2 == 0 -4.875 summary(out) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: aov(formula = reading ~ -1 + training, data = dat) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## control vs treatments == 0 -7.062 2.768 -2.552 0.0364 * ## treatment 1 vs treatment 2 == 0 -4.875 3.196 -1.525 0.2601 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) Two-Way ANOVA (main effects only) model2 &lt;- aov(reading ~ gender + training, data = dat) Male Condition B Condition C B - A 0 1 0 C - A 0 0 1 C - B 0 -1 1 male - female 1 0 0 model2 &lt;- aov(reading ~ gender + training, data = dat) First, extract the appropriate contrast matrices for both factors. K1 &lt;- glht(model2, linfct= mcp(training = &quot;Tukey&quot;))$linfct K2 &lt;- glht(model2, linfct= mcp(gender = &quot;Tukey&quot;))$linfct Then, compare levels of each factor simultaneously. summary(glht(model2, linfct=rbind(K1, K2))) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: aov(formula = reading ~ gender + training, data = dat) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## B - A == 0 9.500 3.246 2.927 0.0296 * ## C - A == 0 4.625 3.246 1.425 0.4505 ## C - B == 0 -4.875 3.246 -1.502 0.4065 ## girl - boy == 0 1.583 2.650 0.597 0.9185 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) Two-Way ANOVA (with interaction effect) We may compare the levels of one factor (e.g., training effects)at each level of the other factor (gender). The contrast matrix is now a block-diagonal matrix. A B C A B C B - A -1 1 0 0 0 0 Female C - A -1 0 1 0 0 0 C - B 0 -1 1 0 0 0 B - A 0 0 0 -1 1 0 Male C - A 0 0 0 -1 0 1 C - B 0 0 0 0 -1 1 Step 1: Define the contrast matrix: K &lt;- rbind(c(-1, 1, 0, 0, 0 ,0), c(-1, 0, 1, 0, 0 , 0), c(0, -1, 1, 0, 0 , 0), c(0, 0, 0, -1, 1, 0), c(0, 0, 0, -1, 0, 1), c(0, 0, 0, 0, -1, 1)) colnames(K) &lt;- rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;),2) rownames(K) &lt;- c(&quot;boy:B-A&quot;, &quot;boy:C-A&quot;, &quot;boy:C-B&quot;, &quot;girl:B-A&quot;, &quot;girl:C-A&quot;, &quot;girl:C-B&quot;) K ## A B C A B C ## boy:B-A -1 1 0 0 0 0 ## boy:C-A -1 0 1 0 0 0 ## boy:C-B 0 -1 1 0 0 0 ## girl:B-A 0 0 0 -1 1 0 ## girl:C-A 0 0 0 -1 0 1 ## girl:C-B 0 0 0 0 -1 1 Step 2: Estimate auxiliary cell-means model (i.e., model without intercept) dat$gender.train &lt;- interaction(dat$training, dat$gender) # head(dat) model3 &lt;- aov(reading ~ -1 + gender.train, data = dat) coef(model3) ## gender.trainA.boy gender.trainB.boy gender.trainC.boy gender.trainA.girl gender.trainB.girl gender.trainC.girl ## 37.75 45.00 43.75 38.75 50.50 42.00 Step 3: Compute contrasts out &lt;- glht(model3, linfct = K) summary(out) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: aov(formula = reading ~ -1 + gender.train, data = dat) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## boy:B-A == 0 7.250 4.682 1.548 0.487 ## boy:C-A == 0 6.000 4.682 1.281 0.653 ## boy:C-B == 0 -1.250 4.682 -0.267 0.998 ## girl:B-A == 0 11.750 4.682 2.509 0.103 ## girl:C-A == 0 3.250 4.682 0.694 0.943 ## girl:C-B == 0 -8.500 4.682 -1.815 0.338 ## (Adjusted p values reported -- single-step method) Step 4: Visualization use plot() function plot(out) use ggplot() function ggstatsplot::ggcoefstats(out) + ggplot2::labs(x = &quot;Linear Function&quot;, y=NULL) 7.7 Factorial ANOVA as Multiple Regression The regression model for the 3 X 2 design can be written as: \\[\\begin{array}{l} \\hat Y = a + {b_1}boy + {b_2}condB + {b_3}condC + \\\\ {b_4}boy*condB + {b_5}boy*CondC \\end{array}\\] Where \\({b_1}\\), \\({b_2}\\), and \\({b_3}\\), are the main effects and \\({b_4}\\), and \\({b_5}\\) are the interaction effects. lm1 &lt;- lm(reading~ gender, data = dat) lm2 &lt;- lm(reading ~ gender + training, data = dat) lm3 &lt;- lm(reading ~ gender * training, data = dat) anova(lm1) ## Analysis of Variance Table ## ## Response: reading ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gender 1 15.04 15.042 0.2749 0.6053 ## Residuals 22 1203.92 54.723 anova(lm2) ## Analysis of Variance Table ## ## Response: reading ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gender 1 15.04 15.042 0.3569 0.55692 ## training 2 361.08 180.542 4.2842 0.02828 * ## Residuals 20 842.83 42.142 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(lm3) ## Analysis of Variance Table ## ## Response: reading ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gender 1 15.04 15.042 0.3430 0.56535 ## training 2 361.08 180.542 4.1175 0.03369 * ## gender:training 2 53.58 26.792 0.6110 0.55368 ## Residuals 18 789.25 43.847 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.8 A Blog on ANOVA in R ANOVA in R "],["analysis-of-covariance.html", "Chapter 8 Analysis of Covariance 8.1 Purposes of ANCOVA 8.2 Choice of covariates to use 8.3 Statistical Model for ANCOVA 8.4 ANCOVA Assumptions 8.5 Empirical Example 8.6 Post-hoc Tests using the multcomp package 8.7 ANCOVA as a GLM", " Chapter 8 Analysis of Covariance library(rio); library(ggplot2); library(QuantPsyc); library(psych); library(car); library(memisc); library(multcomp) One-way ANCOVA is used to describe the differences in predicted outcomes for a single dependent variable among multiple groups. The design has one treatment factor that represents group membership, a continuous independent variable, and a continuous dependent measure. ANCOVA analysis is used when the treatment and covariate do not interact. In this case, ANCOVA allows us to increase the power of our analysis and adjust outcome means due to initial group differences on the covariate. Example: If we are conducting a study in which we are interested in treatment effects on an achievement variable and we see that IQ is positively correlated with achievement (r = .60). By squaring the correlation between IQ and achievement, we see that IQ accounts for 36% of the within group variance in achievement. We are only interested in the effects of treatment. ANCOVA luckily removes the part of the variance due to IQ. It controls for the effects of IQ. Treatment 1 Treatment 2 IQ Achievement IQ Achievement 100.00 23.0 96.00 19.00 113.00 31.0 108.00 26.00 98.00 35.0 122.00 31.00 110.00 28.0 103.00 22.00 124.00 40.0 132.00 36.00 135.00 42.0 120.00 38.00 118.00 37.0 111.00 31.00 93.00 29.0 93.00 25.00 120.00 34.0 115.00 29.00 127.00 45.0 125.00 41.00 115.00 33.0 102.00 27.00 104.00 25.0 107.00 21.00 Mean 113.08 33.5 111.17 28.83 Run a t-test comparing the two treatment groups. dat &lt;- import(&quot;data/ancova_example.sav&quot;) head(dat) ## treatment achieve IQ ach_mean iq_mean achmsqrd iqmsqrd cross ## 1 1 23 100 -8.17 -12.125 66.7489 147.015625 99.06125 ## 2 1 31 113 -0.17 0.875 0.0289 0.765625 -0.14875 ## 3 1 35 98 3.83 -14.125 14.6689 199.515625 -54.09875 ## 4 1 28 110 -3.17 -2.125 10.0489 4.515625 6.73625 ## 5 1 40 124 8.83 11.875 77.9689 141.015625 104.85625 ## 6 1 42 135 10.83 22.875 117.2889 523.265625 247.73625 t.test(dat$achieve ~ dat$treatment, var.equal = TRUE) # assume equal variance between groups ## ## Two Sample t-test ## ## data: dat$achieve by dat$treatment ## t = 1.675, df = 22, p-value = 0.1081 ## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0 ## 95 percent confidence interval: ## -1.111453 10.444786 ## sample estimates: ## mean in group 1 mean in group 2 ## 33.50000 28.83333 # t.test(dat$achieve ~ dat$treatment, var.equal = FALSE) # equal variance between groups not assumed. This is default. # leveneTest(dat$achieve ~ as.factor(dat$treatment)) # test equality of variance between groups Example: If we are conducting a study in which we are interested in treatment effects on an achievement variable and we see that IQ is positively correlated with achievement (r = .60). By squaring the correlation between IQ and achievement, we see that IQ accounts for 36% of the within group variance in achievement. We are only interested in the effects of treatment. ANCOVA luckily removes the part of the variance due to IQ. It controls for the effects of IQ. Run an ANOVA model.anova &lt;- aov(achieve ~ as.factor(treatment), data = dat) summary(model.anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## as.factor(treatment) 1 130.7 130.67 2.805 0.108 ## Residuals 22 1024.7 46.58 Run an ANCOVA model.ancova &lt;- aov(achieve ~ IQ + as.factor(treatment), data = dat) # Type I Sums of Squares. Order of entry matters! summary(model.ancova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## IQ 1 711.2 711.2 41.76 2.1e-06 *** ## as.factor(treatment) 1 86.5 86.5 5.08 0.035 * ## Residuals 21 357.6 17.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ANCOVA provides a more powerful test than ANOVA (and the t-test) for this example. Other Examples: 1. Reduce bias when comparing intact or self-selected groups (e.g., males vs. females) 2. Adjust the posttest means on the dependent variable for any initial differences that may be present 8.1 Purposes of ANCOVA Elimination of systematic bias Systematic bias: Groups differ systematically on some variable that is related to the dependent variable. Not sure if differences are due to treatment or group differences when beginning the study. Random assignment takes care of systematic bias, but we are not always able to randomly assign participants to groups. You could match participants on certain variables. ANCOVA can reduce this bias. Reduction of within group or error variance What happens when we have smaller error variance? \\(M{S_b}/M{S_w}\\) To remove the variance due to the covariate: - The amount of variance on the dependent variable that is accounted for by the covariate is the squared value of the correlation between the two variables ( $r_{xy}^{2}$). The within group variance in ANCOVA has removed the portion due to the covariate: \\[M{{S}_{w}}-M{{S}_{w}}r_{xy}^{2}=M{{S}_{w}}(1-r_{xy}^{2})\\] - The new error term used in ANCOVA: \\(MS_{w}^{*}=M{{S}_{w}}(1-r_{xy}^{2})[1+1/({{f}_{e}}-2)]\\), where \\({f_e}\\) is the error degrees of freedom. - Example: One-way ANOVA (3 groups with 20 participants per group). The F = 200/100 = 2, which is not significant. Participants were pretested, but the pretest was not used as a covariate and is correlated with the posttest at .71. Using ANCOVA: $MS_{w}^{*}\\approx 100[1-{{(.71)}^{2}}]=50$ So, for example: \\({{F}^{*}}=190/50=3.8\\), which is significant. 8.2 Choice of covariates to use Variables that should correlate with the dependent variable Variables that have been shown to correlate with similar types of participants Ideally, the covariates should correlate significantly with the dependent variable but low correlations with other covariates (if two covariates are highly correlated with each other (e.g., .80), they will be removing much of the same error variance from the dependent variable. Limit the number of covariates to satisfy the following relationship: \\(\\frac{[C+(J-1)]}{N}&lt;.10\\), where C is the number of covariates, J is the number of treatment groups, and N is the total sample size. In studies where the ratio is greater than .10, the adjusted means become unstable. 8.3 Statistical Model for ANCOVA \\[y_{ij} = \\mu + {\\alpha _j} + \\beta {z_{ij}} + {\\varepsilon _{ij}}\\] - \\(y_{ij}\\) is the ith score of the jth group. - \\(\\mu\\) is the constant common to all y scores - \\(\\alpha_j\\) is teh effect of the jth treatment level (i.e, the group-specific effect) - \\(\\beta {z_{ij}}\\) quantifies the influence of the covariate (z) on the dependent variable (y). Note that the parameter \\(\\beta\\) represents the degree of (linear) relation between the covariate and teh dependent variable. - \\(\\varepsilon _{ij}\\) reflects random variation due to any uncontrolled source. ANCOVA combined features of ANOVA and linear regression. The group effects are specificed as in ANOVA, the relation between the covariate and the dependent variable is specified as in linear regression. 8.4 ANCOVA Assumptions Same three assumptions in ANOVA apply 7.4 Independence: the residuals are independent in the population. Consider study circumstances to identify any possible violations. Normality: the residuals are normally distributed for each group in the population. Inspect the distribution of the residuals for each group (use visual displays, descriptive and Shapiro-Wilk test to determine if residuals for each group are non-normally distributed). Homogeneity of Variance: the variance of the residuals is the same for each group in the population. Use Levenes test to identify if the residual variation is the same across groups. ANCOVA-specific Assumptions Linearity: The relationship between the covariate and the dependent variable is linear for each group in the population. Inspect the scatterplot of the covariate and outcome within each group to determine that the relationship is reasonably linear. Independence of the covariate and the independent variable: The covariate shares its variance only with the DV that is not explained by the IV. Rarely the case in reality If violated, the effect of the IV is confounded with the effect of the covariate. Homogeneity of regression slopes: The regression slope has the same value across all groups in the population. The regression lines of different groups should be parallel. No interaction effect between the covariate and the IV. Test if the interaction term between the IV and the covariate is statistically significant. Homogeneity of regression slopes Below is a figure where the regression slope does not depend on the groups. This is a figure where the regression slop varies with the groups. 8.5 Empirical Example Reading achievement and training conditions - Dependent variable: Reading achievement (RA) - Independent variable: Training condition (a = control, b = non-parental, c = parental) - Covariate: Initial reading experience (IRE) reading &lt;- c(40,33,34,35,44,48,43,51,46,38,44,46,42,54,43) groups &lt;- as.factor(rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 5)) init.read &lt;- c(37,23,30,35,41,45,40,44,45,30,40,41,37,51,39) dat &lt;- data.frame(reading, groups, init.read) model.anova&lt;-aov(reading ~ groups, data = dat) summary(model.anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## groups 2 230.5 115.3 4.968 0.0268 * ## Residuals 12 278.4 23.2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 model.ancova&lt;-aov(reading ~ init.read + groups, data = dat) #&lt;&lt; summary(model.ancova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## init.read 1 443.6 443.6 101.222 6.96e-07 *** ## groups 2 17.1 8.6 1.956 0.188 ## Residuals 11 48.2 4.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 8.5.1 Assumptions - Normality tapply(dat$reading, groups, shapiro.test) # Testing each grouop separately ## $a ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.88391, p-value = 0.3274 ## ## ## $b ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.98143, p-value = 0.9421 ## ## ## $c ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.81558, p-value = 0.1079 #tapply(dat$init.read, groups, shapiro.test) 8.5.2 Assumptions - Homogeneity of Variance #library(car) leveneTest(reading ~ groups, data = dat) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0.0363 0.9645 ## 12 8.5.3 Assumptions - Independence of the covariate and the independent variable This assumption can be tested using a regression model where the covariate is used as the outcome. Non-significant terms indicate independence. summary(lm(init.read ~ groups, data = dat)) ## ## Call: ## lm(formula = init.read ~ groups, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.8 -2.9 -0.6 4.0 9.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.200 2.813 11.802 5.82e-08 *** ## groupsb 7.600 3.978 1.910 0.0803 . ## groupsc 8.400 3.978 2.111 0.0564 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.29 on 12 degrees of freedom ## Multiple R-squared: 0.3116, Adjusted R-squared: 0.1969 ## F-statistic: 2.716 on 2 and 12 DF, p-value: 0.1064 8.5.4 Assumptions - Homogeneity of regression slopes Homogeneity of regression slopes can be tested through including an interaction term in the ANCOVA model. A non-significant interaction term indicates equal regression slopes. model2 &lt;- aov(reading ~ init.read* groups, data = dat) summary(model2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## init.read 1 443.6 443.6 95.269 4.38e-06 *** ## groups 2 17.1 8.6 1.841 0.214 ## init.read:groups 2 6.3 3.2 0.677 0.532 ## Residuals 9 41.9 4.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Anova(model2, type = &quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: reading ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 53.403 1 11.4693 0.008042 ** ## init.read 65.995 1 14.1738 0.004452 ** ## groups 3.636 2 0.3905 0.687664 ## init.read:groups 6.300 2 0.6766 0.532440 ## Residuals 41.905 9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ggplot(data=dat, aes(x=init.read, y=reading, group=groups, color=groups)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) 8.6 Post-hoc Tests using the multcomp package # library(multcomp) out &lt;- glht(model.ancova, linfct = mcp(groups = &quot;Tukey&quot;)) summary(out) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: aov(formula = reading ~ init.read + groups, data = dat) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## b - a == 0 2.70817 1.51197 1.791 0.217 ## c - a == 0 2.75114 1.55054 1.774 0.222 ## c - b == 0 0.04297 1.32621 0.032 0.999 ## (Adjusted p values reported -- single-step method) User-defined contrasts: K &lt;-rbind(c(1, -1/2, -1/2), # compares control versus mean of both treatment groups c(0, -1, 1)) # compares means of treatment 1 and 2 rownames(K) &lt;-c(&quot;control vs ave.treatment&quot;, &quot;treatment 2 vs treatment 1&quot;) colnames(K) &lt;-c(&quot;Group A&quot;, &quot;Group B&quot;, &quot;Group C&quot;) K ## Group A Group B Group C ## control vs ave.treatment 1 -0.5 -0.5 ## treatment 2 vs treatment 1 0 -1.0 1.0 out &lt;-glht(model.ancova, linfct= mcp(groups = K)) summary(out) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Fit: aov(formula = reading ~ init.read + groups, data = dat) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## control vs ave.treatment == 0 -2.72965 1.38036 -1.977 0.138 ## treatment 2 vs treatment 1 == 0 0.04297 1.32621 0.032 0.999 ## (Adjusted p values reported -- single-step method) 8.7 ANCOVA as a GLM The ANCOVA model can be reformulated as as GLM using the following regression equation: \\[y = {\\beta _0} + \\sum\\limits_{j = 1}^{k - 1} {{\\beta _j}{x_j} + {\\beta _{yz}}z + \\varepsilon }\\] \\({\\beta _0}\\) is the intercept; \\({\\beta _j}\\) represents the k-1 slopes of the k groups; \\({\\beta _{yz}}\\) is the slope of the covariate (z); \\(\\varepsilon\\) is the error term. Equation of the ANCOVA example, with group A as the reference group: \\[y = {\\beta _0} + {\\beta _1}condB + {\\beta _2}condC + {\\beta _3}init.read + \\varepsilon\\] Dummy coding with Group A as the reference group. Group B Group C Initial Reading 0 0 37 0 0 23 Group A 0 0 30 0 0 35 0 0 41 1 0 45 1 0 40 Group B 1 0 44 1 0 45 1 0 30 0 1 40 0 1 41 Group C 0 1 37 0 1 51 0 1 39 ANCOVA can be re-characterized as model selection problem: Step 1: Estimate a base model that incorporates all covariates. m0 &lt;-lm(reading ~ init.read, data = dat) Step 2: Add the independent variable of interest to the model. m1 &lt;-lm(reading ~ init.read + groups, data = dat) Step 3: Check whether the independent variable contributes to predicting the outcome. anova(m0, m1) ## Analysis of Variance Table ## ## Model 1: reading ~ init.read ## Model 2: reading ~ init.read + groups ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 13 65.347 ## 2 11 48.205 2 17.141 1.9557 0.1876 contrasts(dat$groups) &lt;- contr.treatment(n = 3, base = 1) model3 &lt;-lm(reading ~ init.read + groups, data = dat) contrasts(dat$groups) ## 2 3 ## a 0 0 ## b 1 0 ## c 0 1 summary(model3) ## ## Call: ## lm(formula = reading ~ init.read + groups, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4533 -0.9807 -0.1244 0.9933 3.5719 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.08307 3.32414 4.237 0.0014 ** ## init.read 0.69629 0.09607 7.248 1.65e-05 *** ## groups2 2.70817 1.51197 1.791 0.1008 ## groups3 2.75114 1.55054 1.774 0.1037 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.093 on 11 degrees of freedom ## Multiple R-squared: 0.9053, Adjusted R-squared: 0.8794 ## F-statistic: 35.04 on 3 and 11 DF, p-value: 6.351e-06 anova(model3) ## Analysis of Variance Table ## ## Response: reading ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## init.read 1 443.59 443.59 101.2220 6.957e-07 *** ## groups 2 17.14 8.57 1.9557 0.1876 ## Residuals 11 48.21 4.38 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 contrasts(dat$groups) &lt;- contr.treatment(n = 3, base = 2) model4 &lt;-lm(reading ~ init.read + groups, data = dat) contrasts(dat$groups) ## 1 3 ## a 1 0 ## b 0 0 ## c 0 1 summary(model4) ## ## Call: ## lm(formula = reading ~ init.read + groups, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4533 -0.9807 -0.1244 0.9933 3.5719 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.79124 4.02999 4.167 0.00157 ** ## init.read 0.69629 0.09607 7.248 1.65e-05 *** ## groups1 -2.70817 1.51197 -1.791 0.10079 ## groups3 0.04297 1.32621 0.032 0.97474 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.093 on 11 degrees of freedom ## Multiple R-squared: 0.9053, Adjusted R-squared: 0.8794 ## F-statistic: 35.04 on 3 and 11 DF, p-value: 6.351e-06 anova(model4) ## Analysis of Variance Table ## ## Response: reading ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## init.read 1 443.59 443.59 101.2220 6.957e-07 *** ## groups 2 17.14 8.57 1.9557 0.1876 ## Residuals 11 48.21 4.38 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["repeated-measures-anova-and-mixed-designs.html", "Chapter 9 Repeated Measures ANOVA and Mixed Designs 9.1 Repeated-Measures ANOVA 9.2 Mixed Designs", " Chapter 9 Repeated Measures ANOVA and Mixed Designs library(rio); library(ggplot2); library(QuantPsyc); library(psych); library(car); library(reshape); library(nlme); library(ez) 9.1 Repeated-Measures ANOVA Previous ANOVA designs discussed in class have measured participants at 1 time. In repeated-measures designs (also called within-subjects designs), participants are measured at different times (2 or more times) either using the same dependent variable measures or different dependent variable measures. RECALL: t Test for Dependent Samples: used when every participant is exposed to both levels of the independent variable or if two groups of participants are formed in such a way that some relationship exists between the two groups (e.g. participants in group A are matched on intelligence with participants in group B). Examples: a. Pretest  Treatment/Intervention  Posttest b. Two groups of matched or paired participants are compared on some dependent variable c. We are comparing naturally occurring correlated pairs, such as twins, husband and wife, parent and child, etc. While ANOVA (with more than 2 groups) is the generalization of the t test for independent samples (2 groups), repeated-measures ANOVA (when participants are measured more than twice) is the generalization of the t test for dependent (correlated) samples. Examples: trends/change across time: Vocab. Grade 8 Vocab. Grade 9 Vocab. Grade 10 Vocab. Grade 11 Participant 1 Participant 2  Participant n same participants under different treatments: Treatment 1 Treatment 2 Treatment 3 Treatment 4 Participant 1 Participant 2  Participant n same participants are given a series of tests or subtests: Information Vocabulary Digit Span Block Design Participant 1 Participant 2  Participant n combination with one-way ANOVA design (one between and one within-subjects variable): Posttest 6 Weeks 12 Weeks Teaching Method 1 66 64 63 Teaching Method 2 69 65 59 Teaching Method 3 62 56 52 If the above example was simply a one-way ANOVA using teaching method as the independent variable and the posttest as the dependent variable, we would see that participants do better with teaching method 2. However, if we were interested in the effects of these three teaching methods over time (6 and 12 weeks later), we would see that there is a decline for students with teaching methods 2 and 3 and that participants with teaching method 1 do the best across time. This is an example of a method (between) by time (within) interaction. A between variable is a grouping/classification variable (e.g., gender, race, social class, age group, method, mental illness). A within variable is one on which the participants have been measured repeatedly (e.g., time, treatment). One between and two within design: Situation 1 Situation 2 Time Morning Afternoon Morning Afternoon Age 3 years old 4 years old 5 years old You can think of the above example as a three-way ANOVA, however, the analysis we conduct is different due to correlations among the participants scores which must be taken into account. 9.1.1 Advantages of repeated-measures design Variability among participants due to individual differences is removed from the error term. We block on each participant. Blocking is a method of handling concomitant variables that is relatively common in some research areas. Participants are first sorted into groups or blocks that are relatively homogeneous as far as scores on the concomitant variable are concerned, and then treatments are randomly assigned within each block. The method involves treating the concomitant variable explicitly as a factor having certain discrete levels in the analysis (e.g., age categories may be created when examining motor skills of elderly participants). Repeated measures designs are the ultimate block design because a single person is in each block: Economy of participants. That is, fewer participants are required for the study. Suited to address specific research questions: learning, growth, development, etc. 9.1.2 Disadvantages of repeated-measures design Fatigue may occur when participants are repeatedly measured. Order effects may occur when administering treatments. Counterbalancing the order of treatments is an effective method of minimizing order effects. Counterbalance = all possible orders of treatments are administered: 3 Treatments = 3! = 3 * 2 * 1 = 6 possible orders: 1, 2, 3 1, 3, 2 2, 1, 3 2, 3, 1 3, 1, 2 3, 2, 1 Carryover effects refer to the impact of a previous trial (condition) on a participants performance on subsequent trials. Allowing an adequate amount of time to pass between treatments will minimize possible effects. Risk of attrition due to multiple measurement occasions 9.1.3 Variance partitioning in one-way repeated measures design Participant Treatment 1 Treatment 2 Treatment 3 Row Mean 1 30.0 28.0 34 30.67 2 14.0 18.0 22 18.00 3 24.0 20.0 30 24.67 4 38.0 34.0 44 38.67 5 26.0 28.0 30 28.00 Column Mean 26.4 25.6 32 28.00 Completely Randomized Design (One-Way ANOVA): Sums of Squares: \\(S{{S}_{b}}=\\sum\\limits_{i=1}^{k}{{{n}_{i}}{{({{{\\bar{x}}}_{i}}-\\bar{x})}^{2}}}\\) \\(= 5\\left[ {{{\\left( {26.4-28} \\right)}^2} + {\\rm{ }}{{\\left( {25.6-28} \\right)}^2} + {\\rm{ }}{{\\left( {32-28} \\right)}^2}} \\right]{\\rm{ }} = {\\rm{ }}121.60\\) \\(S{{S}_{w}}=\\sum\\limits_{1}{{{({{x}_{i1}}-{{{\\bar{x}}}_{1}})}^{2}}+\\sum\\limits_{2}{{{({{x}_{i2}}-{{{\\bar{x}}}_{2}})}^{2}}+\\cdots +\\sum\\limits_{k}{{{({{x}_{ik}}-{{{\\bar{x}}}_{k}})}^{2}}}}}\\) \\(= {(30 - 26.4)^2} + {(14 - 26.4)^2} + ... + {(26 - 26.4)^2}\\) \\(+ {(28 - 25.6)^2} + {(18 - 25.6)^2} + ... + {(28 - 25.6)^2}\\) \\(+ {(34 - 32)^2} + {(22 - 32)^2} + ... + {(30 - 32)^2}\\) \\(=734.4\\) Mean Square Between: \\(M{{S}_{b}}=S{{S}_{b}}/(k-1)= 121.6/2 = 60.80\\) Mean Square Within: \\(M{{S}_{w}}=S{{S}_{w}}/(N-k) = 734.4/12 = 61.20\\) F ratio: 60.80/61.20 = .99, which is not significant. Univariate Repeated Measures Analysis (we block on participants): Error variance is split into two parts in repeated measures designs: \\(S{S_w} = S{S_{bl}} + S{S_{res}}\\) where \\(S{S_{bl}}\\) is the sum of squares for block and \\(S{S_{res}}\\) is the sum of squares for residuals. \\(S{{S}_{bl}}=k\\sum{{{({{{\\bar{x}}}_{i}}-\\bar{x})}^{2}}}\\), where k is the number of repeated measures, \\({{\\bar{x}}_{i}}\\) is the mean for person i, and \\(\\bar{x}\\) is the grand mean. \\(S{{S}_{bl}} = 3[(30.67  28)^2 + (18  28)^2 +  + (28  28)^2] = 696.02\\) The error term for the repeated measures analysis is calculated by: \\(S{S_{res}} = S{S_w} - S{S_{bl}}\\) \\(S{S_{res}} = 734.4  696.02 = 38.38\\) This removes the variability due to individual differences (696.02) from the error term. What is left is within subject variability across treatments. Mean Square Residual: \\(M{{S}_{res}}=S{{S}_{res}}/(n-1)(k-1) = 38.38/4(2) = 4.80\\) F ratio: \\(\\frac{{M{S_b}}}{{M{S_{res}}}} = \\frac{{60.80}}{{4.80}} = 12.67\\) The critical F value with (k  1) = 2 and (n  1)(k  1) = 4(2) = 8 degrees of freedom is 4.46 at alpha = .05. Our obtained F value is greater than the critical value. Thus, there is a significant effect of treatment, F(2, 8) = 12.67, p &lt; .05. \\[{y_{ij}} = \\mu + {\\pi _i} + {\\tau _j} + {\\varepsilon _{ij}}\\] - \\({y_{ij}}\\) the score of subject i for measurement occasion j \\(\\mu\\) is the grand mean \\(\\pi _i\\) captures individual differences which is assumed to be constant over time and normally distributed: \\({\\pi _i} \\sim N(0,\\sigma _\\pi ^2)\\). This is the between-subjects variance. \\({\\tau _j}\\) reflects the time effect which is assumed to be equal for all subjects. \\({\\varepsilon _{ij}}\\) is the error component which is assumed to be normally distributed (i.e., \\({\\varepsilon _{ij}} \\sim N(0,\\sigma _\\varepsilon ^2)\\)). This the within-subjects variance. Repeated-Measures ANOVA Table: Table 9.1: Repeated-Measures ANOVA Table Source of Variaton Sum of Squares Degrees of Freedom Mean Squares \\(F\\) Statistic Subjects \\(SSQ_s = K\\sum\\limits_{i = 1}^n {{({\\mu _i} - \\mu)}^2}\\) \\(n-1\\) \\(MS _S = {{SSQ_S} \\over {n - 1}}\\) \\(F = {{MS _S} \\over {MS _E}}\\) Time \\(SSQ_T = n\\sum\\limits_{j = 1}^K {{({\\mu _j} - \\mu)}^2}\\) \\(K-1\\) \\(MS _T = {{SSQ _T} \\over {K-1}}\\) \\(F = {{MS _T} \\over {MS_E}}\\) Error \\(SSQ_E = \\sum\\limits_{i = 1}^n {\\sum\\limits_{j = 1}^K {{({y _{ij}} - {\\mu _i} - {\\mu _j} + {\\mu})}^2}}\\) \\((n-1)(K-1)\\) \\(MS _E = {{SSQ _E} \\over {(n-1)(K-1)}}\\) Total \\(SSQ_y = \\sum\\limits_{i = 1}^n {\\sum\\limits_{j = 1}^K {{({y _{ij}} - {\\mu})}^2}}\\) \\(nK-1\\) 9.1.4 Assumptions in repeated measures analysis Multivariate normality (inspect the distribution of scores within each trial). Fairly robust against non-normality. Can use more stringent alpha level. Independence of the observations (consider study circumstances to identify any possible violations). Can impact Type I error rate. Use a more stringent alpha level or alternative analyses (e.g., HLM). Sphericity Sphericity requires that the variances of the differences for all pairs of repeated measures are equal Mauchlys Test of Sphericity Sphericity requires that the variances of the differences for all pairs of repeated measures are equal. A significant result indicates that the sphericity assumption is violated. When sphericity is not met, the F ratio/test in the univariate approach is positively biased (falsely rejecting the null too often). That is, alpha is set at .05, but we are rejecting the null falsely 8-10% of the time. When sphericity is not met, Epsilon can be used to adjust degrees of freedom for the F test. Epsilon is a measure that indicates the extent to which the covariance matrix deviates from sphericity. When sphericity is met, Epsilon ( \\(\\varepsilon\\)) = 1. Greenhouse-Geisser \\(\\varepsilon\\). It can be quite conservative. This is recommended when the Greenhouse-Geisser Epsilon &lt; .70. The Huynh-Feldt \\(\\varepsilon\\) is less conservative. This is recommended when the Greenhouse-Geisser Epsilon &gt; .70. Another option when sphericity is not met is to use the results from the Multivariate procedure when reporting/interpreting the overall test of mean differences in the primary analysis (omnibus F test). The multivariate procedure (Multivariate Analysis of Variance: MANOVA) compares groups on the dependent variables simultaneously. Compound Symmetry Assumption The variance/covariance matrix is assumed to have a compound symmetry structure. The diagonal of the variance-covariance matrix contains constant variances \\(\\sigma _\\pi ^2 + \\sigma _\\varepsilon ^2\\) (i.e., variances are assumed to be homogeneous across time). The off-diagonal elements contain constant covariances\\(\\sigma _\\pi ^2\\) (i.e., covariancesare assumed to be homogeneous across time). $${_{yi}} = $$ The less restrictive assumption of sphericity states that the variance of the differences between each pair of groups (e.g., \\({y_{i1}} - {y_{i2}}\\), \\({y_{i1}} - {y_{i2}}\\), \\({y_{i2}} - {y_{i3}}\\)) is constant. 9.1.5 Alternative Specification of the Variance-Covariance Matrix Unstructured variance-covariance matrix:All variances and covariances are assumed to be unique. $${_{yi}} = $$ Autoregressive structure: More proximate observations are more correlated than more distant measures (i.e., the correlation steadily decreases with increasing distance between observations). $${_{yi}} = $$ Alternative model specifications can be tested using the (nested) likelihood ratio test. The test statistic is defined as 2 times the log of the likelihood ratio which follows a \\({\\chi ^2}\\) distribution (degrees of freedom = difference in the number of parameters in the two covariance structures). The model with a significantly smaller log-likelihood is preferred. 9.1.6 Empirical example Participant Treatment 1 Treatment 2 Treatment 3 1 30 28 34 2 14 18 22 3 24 20 30 4 38 34 44 5 26 28 30 subj &lt;- 1:5 T1 &lt;- c(30, 14, 24, 38, 26) T2 &lt;- c(28, 18, 20, 34, 28) T3 &lt;- c(34, 22, 30, 44, 30) dat &lt;- data.frame(subj, T1, T2, T3) Convert data from the wide format to the long format # library(reshape) dat.long &lt;- reshape(dat, idvar = &quot;subj&quot;, v.names = &quot;score&quot;, varying = c(&quot;T1&quot;, &quot;T2&quot;, &quot;T3&quot;), timevar = &quot;treatment&quot;, direction = &quot;long&quot;) dat.long$treatment &lt;- as.factor(dat.long$treatment) dat.long ## subj treatment score ## 1.1 1 1 30 ## 2.1 2 1 14 ## 3.1 3 1 24 ## 4.1 4 1 38 ## 5.1 5 1 26 ## 1.2 1 2 28 ## 2.2 2 2 18 ## 3.2 3 2 20 ## 4.2 4 2 34 ## 5.2 5 2 28 ## 1.3 1 3 34 ## 2.3 2 3 22 ## 3.3 3 3 30 ## 4.3 4 3 44 ## 5.3 5 3 30 Plot the Data ggplot(dat.long, aes(x=treatment, y=score, group=subj))+ geom_point() + geom_line() Method 1: Univariate approach using aov() model1 &lt;- aov(score ~ treatment + Error(subj/treatment), data = dat.long) summary(model1) ## ## Error: subj ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 1 70.53 70.53 ## ## Error: subj:treatment ## Df Sum Sq Mean Sq ## treatment 2 95.18 47.59 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 2 26.7 13.35 0.181 0.837 ## Residuals 9 663.6 73.73 Method 2: Univariate approach using lme() (multilevel model) #library(nlme) model2 &lt;- lme(score ~ treatment, random = ~1|subj/treatment, method = &quot;ML&quot;, data = dat.long) #&lt;&lt; summary(model2) ## Linear mixed-effects model fit by maximum likelihood ## Data: dat.long ## AIC BIC logLik ## 92.70244 96.95074 -40.35122 ## ## Random effects: ## Formula: ~1 | subj ## (Intercept) ## StdDev: 6.717142 ## ## Formula: ~1 | treatment %in% subj ## (Intercept) Residual ## StdDev: 1.626497 1.092936 ## ## Fixed effects: score ~ treatment ## Value Std.Error DF t-value p-value ## (Intercept) 26.4 3.498571 8 7.545938 0.0001 ## treatment2 -0.8 1.385641 8 -0.577350 0.5796 ## treatment3 5.6 1.385641 8 4.041452 0.0037 ## Correlation: ## (Intr) trtmn2 ## treatment2 -0.198 ## treatment3 -0.198 0.500 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -0.76160042 -0.36509711 -0.05496085 0.35135689 0.68308491 ## ## Number of Observations: 15 ## Number of Groups: ## subj treatment %in% subj ## 5 15 anova(model2) ## numDF denDF F-value p-value ## (Intercept) 1 8 67.58621 &lt;.0001 ## treatment 2 8 12.66667 0.0033 Method 3: Use the lm function. model3 &lt;- lm(score ~ treatment + factor(subj), data = dat.long) summary(model3) ## ## Call: ## lm(formula = score ~ treatment + factor(subj), data = dat.long) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.400 -1.333 0.000 1.133 2.400 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.067 1.497 19.421 5.13e-08 *** ## treatment2 -0.800 1.386 -0.577 0.579584 ## treatment3 5.600 1.386 4.041 0.003728 ** ## factor(subj)2 -12.667 1.789 -7.081 0.000104 *** ## factor(subj)3 -6.000 1.789 -3.354 0.010019 * ## factor(subj)4 8.000 1.789 4.472 0.002077 ** ## factor(subj)5 -2.667 1.789 -1.491 0.174373 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.191 on 8 degrees of freedom ## Multiple R-squared: 0.9551, Adjusted R-squared: 0.9215 ## F-statistic: 28.39 on 6 and 8 DF, p-value: 5.647e-05 anova(model3) ## Analysis of Variance Table ## ## Response: score ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 2 121.6 60.8 12.667 0.003318 ** ## factor(subj) 4 696.0 174.0 36.250 3.581e-05 *** ## Residuals 8 38.4 4.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Method 4: Us the ezANOVA function from the ez package # library(ez) model4 &lt;- ezANOVA(data = dat.long, dv = .(score), wid = .(subj), within = .(treatment), detailed = TRUE, type = 3) #&lt;&lt; ## Warning: Converting &quot;subj&quot; to factor for ANOVA. model4 ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ges ## 1 (Intercept) 1 4 11760.0 696.0 67.58621 0.001193346 * 0.9412217 ## 2 treatment 2 8 121.6 38.4 12.66667 0.003317760 * 0.1420561 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 2 treatment 0.4976852 0.351101 ## ## $`Sphericity Corrections` ## Effect GGe p[GG] p[GG]&lt;.05 HFe p[HF] p[HF]&lt;.05 ## 2 treatment 0.6656394 0.01218473 * 0.8724018 0.005427211 * 9.2 Mixed Designs 9.2.1 One between and one within factor design The one between and one within factor design is used to identify interactions and main effects in experimental designs with two independent variables. The design has two categorical factors  one between-subjects factor and one within-subjects factor  and a continuous dependent measure. The primary advantages in using this repeated measures design is that it allows you to assess the impact of a treatment across time, or examine the performance of the same sets of participants under different treatments, and that such designs tend to be more powerful than designs having only between-subjects factors. Assumptions Multivariate normality (inspect the distribution of scores in each cell) Independence of observations (consider the sampling design and study circumstances to identify any possible violations) Sphericity (assume that this is violated and use an adjusted F test, such as the Greenhouse-Geisser, or use the multivariate test results for the overall tests in the primary analysis) Between-groups equality of variance (use Levenes test; examine standard deviations for each group) Example: The effects of therapy (family therapy group versus control group) on weight gain across time (beginning weight, weight at 12 weeks, and weight at 24 weeks) for teen girls with anorexia. mydata &lt;- import(&quot;data/2_group_repeated_measures.sav&quot;) head(mydata) ## weight1 weight2 weight3 group ## 1 83.8 95.2 107.1 1 ## 2 83.3 94.3 109.2 1 ## 3 86.0 91.5 110.0 1 ## 4 82.5 91.9 106.5 1 ## 5 86.7 100.3 104.2 1 ## 6 79.6 82.5 100.2 1 mydata$group &lt;- as.factor(mydata$group) mydata.long &lt;- reshape(mydata, idvar = &quot;id&quot;, v.names = &quot;weight&quot;, varying = c(&quot;weight1&quot;, &quot;weight2&quot;, &quot;weight3&quot;), direction = &quot;long&quot;) mydata.long$time &lt;- as.factor(mydata.long$time) ezANOVA(data = mydata.long, dv = .(weight), wid = .(id), between = .(group), within = .(time), type = 3, detailed = TRUE) ## Warning: Converting &quot;id&quot; to factor for ANOVA. ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ges ## 1 (Intercept) 1 38 958797.141 2996.4503 12159.15077 3.067707e-49 * 0.9963410 ## 2 group 1 38 3111.008 2996.4503 39.45279 2.343883e-07 * 0.4690783 ## 3 time 2 76 2293.617 524.7147 166.10444 1.808772e-28 * 0.3944459 ## 4 group:time 2 76 2379.349 524.7147 172.31317 5.792294e-29 * 0.4032443 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 3 time 0.984454 0.7483688 ## 4 group:time 0.984454 0.7483688 ## ## $`Sphericity Corrections` ## Effect GGe p[GG] p[GG]&lt;.05 HFe p[HF] p[HF]&lt;.05 ## 3 time 0.984692 4.521768e-28 * 1.038089 1.808772e-28 * ## 4 group:time 0.984692 1.473336e-28 * 1.038089 5.792294e-29 * Within-Subjects Analysis: - Is the average weight for all 40 subjects different at the 3 points in time? This captures the main effect of time. (This table reports the results of the traditional repeated-measures ANOVA.) Is the difference in means between the therapy and control groups the same or different across time? (If this contrast varies across time, there is an interaction between time and group.) This question is of primary interest in this study as we expect the two groups to have similar weight at time 1, but different average weight at times 2 and 3 (due to the therapy). Between-Subjects Analysis: Is there an overall weight difference between the family therapy and control group in the population (averaging across the three time points)? (There is a 10-point difference in the sample favoring the family therapy group.) This question is not of primary interest because the overall treatment effects will be diluted, as we expect no between-group difference at time 1, due to random assignment. 9.2.2 A few online resources A blog on repeated-measures ANOVA in R For mixed models, the lmer() function from the lme4 package is the recommended method. https://arbor-analytics.com/post/mixed-models-a-primer https://m-clark.github.io/mixed-models-with-R/ "],["power-analysis.html", "Chapter 10 Power Analysis 10.1 Components of Power Analysis 10.2 Effect Sizes 10.3 Types of Power Analysis 10.4 a Two-Tailed t Test for Independent Samples Power Example 10.5 Software", " Chapter 10 Power Analysis When you conduct null hypothesis significance testing (NHST), based on your data, your conclusion is to either reject or fail to reject the null hypothesis. Your conclusion may be correct or incorrect. When the null hypothesis is incorrectly rejected, youve made a Type I error. When the null hypothesis is incorrectly not rejected, youve made a Type II error. Formally, Type I error (\\(\\alpha\\)) is the probability of rejecting the null hypothesis when it is true. Type I error is usually set at .05 (we are willing to be wrong 5 out of 100 times). Type II error (\\(\\beta\\)) is the probability of failing to reject the null when it is false (saying that the groups dont differ when they do). Outcomes of hypothesis testing: Probabilities of outcomes of hypothesis testing: (1  \\(\\beta\\)) is called the statistical power of a test. 10.1 Components of Power Analysis Statistical procedure t-tests (independent samples, correlations, and any other t-test), F-tests (ANOVAS, multiple correlation and regression, and any other F-test), and Chi2-tests (goodness of fit and contingency tables) Specification of Null and Alternative Hypothesis Nominal significance level \\(\\alpha\\) Desired power 1  \\(\\beta\\) Effect size Sample size n 10.2 Effect Sizes Common effect sizes Determining Effect Sizes substantive knowledge findings from prior studies results from a pilot study common effect size convention (cf. Cohen, 1988) 10.3 Types of Power Analysis A priori Power Analysis The necessary sample size is calculated as a function of user-specified significance level ( \\(\\alpha\\)), statistical power, and the to-be-detected effect size. Post-hoc Power Analysis The statistical power is computed as a function of the significance level ( \\(\\alpha\\)), sample size (n), and the (population) effect size. Sensitivity Analysis The required population effect size is computed as a function of the significance level ( \\(\\alpha\\)), the power (1  \\(\\beta\\)), and sample size (n). Compromise Analysis The critical cut-off value and the associated \\(\\alpha\\) and \\(\\beta\\) values are computed as a function of the desired Criterion Analysis The required significance level ( \\(\\alpha\\) is computed as a function of power, sample size (n), and population effect. 10.3.1 A priori power analysis Steps for Sample Size Determination Specify statistical procedure (e.g., ANOVA, regression, etc.) Specify null- and alternative hypothesis (one- or two-sided testing). Specify alpha level and desired power (e.g., \\(\\alpha\\) = 0.05, (1  \\(\\beta\\)) = 0.80) Specify desired effect size 10.3.2 Post-hoc power analysis Steps for Power Determination Specify statistical procedure (e.g., ANOVA, regression, etc.) Specify null- and alternative hypothesis (one- or two-sided testing). Specify alpha level Specify sample size Estimate observed effect size 10.4 a Two-Tailed t Test for Independent Samples Power Example In t tests for independent samples, the effect size is given by: \\(d = {{(\\mu_1 - \\mu _2)} \\over \\sigma}\\), where \\(\\sigma\\) is the assumed common population standard deviation. This population effect size is estimated by: \\(\\hat d = ({\\bar x_1} - {\\bar x_2})/s\\), where \\(\\hat \\sigma = s = \\sqrt {{{({n_1} - 1)s_1^2 + ({n_2} - 1)s_2^2} \\over {{n_1} + {n_2} - 2}}}\\) is the estimate of the assumed common population standard deviation. An effect size of \\(d\\) = .2 is small, \\(d\\) = .5 is medium, and \\(d\\) = .8 is large. For a medium effect \\(d\\) = .5 and a sample size \\(n\\) = 15, we can see that as \\(\\alpha\\) decreases, \\(\\beta\\) increases, and statistical power decreases. \\(\\alpha\\) \\(\\beta\\) 1-\\(\\beta\\) (Power) 0.10 0.62 0.38 0.05 0.74 0.26 0.01 0.90 0.10 For a medium effect \\(d\\) = .5 and Type I error \\(\\alpha\\) = 0.05, we can see that as the sample size increases, \\(\\beta\\) decreases, and statistical power increases. \\(n\\) \\(\\beta\\) 1-\\(\\beta\\) (Power) 10 0.81 0.19 50 0.30 0.70 100 0.06 0.94 With a sample size \\(n\\) = 75 and Type I error \\(\\alpha\\) = 0.05, we can see that as the effect size increases, \\(\\beta\\) decreases, and statistical power increases. \\(d\\) \\(\\beta\\) 1-\\(\\beta\\) (Power) 0.2 0.77 0.23 0.5 0.14 0.86 0.8 0.00 1.00 10.5 Software **G*Power** performs high-precision statistical power analyses for the most common statistical tests in behavioral research including t tests (independent samples, correlations, and any other t-test), F-tests (ANOVAS, multiple correlation and regression, and any other F-test), and Chi2-tests (goodness of fit and contingency tables). It can be used to conduct all five types of power analysis (a priori, Post-hoc, sensitivity, compromise, and criterion). pwr R package A quick tutorial to pwr Superpower R package WebPower Simply google power analysis calculator, you will see many tools. "],["references.html", "References", " References "]]
