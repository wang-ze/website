<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>ESC_PS 8850:  Quantitative Foundations in Educational Research</title>
    <meta charset="utf-8" />
    <meta name="author" content="Ze Wang" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# ESC_PS 8850: <br>Quantitative Foundations in Educational Research
## <br/>Monday, 4:00pm-6:30pm; online
### Ze Wang
### University of Missouri
### 2021-12-6

---

layout: true

&lt;div class = 'my-header'&gt;&lt;span&gt;ESC_PS 8850 Quant Foundations&lt;/span&gt;&lt;/div&gt;

&lt;div class = 'my-footer'&gt;&lt;span&gt;wangze@missouri.edu
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
Ze Wang, Ph.D.&lt;/span&gt;&lt;/div&gt; 


---
class: center, middle








# .blue[Week 14]

## .blue[Review]
## .blue[Power Analysis]

---

```r
library(ggplot2); library(rio); library(QuantPsyc); library(psych); library(car); library(reshape); library(nlme); library(ez)
```

---



---
class: center, middle

## .blue[Review]

---
### What is the general linear model?

- The General Linear Model (GLM) is the overarching method that includes regression and ANOVA.

- The GLM conception is that observed data can be described in terms of a particular model and some error, 

.center[
`\(data = model + error\)`
]

  where the *model* term represents the hypothesis of interest; and the *error* term describes other (unconsidered) influences.

- The relative size of model and error components can be used to judge how well the model describes the data.

---
### What is the general linear model?

- .red[**General**] refers to the fact that GLM can be used to analyze continuous independent variables (as in regression analysis) and categorical independent variables (e.g., groups; as in ANOVA).

- .red[**Linear**] means that linearity is assumed in terms of model parameters (not necessarily in terms of variables)

  - Examples of linear models:
  
    `\({Y_i} = \alpha  + \beta {X_i} + {\varepsilon _i}\)`
    
    `\({Y_i} = \alpha  + \beta X_i^2 + {\varepsilon _i}\)`


---
### Logistic Regression

Binary logistic regression is a form of regression which is used when the DV is a dichotomy and the IVs are of any type. It belongs to **generalized** linear models.

The DV in logistics regression does not follow a normal distribution. For binary logistics regression, the DV follows a Binomial distribution for which you can specify a link function (logit and probit are two commonly used link functions).


---

### Which Analysis Technique to Use?

&lt;img src="images/FieldBookLastPageWhichTechnique.png" width="80%" /&gt;


---
### Assumptions of Linear Models

  - Independence of Observations
  - Linearity
  - Lack of multicollinearity
  - Homoscedasticity of Residuals/Homogeneity of Variance
  - Normality
  - Independence of the covariate and the independent variable (ANCOVA; rarely the case in reality)
  - Homogeneity of regression slopes (ANCOVA)
  
---
### Outlier Diagnostics

- Leverage
- Discrepancy
- Influence

---

### Correlations

- Pearson Correlation (direction, strength, statistical significance, normality, linearity)
- Point Biserial Correlation
- Biserial Correlation
- Spearman Rank
- Phi Coefficient
- Tetrachoric Correlation
- Polychoric Correlation
- Chi-Square
- Pearson's Chi-Square

---

### Correlations

.center[
&lt;table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;Summary of Measures of Association&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Correlation Coefficient &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; X Variable &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Y Variable &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Pearson product-moment &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; continuous &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; continuous &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Point Biserial &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; continuous &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; true dichotomy &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Biserial &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; continuous &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; forced dichotomy &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Phi &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; true dichotomy &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; true dichotomy &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Tetrachoric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; forced dichotomy &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; forced dichotomy &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Polychoric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; forced polytomy &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; forced polytomy &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Spearman's Rank &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; ranked &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; ranked &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Pearson Chi-Square &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; categorical &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; categorical &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

---

### Simple Linear Regression

Equation: `\(Y=a+bX+e\)`

Intercept: `\(a=\bar{Y}-b\bar{X}\)`

Slope: `\(b=\frac{\sum{xy}}{\sum{{{x}^{2}}}}=\frac{\sum{(X-\bar{X})(Y-\bar{Y})}}{\sum{{{(X-\bar{X})}^{2}}}}\)`

Standardized regression coefficient: 
`$$\beta =b\frac{{{s}_{x}}}{{{s}_{y}}}=\frac{\sum{xy}\sqrt{\sum{{{x}^{2}}}}\sqrt{n-1}}{\sum{{{x}^{2}}}\sqrt{n-1}\sqrt{\sum{{{y}^{2}}}}}=\frac{\sum{xy}}{\sqrt{\sum{{{x}^{2}}}\sqrt{\sum{{{y}^{2}}}}}}={{r}_{xy}}$$`
---
### Simple Linear Regression

`\(S{{S}_{total}}=S{{S}_{reg}}+S{{S}_{res}}\)`

`$${{R}^{2}}=1-\frac{S{{S}_{res}}}{S{{S}_{total}}}$$` where `\(S{{S}_{res}}\)` =  `\(\sum{{{(Y-{\hat Y})}^{2}}=\sum{{{e}^{2}}}}\)` and `\(S{{S}_{total}}\)` = `\(\sum{{{(Y-\bar{Y})}^{2}}}\)`.


`$$R_{adj}^{2}={{R}^{2}}-\frac{(1-{{R}^{2}})k}{n-k-1}$$` 

Significance of model: `\(F=\frac{MS_{reg}}{MS_{res}}\)` with `\(df_{reg}\)`

Significance of slope: `\(t=\frac{b}{{{s}_{b}}}=\frac{b-0}{{{s}_{b}}}\)`

Prediction intervals (for the mean vs. for the individual)

---
### Multiple Regression

Equation: `\(Y=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+e\)`

`\(Y=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+...+{{b}_{k}}{{X}_{k}}+e\)`


`$${{b}_{1}}={{\beta }_{1}}\frac{{{s}_{Y}}}{{{s}_{X1}}}$$`
`$${{b}_{2}}={{\beta }_{2}}\frac{{{s}_{Y}}}{{{s}_{X2}}}$$`
`$$a=\bar{Y}-{{b}_{1}}{{\bar{X}}_{1}}-{{b}_{2}}{{\bar{X}}_{2}}$$`
---
### Multiple Regression

Partial standardized regression coefficients

Partial regression coefficients

When we interpret the partial regression coefficient for one independent variable, we should state that we are *holding the other independent variable constant*. 

---
### Binary Logistic Regression

`$${\rm{logit}} = a + bX$$`
`$$Odds = {e^{(a + bX)}}$$`
`$$P(Y = 1) = \frac{{{e^{a + bX}}}}{{1 + {e^{a + bX}}}} = \frac{1}{{1 + {e^{ - (a + bX)}}}}$$`


---
### ANOVA

Are there significant differences between mean scores of a  dependent variable across a number of groups?

- Statistical Hypotheses:

  - Null Hypothesis: `\({H_0}:{\mu _1} = {\mu _2} = {\mu _3} = ... = {\mu _k}\)` (The group means are equal in the population, where *k* is the number of groups)
  
  - Alternative Hypothesis: At least two of the `\({\mu j}\)` (group means) are different in the population.

- **Variance Partitioning** (It’s all about the Variance). 

  - **Total Sum of Squares**: The sum of squared deviations of individual data points around the overall mean (i.e., the grand mean).

  - **Between-group Sum of Squares**: The sum of squared deviations of the group means around the overall mean.

  - **Within-group Sum of Squares**: The sum of squared deviations of individual data  points around the group means. 

---
### ANOVA

&lt;table class=" lightable-classic table table-striped" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;'&gt;
&lt;caption&gt;ANOVA Table for One-Way ANOVA&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Source of Variaton &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Sum of Squares &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Degrees of Freedom &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Mean Squares &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(F\)` Statistic &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Between-group (model) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(SS_b = \sum\limits_{j = 1}^k {{n_j}{{({\mu _j} - \mu )}^2}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(df = k-1\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `$$MS _b = {{SS_b} \over {k - 1}}$$` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(F = {{MS _b} \over {MS _w}}\)` &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Within-group (error) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(SS_w = \sum\limits_{j = 1}^k {\sum\limits_{i = 1}^{n _j} {{({y _{ij}} - \mu _j)}^2}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(df = N-k\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `$$MS _w = {{SS _w} \over {N-k}}$$` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Total &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(SS_t = \sum\limits_{j = 1}^k {\sum\limits_{i=1}^{n _j} {{({y _{ij}} - \mu)}^2}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(df = N-1\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

- Planned Comparisons
- Post-hoc Pairwise Comparisons (Bonferroni Correction; Tukey‘s Honest Significant Difference)

---
### Factorial ANOVA

Factorial ANOVA is used when you have two or more categorical independent variables.

**Two-Way ANOVA** allows **two factors** to be included in the same model (J X K groups). A 3 X 2 ANOVA means there are three levels of Factor A and two levels of Factor B.

- Main Effect for A:  `\({{H}_{0}}:{{\mu }_{.1}}={{\mu }_{.2}}=\cdots ={{\mu }_{.J}}\)`  Is there a significant difference between training groups, holding gender constant?

- Main Effect for B:  `\({{H}_{0}}:{{\mu }_{1.}}={{\mu }_{2.}}=\cdots ={{\mu }_{K.}}\)`  Is there a significant gender difference, holding training conditions constant? 

- Interaction Effect (Factor A X Factor B): ):  `\({{H}_{0}}:{{\varphi }_{ij}}=0\)` Are the differences between boys and girls different across different training conditions? Alternatively, are training effects different for boys and girls?

---
### Factorial ANOVA

&lt;table class=" lightable-classic table table-striped" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto; font-size: 16px; margin-left: auto; margin-right: auto;'&gt;
&lt;caption style="font-size: initial !important;"&gt;ANOVA Table for Two-Way ANOVA&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Source of Variaton &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Sum of Squares &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Degrees of Freedom &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Mean Squares &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(F\)` Statistic &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Factor A &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(SSQ_A = {nK}\sum\limits_{j = 1}^J {{({\mu _j} - \mu)}^2}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(J-1\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(MS _A = {{SSQ_A} \over {J - 1}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(F = {{MS _A} \over {MS _E}}\)` &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Factor B &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(SSQ_B = {nJ}\sum\limits_{k = 1}^K {{({\mu _k} - \mu)}^2}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(K-1\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(MS _B = {{SSQ _B} \over {K-1}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(F = {{MS _B} \over {MS_E}}\)` &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Interaction (A `\(\times\)` B) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(SSQ_{AB} = n\sum\limits_{j}^J {\sum\limits_{k}^K {{({\mu _{jk}} - {\mu _j} - {\mu _k} + {\mu})}^2}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\((J-1)(K-1)\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(MS _{AB} = {{SSQ _{AB}} \over {(J-1)(K-1)}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(F = {{MS _{AB}} \over {MS_E}}\)` &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Error &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(SSQ_E = \sum\limits_{i}^{n} {\sum\limits_{j}^J {\sum\limits_{k}^K {{({\mu _{ijk}} - {\mu _jk})}^2}}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(JK(n-1)\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(MS _E = {{SSQ _E} \over {JK(n-1)}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Total &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(SSQ_y = \sum\limits_{i}^{n} {\sum\limits_{j}^J {\sum\limits_{k}^K {{({\mu _{ijk}} - {\mu})}^2}}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(nJK-1\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

Note that `$$R^2 = {{SSQ_A + SSQ_B + SSQ_AB} \over {SSQ_y}}$$`


---
### ANCOVA

- One-way ANCOVA is used to describe the differences in predicted outcomes for a single dependent variable among multiple groups.  

- The design has one treatment factor that represents group membership, a continuous independent variable, and a continuous dependent measure.  

- ANCOVA analysis is used when the treatment and covariate do not interact.  In this case, ANCOVA allows us to increase the power of our analysis and adjust outcome means due to initial group differences on the covariate.  


---
### Repeated-Measures ANOVA

In *repeated-measures* designs (also called within-subjects designs), participants are measured at different times (2 or more times) either using the same dependent variable measures or different dependent variable measures.



&lt;table class=" lightable-classic table table-striped" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto; font-size: 16px; margin-left: auto; margin-right: auto;'&gt;
&lt;caption style="font-size: initial !important;"&gt;Repeated-Measures ANOVA Table&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Source of Variaton &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Sum of Squares &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Degrees of Freedom &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Mean Squares &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(F\)` Statistic &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Subjects &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(SSQ_s = K\sum\limits_{i = 1}^n {{({\mu _i} - \mu)}^2}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(n-1\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(MS _S = {{SSQ_S} \over {n - 1}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(F = {{MS _S} \over {MS _E}}\)` &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Time &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(SSQ_T = n\sum\limits_{j = 1}^K {{({\mu _j} - \mu)}^2}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(K-1\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(MS _T = {{SSQ _T} \over {K-1}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(F = {{MS _T} \over {MS_E}}\)` &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Error &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(SSQ_E = \sum\limits_{i = 1}^n {\sum\limits_{j = 1}^K {{({y _{ij}} - {\mu _i} - {\mu _j} + {\mu})}^2}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\((n-1)(K-1)\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(MS _E = {{SSQ _E} \over {(n-1)(K-1)}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Total &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(SSQ_y = \sum\limits_{i = 1}^n {\sum\limits_{j = 1}^K {{({y _{ij}} - {\mu})}^2}}\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; `\(nK-1\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
### Repeated-Measures ANOVA

Assumptions

- Multivariate normality

- Independence of observations

- Sphericity

---
### Mixed Designs

Has both within-subjects and between-subjects independent variables. 

Assumptions

- Multivariate normality

- Independence of observations

- Sphericity

- Between-groups equality of variance


---
class: center, middle

## .blue[Power Analysis]

---
### Power Analysis

When you conduct null hypothesis significance testing (NHST), based on your data, your conclusion is to either reject or fail to reject the null hypothesis. Your conclusion may be correct or incorrect. When the null hypothesis is incorrectly rejected, you've made a _Type I error_. When the null hypothesis is incorrectly not rejected, you've made a _Type II error_. Formally, Type I error ($\alpha$) is the probability of rejecting the null hypothesis when it is true.  Type I error is usually set at .05 (we are willing to be wrong 5 out of 100 times). Type II error ($\beta$) is the probability of failing to reject the null when it is false (saying that the groups don't differ when they do).  


---
### Power Analysis

&lt;img src="images/Table3.1_no_title.png" width="70%" /&gt;

---
### Power Analysis

&lt;img src="images/Table3.2_no_title.png" width="50%" /&gt;

(1 − `\(\beta\)`) is called the statistical power of a test.



---
### Components of Power Analysis

- Statistical procedure 
  - t-tests (independent samples, correlations, and any other t-test), 
  - F-tests (ANOVAS, multiple correlation and regression, and any other F-test), and 
  - Chi^2^-tests (goodness of fit and contingency tables)

- Specification of Null and Alternative Hypothesis

- Nominal significance level `\(\alpha\)`


- Desired power 1 − `\(\beta\)` 

- Effect size

- Sample size *n*


---
### Common Effect Sizes

&lt;img src="images/Common_effect_sizes.png" width="70%" /&gt;

---
### Determining Effect Sizes

- substantive knowledge
- findings from prior studies
- results from a pilot study
- common effect size convention (cf. Cohen, 1988)


&lt;img src="images/Common_effect_size_convention.png" width="70%" /&gt;


---
### Types of Power Analysis

- **A priori Power Analysis**
The *necessary sample size* is calculated as a function of user-specified significance  level ( `\(\alpha\)`), statistical power, and the to-be-detected effect size.

- **Post-hoc Power Analysis**
The *statistical power* is computed as a function of the significance level ( `\(\alpha\)`), sample  size (n), and the (population) effect size.

- **Sensitivity Analysis** 
The required *population effect size* is computed as a function of the significance level ( `\(\alpha\)`), the power (1 − `\(\beta\)`), and sample size (n).

- **Compromise Analysis**
The *critical cut-off* value and the associated `\(\alpha\)` and `\(\beta\)` values are computed as a function  of the desired 

- **Criterion Analysis**
The *required significance level* ( `\(\alpha\)` is computed as a function of power, sample size (n),  and population effect.


---
### A priori Power Analysis

**Steps for Sample Size Determination**

- Specify statistical procedure (e.g., ANOVA, regression, etc.)
- Specify null- and alternative hypothesis (one- or two-sided testing).
- Specify alpha level and desired power (e.g., `\(\alpha\)` = 0.05, (1 – `\(\beta\)`) = 0.80)
- Specify desired effect size

---
### Post-hoc Power Analysis

**Steps for Power Determination**

- Specify statistical procedure (e.g., ANOVA, regression, etc.)
- Specify null- and alternative hypothesis (one- or two-sided testing).
- Specify alpha level
- Specify sample size
- Estimate observed effect size



---
## a Two-Tailed _t Test_ for Independent Samples Power Example

In t tests for independent samples, the effect size is given by: `\(d = {{(\mu_1 - \mu _2)} \over \sigma}\)`, where `\(\sigma\)` is the assumed common population standard deviation.  

This population effect size is estimated by: `\(\hat d = ({\bar x_1} - {\bar x_2})/s\)`, where `\(\hat \sigma  = s = \sqrt {{{({n_1} - 1)s_1^2 + ({n_2} - 1)s_2^2} \over {{n_1} + {n_2} - 2}}}\)` is the estimate of the assumed common population standard deviation. 
An effect size of `\(d\)` = .2 is small, `\(d\)` = .5 is medium, and `\(d\)` = .8 is large. For a medium effect `\(d\)` = .5 and a sample size `\(n\)` = 15, we can see that as `\(\alpha\)` decreases, `\(\beta\)` increases, and statistical power decreases. 

&lt;table class=" lightable-classic" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto;'&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; `\(\alpha\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(\beta\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 1-$\beta$ (Power) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.10 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.62 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.38 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.05 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.74 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.26 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.01 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.10 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
## a Two-Tailed _t Test_ for Independent Samples Power Example

For a medium effect `\(d\)` = .5 and Type I error `\(\alpha\)` = 0.05, we can see that as the sample size increases, `\(\beta\)` decreases, and statistical power increases.

&lt;table class=" lightable-classic" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto;'&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; `\(n\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(\beta\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 1-$\beta$ (Power) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.81 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.19 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.30 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.70 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 100 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.06 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.94 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


---
## a Two-Tailed _t Test_ for Independent Samples Power Example
With a sample size `\(n\)` = 75 and Type I error `\(\alpha\)` = 0.05, we can see that as the effect size increases, `\(\beta\)` decreases, and statistical power increases.

&lt;table class=" lightable-classic" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto;'&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; `\(d\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(\beta\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 1-$\beta$ (Power) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.77 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.23 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.14 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.86 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.00 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


---
### G*Power

[**G*Power**](https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html) performs high-precision statistical power analyses for the most common statistical tests in behavioral research including t tests (independent samples, correlations, and any other t-test), F-tests (ANOVAS, multiple correlation and regression, and any other F-test), and Chi^2^-tests (goodness of fit and contingency tables). It can be used to conduct all five types of power analysis (a priori, Post-hoc, sensitivity, compromise, and criterion). 


.pull-left[
&lt;img src="images/G_power_a_prior_example.png" width="70%" /&gt;
]


.pull-right[
&lt;img src="images/G_power_a_prior_plot.png" width="70%" /&gt;
]

---
### Other Software

- **`pwr` R package**

[A quick tutorial to `pwr`](https://www.statmethods.net/stats/power.html)


- [**`Superpower` R package**](https://aaroncaldwell.us/SuperpowerBook/)

- **[WebPower](https://webpower.psychstat.org/)**

Simply google "power analysis calculator", you will see many tools.
???
- [WebPower](https://webpower.psychstat.org/) (developed by Zhiyong Zhang from U. of Notre Dame)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
