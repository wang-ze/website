<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>ESC_PS 8850:  Quantitative Foundations in Educational Research</title>
    <meta charset="utf-8" />
    <meta name="author" content="Ze Wang" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# ESC_PS 8850: <br>Quantitative Foundations in Educational Research
## <br/>Monday, 4:00pm-6:30pm; online
### Ze Wang
### University of Missouri
### 2021-9-27

---

layout: true

&lt;div class = 'my-header'&gt;&lt;span&gt;ESC_PS 8850 Quant Foundations&lt;/span&gt;&lt;/div&gt;

&lt;div class = 'my-footer'&gt;&lt;span&gt;wangze@missouri.edu
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
Ze Wang, Ph.D.&lt;/span&gt;&lt;/div&gt; 


---
class: center, middle








# .blue[Week 5]

## Multiple Regression Analysis

---

```r
library(ggplot2); library(rio); library(QuantPsyc)
```

---

## Multiple Regression Analysis

**Recall**:  Simple Linear Regression:	`\(Y=a+bX+e\)` 

Regression with 2 Independent Variables:	`\(Y=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+e\)`, where a is the intercept/constant (value of Y when X1 and X2 are both equal to zero), `\({{b}_{1}}\ \text{and}\ {{b}_{2}}\)` are regression coefficients associated with X1 and X2, respectively, X1 and X2 are raw scores on the independent variables, and e is the error or residual.

Can be extended to incorporate more independent variables: `\(Y=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+...+{{b}_{k}}{{X}_{k}}+e\)`, where k is the number of independent variables.


---

## Multiple Regression Analysis

**Recall**:  Simple Linear Regression: `\({\hat Y}=a+bX\)`.  Solved for *a* and *b* using Least Squares solution that minimizes the sum of squared residuals ( `\(SS_{res}\)`; `\(\sum{{{(Y-{\hat Y})}^{2}}=\sum{{{e}^{2}}}}\)` )

  - Intercept: `\(a=\bar{Y}-b\bar{X}\)`

  - Slope: `\(b=\frac{\sum{xy}}{\sum{{{x}^{2}}}}=\frac{\sum{(X-\bar{X})(Y-\bar{Y})}}{\sum{{{(X-\bar{X})}^{2}}}}\)`


Regression with 2 Independent Variables:  `\({\hat Y}=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}\)`

Now, `\({{b}_{1}}\ \text{and}\ {{b}_{2}}\)` are considered partial regression coefficients associated with variables X1 and X2, respectively.  X1 and X2 are used together in predicting Y.  `\({{b}_{1}}\)` is the partial regression coefficient for Y on X1 with X2 in the regression equation/model.  `\({{b}_{2}}\)` is the partial regression coefficient for Y on X2 with X1 in the regression equation/model.  Still trying to minimize the difference between observed Y scores and predicted Y scores (i.e., `\(\sum{(Y-{\hat Y})}^{2}\)` ; Least Squares solution) when solving for the intercept (*a*) and the partial regression coefficients ( `\({{b}_{1}}\)` and `\({{b}_{2}}\)` ).

---

## Multiple Regression Analysis

Using our professors' salary example, we want to estimate/predict Salary from Time since Ph.D. was earned (X1) and Number of citations (X2).  Regress Salary on Time since Ph.D. and Number of citations.  

To compute the partial regression coefficients, `\({{b}_{1}}\ \text{and}\ {{b}_{2}}\)`, let's first calculate the partial *standardized* regression coefficients or beta weights ( `\(\beta_1\)` and `\(\beta_2\)` ):

`\(b_1 = \beta_1\frac{{s_Y}}{{s_{X1}}}\)` ;   		`\(b_2={\beta_2}\frac{{s_Y}}{{s_{X2}}}\)`

In simple linear regression, the standardized regression coefficient (Beta) is the correlation coefficient between X and Y:

`$$\beta =b\frac{{{s}_{x}}}{{{s}_{y}}}=\frac{\sum{xy}\sqrt{\sum{{{x}^{2}}}}\sqrt{n-1}}{\sum{{{x}^{2}}}\sqrt{n-1}\sqrt{\sum{{{y}^{2}}}}}=\frac{\sum{xy}}{\sqrt{\sum{{{x}^{2}}}\sqrt{\sum{{{y}^{2}}}}}}={{r}_{xy}}$$`
---

## Multiple Regression Analysis

  - In Simple Linear Regression, the standardized regression coefficient (Beta) has fixed upper lower and upper limits a correlation coefficient (i.e., ranges from -1 to +1).

  - In Multiple Regression, the partial standardized regression coefficients (Beta weights) no longer have this fixed lower and upper limit.  Generally speaking, however, the partial standardized regression coefficient should not exceed the absolute value of 1.  If it does, this may be an indication of a problem among the independent variables (to be discussed later).  They are called ***partial standardized regression coefficients*** because they are the partial slopes of an equation using standardized scores (i.e., z scores):  `\(z_{\hat y}={{\beta }_{1}}{{z}_{1}}+{{\beta }_{2}}{{z}_{2}}\)`, where `\({{\beta }_{1}}\ and\ {{\beta }_{2}}\)` are standardized regression coefficients; `\({{z}_{1}}\ and\ {{z}_{2}}\)` are standard scores on X1 and X2, respectively.  

  - Unstandardized regression coefficients indicate the direction and by how many units Y will change with a 1 unit increase in X.  They are expressed in their original scale of measurement of the independent variable (e.g., number of years).  Recall that the mean of z scores is zero and the standard deviation is 1.  Because the standard deviation of z scores is 1, a unit change in `\(z_x\)` indicates a change of one standard deviation in X.  Thus, standardized regression coefficients indicate the direction and by how many standard deviations Y will change with 1 standard deviation increase in X. 

---

## Multiple Regression Analysis

The relationships between standardized regression coefficients and correlations with two independent variables are below:

`$${{\beta }_{1}}=\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}$$`

`$${{\beta }_{2}}=\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{1-r_{12}^{2}}$$`

---

## Multiple Regression Analysis

When we interpret the partial regression coefficient for one independent variable, we should state that we are *holding the other independent variable constant*. 

Using the Professor's salary example, we would like to regress Salary (*Y*) on Time (*X1*) and Citations (*X2*). We get

`$${{\beta }_{1}}=\frac{{{r}_{Y1}}-{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}=\frac{0.608-(0.55)(0.373)}{1-{{(0.373)}^{2}}}=0.468$$`
`$${{\beta }_{2}}=\frac{{{r}_{Y2}}-{{r}_{Y1}}{{r}_{12}}}{1-r_{12}^{2}}=\frac{0.55-(0.608)(0.373)}{1-{{(0.373)}^{2}}}=0.375$$`
With every 1 standard deviation increase in Time, predicted Salary increases by about .47 standard deviations *while holding number of citations constant*. With every 1 standard deviation increase in number of citations, predicted Salary increases by about .38 standard deviations *while holding time since Ph.D. was earned constant*.



---

## Multiple Regression Analysis

Why do we state that we are holding the other independent variable constant?

  - In multiple regression, there is redundancy or shared variance between X1 and X2 which must now be taken into account.  For example, if two predictors, X1 and X2, are positively correlated and both affect Y (are correlated with the criterion/dependent variable), then the change in Y that is associated with an increase in X1 is partly caused by the increase in X1 and partly caused by the corresponding increase in X2.  The regression coefficient or effect of a given predictor variable should indicate the change in Y produced by a unit increase in that predictor variable alone, that is, when the other predictor variable does not change.  Thus, when computing multiple regression statistics for a given predictor, we must hold the effects of the other predictor variable statistically constant to determine the extent to which changes in Y result from a change in a given predictor alone.  This basically means that multiple regression statistics, such as the partial regression coefficients (standardized or unstandardized), remove the variance from X1 that it shares with X2 in the multiple regression model.  


---

## Multiple Regression Analysis

Once we have calculated the partial standardized regression coefficients, convert them back to original scale (time in years; number of citations) to get the unstandardized regression coefficients and intercept.

`$${{b}_{1}}={{\beta }_{1}}\frac{{{s}_{Y}}}{{{s}_{X1}}}$$`
`$${{b}_{2}}={{\beta }_{2}}\frac{{{s}_{Y}}}{{{s}_{X2}}}$$`
`$$a=\bar{Y}-{{b}_{1}}{{\bar{X}}_{1}}-{{b}_{2}}{{\bar{X}}_{2}}$$`
---

## Multiple Regression Analysis - Multiple R

  - Multiple R is the correlation coefficient between the observed and predicted Y values ( `\({r_{Y{\hat Y}}}\)`; ranges from 0 to 1).  The Squared multiple correlation, or R Square (${{R}^{2}}$), is interpreted as the proportion of variance in the criterion/dependent variable Y that can be explained by the predictor variables X1 and X2.  The *adjusted R Square* is an adjustment to better reflect the fit of the model in the population.    

`$$R=\sqrt{\frac{r_{Y1}^{2}+r_{Y2}^{2}-2{{r}_{Y1}}{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}}$$`		

`$$R=\sqrt{{{\beta }_{1}}{{r}_{Y1}}+{{\beta }_{2}}{{r}_{Y2}}}$$`
---

## Multiple Regression Analysis - Multiple R

  - Multiple R can never be smaller than the absolute value of the largest correlation between a predictor variable and the dependent variable.

`\({{R}^{2}}=\frac{S{{S}_{reg}}}{\sum{{{y}^{2}}}}=\frac{S{{S}_{reg}}}{S{{S}_{total}}}\)` , where `\(S{S_{reg}} = \sum {{{({\hat Y} - \bar Y)}^2}}\)` and `\(S{S_{total}} = \sum {{{(Y - \bar Y)}^2}}\)`.

`\({{R}^{2}}=\frac{r_{Y1}^{2}+r_{Y2}^{2}-2{{r}_{Y1}}{{r}_{Y2}}{{r}_{12}}}{1-r_{12}^{2}}\)`	

`\({{R}^{2}}={{\beta }_{1}}{{r}_{Y1}}+{{\beta }_{2}}{{r}_{Y2}}\)`		


  - R Square in multiple regression is also referred to as the Coefficient of Multiple Determination (Indicating the amount of variance in the dependent variable explained by the independent variables).  
  
  - Coefficient of Multiple Alienation:  `\(1-{{R}^{2}}\)`, indicating the amount of variance in the dependent variable not explained by the independent variables.


`\(R_{adj}^{2}={{R}^{2}}-\frac{(1-{{R}^{2}})k}{n-k-1}\)` where *n* = sample size and *k* = the number of independent variables included in the model.  


---

## Multiple Regression Analysis - F Test in Multiple Regression

Just as in Simple Linear Regression, we can calculate an F ratio to test the regression of Y on both X1 and X2.   `\(F=\frac{M{{S}_{reg}}}{M{{S}_{res}}}\)` with `\(d{{f}_{reg}}\)` in the numerator and `\(d{{f}_{res}}\)` in the denominator (*df* for the Regression Sum of Squares =  *k*; *df* for the Residual Sum of Squares = *n – k – 1*).  If the *F* ratio exceeds the critical value with ( `\(d{{f}_{reg}}\)`, `\(d{{f}_{res}}\)` ) at alpha = .05, we reject the null that the regression of Y on both X1 and X2 is equal to zero.  That is, we are testing whether R Square is significantly different from zero:

`\(F=\frac{{{R}^{2}}(n-k-1)}{(1-{{R}^{2}})k}\tilde{\ }df=k\ \text{and}\ n-k-1\)`


The null hypothesis of  the F test is that all partial regression coefficients are equal to zero.


---

## Multiple Regression Analysis - Testing the Partial Regression Coefficients

Just as in Simple Linear Regression, you can test the significance of a partial regression coefficient (slope) *b* to see if it is significantly different from zero.

  - **Recall the Variance of Estimate:**  The variance of estimate indicates the variance of the scores about the regression line.  It is the variance of the residuals.

`\(s_{y.x}^{2}=\frac{\sum{{{(Y-{\hat Y})}^{2}}}}{n-k-1}=\frac{S{{S}_{res}}}{n-k-1}\)`	

Recall:  	`\(M{{S}_{res}}=\frac{\sum{{{(Y-{\hat Y})}^{2}}}}{n-k-1}=\frac{S{{S}_{res}}}{d{{f}_{res}}}\)` 


  - **Recall the Standard Error of Estimate**:  The standard error of estimate is the square root of the variance of estimate or the standard deviation of the residuals.

`$${{s}_{y.x}}=\sqrt{\frac{\sum{{{(Y-{\hat Y})}^{2}}}}{n-k-1}}=\sqrt{\frac{S{{S}_{res}}}{n-k-1}}$$`
---

## Multiple Regression Analysis - Testing the Partial Regression Coefficients

  - When testing whether the partial slope or partial regression coefficient is significantly different from zero, the standard error associated with the slope must be calculated.  This differs slightly from the formula used in Simple Linear Regression due to the shared variance between the independent variables that must be taken into account:
  
`\({{s}_{b1}}=\sqrt{\frac{s_{y.x}^{2}}{\sum{x_{1}^{2}(1-r_{12}^{2})}}}=\frac{{{s}_{y.x}}}{\sqrt{\sum{x_{1}^{2}(1-r_{12}^{2})}}}\)`  where `\(s_{b1}\)` is the standard error associated with *b1*,  `\(s_{y.x}^{2}\)` is the variance of estimate, `\(s_{y.x}\)` is the standard error of estimate, and `\(\sum\limits_{{}}{x_{1}^{2}}\)` is the sum of squares for the independent variable X1.

  `\({{s}_{b2}}=\sqrt{\frac{s_{y.x}^{2}}{\sum{x_{2}^{2}(1-r_{12}^{2})}}}=\frac{{{s}_{y.x}}}{\sqrt{\sum{x_{2}^{2}(1-r_{12}^{2})}}}\)`  where `\(s_{b2}\)` is the standard error associated with *b2*, `\(s_{y.x}^{2}\)` is the variance of estimate, `\(s_{y.x}\)` is the standard error of estimate, and `\(\sum\limits_{{}}{x_{2}^{2}}\)` is the sum of squares for the independent variable X2.
  
  

---

## Multiple Regression Analysis - Testing the Partial Regression Coefficients

 `\(s_b\)` is the standard error (standard deviation of the sampling distribution of *b*) and is used when testing the significance of the *b* using the t ratio:  `\(t=\frac{b}{{{s}_{b}}}=\frac{b-0}{{{s}_{b}}}\)`  with *df* of n – k – 1.

`$${{t}_{b1}}=\frac{{{b}_{1}}}{{{s}_{b1}}}$$`  

`$${{t}_{b2}}=\frac{{{b}_{2}}}{{{s}_{b2}}}$$`

With knowledge of the standard error of the partial *bs*, we can create a confidence interval around each of the partial regression coefficients:  `\(b\pm {{t}_{(\alpha /2,df)}}{{s}_{b}}\)`.  


---

## Multiple Regression Analysis - A Complete Example


```r
mydata &lt;- import("data/profs.sav")
# View(mydata)
mymodel &lt;- lm(salary ~ time + cits, data = mydata)
summary(mymodel)
confint(mymodel, level = 0.95)
lm.beta(mymodel)
mydata$y.pred &lt;- fitted(mymodel)
mydata$y.res &lt;- resid(mymodel)
head(mydata)
new.obs &lt;- data.frame(
  time = 12,
  cits = 80
)
predict(mymodel, newdata = new.obs, interval = "confidence") # Prediction interval for the mean
predict(mymodel, newdata = new.obs, interval = "prediction") # Prediction interval for the individual
```

---

## Multiple Regression Analysis - A Complete Example

**Sum up**: Multiple regression analysis was used to examine how well time since Ph.D. was earned and the number of citations for professors would explain/predict their salary.  Time since Ph.D. and number of citations together accounted for approximately 47% of the variance in Salary `\((R_{adj}^{2}=.474)\)`, *F*(2, 59) = 28.43, *p* &lt; .001.  Time since professor’s earned their Ph.D. was a statistically significant predictor of Salary, *t*(59) = 4.67, *p* &lt; .001.  Holding number of citations constant, as time since professors’ earned their Ph.D. increased by 1 year, salary was estimated to increase by  `$`1062 (95%: 607,19, 1516,34). Number of citations for professors was also a statistically significant predictor of salary, *t*(59) = 3.75, *p* &lt; .001. Holding time since Ph.D. was earned constant, as teh number of citations for professors increased by 1, salary was estimated to increase by `$`212 (95% CI: 98.87, 325.36).  

For professors who earned their Ph.D. 12 years ago and who had 80 citations, it is predicted that their salary would be about `$`68783 (95% CI: `$`64185, `$`73382). For any professor who earned their Ph.D. 12 years ago and who had 80 citations, the predicted salary is about `$`68783 (95% CI: `$`53960, `$`83607).


---

### Useful Functions and Symbols Commonly Used in R When Fitting Linear Models
&lt;table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;Useful Functions in R When Fitting Linear Models&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Function &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Action &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; summary() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Displays detailed results for the fitted model &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; coefficients() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Lists the model parameters (intercept and slopes) for the fitted model &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; confint() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Provides confidence intervals for teh model parameters (95% by default) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; fitted() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Lists the predicted values in a fitted model &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; residuals() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Lists the residual values in a fitted model &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; anova() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Generates an ANOVA table for a fitted model, or an ANOVA table comparing two or more fitted models &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; vcov() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Lists the covariance matrix for model parameters &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; AIC() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Prints Akaike's Information Criterion &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; plot() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Generates diagnostic plots for evaluating the fit of a model &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; predict() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Uses a fitted model to predict response values for a new dataset &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


---

### Useful Functions and Symbols Commonly Used in R When Fitting Linear Models

&lt;table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;Symbols Commonly Use in R Formulas&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Symbol &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Usage &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; ~ &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Separates response variables on the left from the explanatory varialbes on the right. For example, a prediction of y from x, z, and w would be coded y ~ x + z + w. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Separates predictor variables. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; : &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Denotes an interaction between predictor variables. A prediction of y from x, z, and the interaction between x and z would be coded y ~ x + z + x:z. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; * &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; A shortcut for denoting all possibel interactions. The code y ~ x*z*w expands to y ~ x + z + w + x:z + x:w + z:w + x:z:w. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; ^ &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Denotes interactions up to a specified degree. The code y ~ (x + z + w)^2 expands to y ~ x + z + w + x:z + x:w + z:w. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; . &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; A placeholder for all other variables in the data frame except the dependent variable. For example, if a data frame contained the varibels x, y, z, and w, then the code y ~ . would expands to y ~ x + z + w. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; - &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; A minus sign removes a variable from teh equation. For example, y ~ (x + z + w)^2 - x:w expands to y ~ x + z + w + x:z + z:w. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; -1 &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Supresses the intercept. For example, the formula y ~ x -1 fits a regression of y on x, and forces the line through the origin at x=0. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; I() &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Elements within the parentheses are interpreted arithmetically. For example, y ~ x + (z + w)^2 would expand to y ~ x + z + w + z:w. In contrast, y ~ x + I(z + w)^2 would expand to y ~ x + h, where h is a new variable created by squaring the sum of z and w. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; function &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Mathematical functions cna be used in formulas. For example, log(y) ~ x + z + w would predict log(y) from x, z, and w &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
## An Open Book on Regression

[Handbook of Regression Modeling in People Analytics: With Examples in R and Python](http://peopleanalytics-regression-book.org/index.html) 

---

### Andy Field's "The General Linear Model" lecture on YouTube

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/7cSArk7tU4w" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
