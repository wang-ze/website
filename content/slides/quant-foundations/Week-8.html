<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>ESC_PS 8850:  Quantitative Foundations in Educational Research</title>
    <meta charset="utf-8" />
    <meta name="author" content="Ze Wang" />
    <meta name="date" content="2021-10-18" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/jquery/jquery.min.js"></script>
    <link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding/datatables.js"></script>
    <link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# ESC_PS 8850: <br>Quantitative Foundations in Educational Research
## <br/>Monday, 4:00pm-6:30pm; online
### Ze Wang
### University of Missouri
### 2021-10-18

---

layout: true

&lt;div class = 'my-header'&gt;&lt;span&gt;ESC_PS 8850 Quant Foundations&lt;/span&gt;&lt;/div&gt;

&lt;div class = 'my-footer'&gt;&lt;span&gt;wangze@missouri.edu
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
Ze Wang, Ph.D.&lt;/span&gt;&lt;/div&gt; 


---
class: center, middle







# .blue[Week 8]

## .blue[Multiple Regression Analysis]

### Interactions in Multiple Regression
### Categorical Predictors

---

```r
library(rio); library(ggplot2); library(QuantPsyc); library(psych); library(car); library(interactions); library(memisc)
```


---

### Interactions in Multiple Regression


**Interactions Among Continuous Predictors**: Interactions represent an interplay among predictors that produces an effect on the outcome of Y that is different from the sum of the effects of the individual predictors.

- Consider how ability and motivation impact achievement in graduate school. The combined impact of ability and motivation on achievement equals the sum of their separate parts. Additive effects are indicated by separate independent variables in the regression equation: `\({\hat Y}=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}\)` . Additive means that the regression of the criterion on one predictor is constant over all values of the other predictor. The whole equals the sum of their separate parts.


--
- What if the impact of ability and motivation as a whole is greater than the sum of their separate parts? When two predictors interact, the regression of Y on one of the predictors *depends on* or is *conditional on* the value of the other predictor.


---

### Interactions in Multiple Regression

- One alternative is that ability and motivation may interact such that graduate students with both high ability and high motivation achieve much more in graduate school than would be expected from the simple sum of the separate effects of ability and motivation.

--

- Another alternative is that ability and motivation compensate for one another in that students with high ability have less motivation to achieve and students with low ability have more motivation to achieve.

--

- We want to consider the effects of ability, motivation, and the interaction between ability and motivation on achievement. The interaction between `\({{X}_{1}}\)` and `\({{X}_{2}}\)` can be tested in multiple regression to determine the amount of variance in the criterion variable it accounts for over and above any additive combination of their separate effects: `\({\hat Y}=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+{{b}_{3}}{{X}_{1}}{{X}_{2}}\)`. The interaction term is simply the product of `\({{X}_{1}}\)` and `\({{X}_{2}}\)`. a is the intercept (the predicted value of `\(Y\)` when `\({{X}_{1}}\)` and `\({{X}_{2}}\)` are zero); `\({{b}_{1}}\)` represent the unit change in `\(Y\)` with a 1-unit increase in `\({{X}_{1}}\)`, holding other variable constant; `\({{b}_{2}}\)` represents the unit change in `\(Y\)` with a 1-unit increase in `\({{X}_{2}}\)`,  holding other variables constant; `\({{b}_{3}}\)` represents the interaction between `\({{X}_{1}}\)` and `\({{X}_{2}}\)`, holding other variables constant.


---

### Interactions in Multiple Regression

#### To Create an Interaction Term

  1. Center the predictor variables (always center the continuous predictor variables when an interaction term is to be included in the regression equation, unless a predictor variable has a meaningful zero point). Centering simply means subtracting the mean value of a predictor variable from each value of that predictor variable ( `\({{X}_{1}}-{{\bar{X}}_{1}}\)`). .red[Do not center the dependent variable.]
  
  2. Multiply the centered variables `\({{X}_{1}}\)` and `\({{X}_{2}}\)`, creating an interaction term for `\({{X}_{1}}\)` and `\({{X}_{2}}\)` to be included in the multiple regression equation.

--

- Centering variables allows a meaningful interpretation of regression coefficents. When you center variables, those who are at the mean of a predictor variable get a value of zero on the predictor variable. For simple effects (e.g., `\({{X}_{1}}\)` alone), you interpret partial regression coefficients as the change in `\(Y\)` with a 1-unit increase in the predictor variable for people at the mean of the `\({{X}_{2}}\)` predictor variable: `\({Y}'=a+{{b}_{1}}{{X}_{1}}+{{b}_{2}}{{X}_{2}}+{{b}_{3}}{{X}_{1}}{{X}_{2}}\)`.

  - Centering does not change the overall fitting of the model. Therefore, the *F*, and `\(R^2\)` values stay the same.



---

### Interactions in Multiple Regression

**Example**: Regress Physical Endurance (minutes on treadmill test) on Age ( `\(X_1\)`) and Number of Years of Vigorous Physical Exercise ( `\(X_2\)`). 



```r
endurance &lt;- import("data/endurance.sav")
# str(endurance)
attach(endurance)
*newage &lt;- scale(age, scale=FALSE)
newyears &lt;- scale(years, scale=FALSE)
detach(endurance)

*mymodel1 &lt;- lm(endurance ~ newage + newyears + newage:newyears, data = endurance)
mymodel2 &lt;- lm(endurance ~ newage*newyears, data = endurance) # alternative formula
# mymodel2 &lt;- lm(endurance ~ I(age-49.18)*I(years-10.67), data = endurance) # use the `I()` symbol
summary(mymodel1)
```


---

### Interactions in Multiple Regression

**Example**: Regress Physical Endurance (minutes on treadmill test) on Age ( `\(X_1\)`) and Number of Years of Vigorous Physical Exercise ( `\(X_2\)`). 



```
## 
## Call:
## lm(formula = endurance ~ newage + newyears + newage:newyears, 
##     data = endurance)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -21.165  -6.939   0.269   6.300  21.299 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     25.88872    0.64662  40.037  &lt; 2e-16 ***
## newage          -0.26169    0.06406  -4.085 6.01e-05 ***
## newyears         0.97272    0.13653   7.124 1.20e-11 ***
## newage:newyears  0.04724    0.01359   3.476 0.000604 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 9.7 on 241 degrees of freedom
## Multiple R-squared:  0.2061,	Adjusted R-squared:  0.1962 
## F-statistic: 20.86 on 3 and 241 DF,  p-value: 4.764e-12
```


---

### Interactions in Multiple Regression

**Example**: Regress Physical Endurance (minutes on treadmill test) on Age ( `\(X_1\)`) and Number of Years of Vigorous Physical Exercise ( `\(X_2\)`). 

- The overall regression of Endurance on Age, Years, and Age*Years:  

`\({\hat Y}=25.89-.262newage+.973newyears+.047newage*newyears\)`. 

All of the partial regression coefficients are significantly different from zero, controlling for the other variables in the model. For every 1 year increase in age, there is an estimated loss of endurance of .26 minutes for people at the mean level of years of exercise in the sample. For every 1 year increase in years of vigorous exercise, there is an estimated increase in endurance of .97 minutes for people at the mean level of age in the sample. The interaction indicates that the decline in endurance with age depends on a history of rigorous exercise.


---

### Interactions in Multiple Regression

**To Interpret Interactions**:  Write the simple regression equation for the regression of the criterion on one predictor at a specific value of the other predictor variable, then plot these values.

For example, write the simple regression equations for the regression of Physical Endurance on Age at three levels of Years of Rigorous Exercise. The three levels are typically 1) at the mean of the other predictor (when centered Years = 0); 2) at 1 standard deviation above the mean (when centered Years = 4.78); and 3) at 1 standard deviation below the mean (when centered Years = -4.78). 


```r
psych::describe(endurance)
```

```
##           vars   n   mean    sd median trimmed   mad min max range skew
## case         1 245 124.99 72.48    124  124.86 91.92   1 250   249 0.02
## age          2 245  49.18 10.11     48   49.11 10.38  20  82    62 0.15
## years        3 245  10.67  4.78     11   10.56  4.45   0  26    26 0.27
## endurance    4 245  26.53 10.82     27   26.39 10.38   0  55    55 0.11
##           kurtosis   se
## case         -1.21 4.63
## age          -0.08 0.65
## years         0.23 0.31
## endurance    -0.30 0.69
```

---

### Interactions in Multiple Regression

**To Interpret Interactions**:  Write the simple regression equation for the regression of the criterion on one predictor at a specific value of the other predictor variable, then plot these values.


- Multiple Regression Equation: `\({\hat Y}=25.89-.262newage+.973newyears+.047newage*newyears\)`

- Rearrange the equation: `\({\hat Y}=25.89+.973newyears+(-.262+.047newyears)*newage\)`

- For Low Years ( `\({{X}_{2}} = {newyear} = -4.78\)`):

  `\({\hat Y}=25.89+.973(-4.78)+(-.262+.047(-4.78))*newage\)`
  `\({\hat Y}=25.89-4.65-.487*newage =21.24-.487*newage\)`

- For Mean Years ( `\({{X}_{2}} = {newyear} = 0)\)`):  
  `\({\hat Y}=25.89+.973(0)+(-.262+.047(0))*newage = 25.89-.262*newage\)`


- For High Years ( `\({{X}_{2}} = {newyear} = 4.78\)`):  
  `\({\hat Y}=25.89+.973(4.78)+(-.262+.047(4.78))*newage\)`
  `\({\hat Y}=25.89+4.65-.037*newage = 30.54-.037*newage\)`

---

### Interactions in Multiple Regression

**To Interpret Interactions**:  Write the simple regression equation for the regression of the criterion on one predictor at a specific value of the other predictor variable, then plot these values.

.center[
Low Years Exercise: `\({\hat Y}=21.24-.487*newage\)`

Mean Years Exercise: `\({\hat Y}=25.89-.262*newage\)`

High Years Exercise: `\({\hat Y}=30.54-.037*newage\)`

]

- Solve for the predicted Y values at .red[low] years exercise for each age level (low age = -10.11; mean age = 0; high age = 10.11). Notice that the low to high levels represent 1 standard deviation below the mean (-10.11), the mean (0), and 1 standard deviation above the mean (10.11):

.center[
Low Years/Low Age:  `\({\hat Y} =21.24-.487(-10.11)=26.16\)`

Low Years/Mean Age: `\({\hat Y}=21.24-.487(0)=21.24\)`

Low Years/High Age: `\({\hat Y}=21.24-.487(10.11)=16.32\)`
]

---

### Interactions in Multiple Regression

- Solve for the predicted Y values at .red[mean] level of years exercise for each age level:

.center[
Mean Years/Low Age: `\({\hat Y}=25.89-.262(-10.11)=28.54\)`

Mean Years/Mean Age: `\({\hat Y}=25.89-.262(0)=25.89\)`

Mean Years/High Age: `\({\hat Y}=25.89-.262(10.11)=23.24\)`
]

- Solve for the predicted Y values at .red[high] level of years exercise for each age level:

.center[
High Years/Low Age: `\({\hat Y}=30.54-.037(-10.11)=30.91\)`

High Years/Mean Age: `\({\hat Y}=30.54-.037(0)=30.54\)`

High Years/High Age: `\({\hat Y}=30.54-.037(10.11)=30.17\)`
]

---

### Interactions in Multiple Regression

.pull-left[
&lt;table class=" lightable-classic" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto;'&gt;
 &lt;thead&gt;
&lt;tr&gt;
&lt;th style="empty-cells: hide;" colspan="2"&gt;&lt;/th&gt;
&lt;th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3"&gt;&lt;div style="border-bottom: 1px solid #111111; margin-bottom: -1px; "&gt;Age&lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;  &lt;/th&gt;
   &lt;th style="text-align:left;"&gt;  &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Low &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Mean &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; High &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Low &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 26.16 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 21.24 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 16.32 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Excercise &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Mean &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 28.54 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 25.89 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 23.24 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; High &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 30.91 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 30.54 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 30.17 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

This indicates that the decline in physical endurance across the life span is less dramatic for those with high number of years of rigorous exercise, whereas there appears to be a dramatic decline in physical endurance across the life span for those with low or average number of years of rigorous exercise.
]

.pull-right[
![](Week-8_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]


---

### Interactions in Multiple Regression

Does including the interaction improve prediction?


```r
endurance &lt;- import("data/endurance.sav")
# str(endurance)
attach(endurance)
newage &lt;- scale(age, scale=FALSE)  
newyears &lt;- scale(years, scale=FALSE)
detach(endurance)

mymodel1 &lt;- lm(endurance ~ newage + newyears + newage:newyears, data = endurance) 
mymodel3 &lt;- lm(endurance ~ newage + newyears, data = endurance) 
*anova(mymodel3, mymodel1)
## Analysis of Variance Table
## 
## Model 1: endurance ~ newage + newyears
## Model 2: endurance ~ newage + newyears + newage:newyears
##   Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    
## 1    242 23810                                 
## 2    241 22674  1    1136.5 12.08 0.0006042 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
                          # Using Type I Sum of Squares
```


---

Use the `interact_plot` function in the `interactions` package. 

- By default, all continuous variables .red[not involved in the interaction] are mean-centered. The `centered` argument can be used to overwrite the default. However, the response variable, pred, modx, and mod2 variables are never centered.


```r
# install.packages("interactions")
# library(interactions)
endurance &lt;- import("data/endurance.sav")
*mymodel &lt;- lm(endurance ~ age*years, data = endurance)
*interact_plot(mymodel, pred = age, modx = years, plot.points = TRUE, point.shape=TRUE)
```

![](Week-8_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;


---

### Categorical Independent Variables

- Examples of nominal or categorical variables include:  religion, treatment groups in experiments, region of country, ethnic group, occupation, diagnosis, or marital status.  

- One Independent Variable with Two Categories:  Regression of Salary on Gender (Females coded as 1 and males coded as 0).


```r
profs &lt;- import("data/profs.sav")
```

<div id="htmlwidget-294bb8419a24847b274d" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-294bb8419a24847b274d">{"x":{"filter":"none","fillContainer":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62"],[4,6,7,8,10,14,15,16,18,22,23,24,25,27,30,31,32,34,35,36,37,38,42,43,44,45,48,51,55,56,57,62,1,2,3,5,9,11,12,13,17,19,20,21,26,28,29,33,39,40,41,46,47,49,50,52,53,54,58,59,60,61],[2,1,2,1,2,1,1,2,2,2,1,1,1,2,1,2,1,1,1,1,1,2,1,2,1,2,2,1,1,2,1,1,2,1,2,2,1,1,2,1,1,2,1,1,2,2,2,2,2,2,1,2,2,2,2,2,1,1,1,2,2,1],[8,6,16,10,5,11,18,6,7,5,7,13,5,8,13,5,3,3,9,3,9,3,11,5,1,21,16,5,4,4,5,4,3,6,3,9,2,5,6,7,9,7,3,7,8,7,2,1,4,10,1,7,5,5,4,11,16,3,6,4,8,3],[17,6,38,48,22,27,37,8,6,7,6,69,11,20,27,14,23,7,19,11,31,9,12,9,6,39,50,5,19,11,13,4,18,3,2,11,9,30,21,10,13,12,29,29,9,41,3,1,12,32,26,16,12,18,16,20,50,6,3,8,11,25],[34,37,48,56,29,40,61,32,69,35,18,90,60,27,56,50,25,1,69,69,27,50,54,47,29,69,55,42,83,49,14,28,50,26,50,41,19,28,31,25,36,47,29,35,30,35,14,35,32,33,45,47,43,33,28,24,31,27,36,34,70,27],[61863,47034,66432,61100,47454,59677,61458,54528,56600,62895,53740,75822,56596,62091,74199,50729,70011,39652,68987,55579,54671,57704,60009,58632,38340,71219,83503,53650,74343,57710,52676,58582,51876,54511,53425,52926,41934,49832,47047,39115,60327,52542,50455,51647,55682,42162,52646,37939,44045,51122,47082,53712,54782,47212,52840,50931,66784,49751,41195,45662,47606,44301],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>case<\/th>\n      <th>dept<\/th>\n      <th>time<\/th>\n      <th>pubs<\/th>\n      <th>cits<\/th>\n      <th>salary<\/th>\n      <th>gender<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"columnDefs":[{"className":"dt-right","targets":[1,2,3,4,5,6,7]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

---

### Categorical Independent Variables



```r
cor.test(profs$gender, profs$salary) # point-biserial correlation
```

```
## 
## 	Pearson's product-moment correlation
## 
## data:  profs$gender and profs$salary
## t = -4.6439, df = 60, p-value = 1.912e-05
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.6770144 -0.3033909
## sample estimates:
##        cor 
## -0.5141948
```

---

### Categorical Independent Variables



```r
mymodel &lt;- lm(salary ~ as.factor(gender), data = profs) #&lt;&lt; 0 for male, 1 for female on gender
summary(mymodel)
```

```
## 
## Call:
## lm(formula = salary ~ as.factor(gender), data = profs)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -21268.9  -5045.2     98.5   3270.3  23894.1 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)           59609       1484  40.173  &lt; 2e-16 ***
## as.factor(gender)1    -9906       2133  -4.644 1.91e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 8394 on 60 degrees of freedom
## Multiple R-squared:  0.2644,	Adjusted R-squared:  0.2521 
## F-statistic: 21.57 on 1 and 60 DF,  p-value: 1.912e-05
```


---

### Categorical Independent Variables

- The regression coefficient associated with gender is statistically significantly different from zero (Not surprising because we already know the correlation coefficient is significantly different from zero).  Because gender in this exampel only has two categories, the regression coefficient represents the difference in mean salaries for males and females.  Thus, the t-test in this situation tests whether the mean salary for males is significantly different from the mean salary for females.  Because females were coded as 1 and males coded as 0 (dummy coding), and the regression coefficient is negative, we know that females have significantly lower salaries than males.  If the regression coefficient was positive, it would indicate that females have significantly higher salaries than males.  

- When a group is coded as zero, it is the reference/comparison group.  The males are the reference group in this situation.  Notice the value of the constant (Y-intercept).  Because males are the reference group, the constant is the mean salary for males.

Regression Equation: `\(\hat Y = a + b(gender)\)`.  What does a represent? What does b represent?

Regression Equation: `\({\hat Y} = 59608.94 - 9905.90*gender\)`

For males (gender = 0): `\({\hat Y} = 59608.94 - 9905.90*(0) = 59608.94\)`

For females (gender = 1): `\({\hat Y} = 59608.94 - 9905.90*(1) = 49703.04\)`


---

### Categorical Independent Variables

**To sum this up**: The Point-Biserial correlation was computed to examine the relationship between gender and salary level, which indicated that a moderate and significant relationship exists, `\(r_{pb}(60) = -.514\)` , *p* &lt; .001.  As indicated by the correlational analysis, gender was a significant predictor of professors’ salary, `\(t(60) = -4.64\)`, *p* &lt; .001, accounting for approximately 25% of the variance in salary ( `\(R_{adj}^2 = .252\)`).  Female professors make significantly less money than male professors.  On average, females make about `$`9906 less than males (95% CI:  -14173, -5639, `\(\beta = -.514\)`). 


---

### Categorical Independent Variables

- Alternatively, the regression model can be specified in R as below. .


```r
*mymodel2 &lt;- lm(salary ~ gender, data = profs)
summary(mymodel2)
```

```
## 
## Call:
## lm(formula = salary ~ gender, data = profs)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -21268.9  -5045.2     98.5   3270.3  23894.1 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    59609       1484  40.173  &lt; 2e-16 ***
## gender         -9906       2133  -4.644 1.91e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 8394 on 60 degrees of freedom
## Multiple R-squared:  0.2644,	Adjusted R-squared:  0.2521 
## F-statistic: 21.57 on 1 and 60 DF,  p-value: 1.912e-05
```



---

### Categorical Independent Variables

.red[What if a variable has more than two categories?]

- When a variable has more than two categories, it must be coded to provide meaningful results in the regression analysis.  To code a variable with more than two categories or groups, *g* – 1 variables must be created and used in the regression equation (where *g* = the number of categories or groups).  

- For example, if we were interested in the effects of different religious backgrounds (Catholic, Protestant, Jewish, Other) on attitudes toward abortion, we would need to create 3 code variables (*g* – 1 = 4 – 1 = 3).  



---

### Categorical Independent Variables

**Example:** Effects of religion on attitude toward abortion


```r
religion &lt;- import("data/religion.sav")
```

<div id="htmlwidget-9ae23a10b79fe2ae3dad" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-9ae23a10b79fe2ae3dad">{"x":{"filter":"none","fillContainer":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36"],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36],["C","O","P","C","C","O","P","C","O","O","C","O","P","C","P","C","C","J","O","C","P","J","P","P","J","O","P","O","J","P","P","P","J","P","J","P"],[61,78,47,65,45,106,120,49,45,62,79,54,140,52,88,70,56,124,98,69,56,135,64,130,74,58,116,60,84,68,90,112,94,80,110,102]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>case<\/th>\n      <th>religion<\/th>\n      <th>ata<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":8,"columnDefs":[{"className":"dt-right","targets":[1,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[8,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

---

### Categorical Independent Variables

```r
table(religion$religion)
```

```
## 
##  C  J  O  P 
##  9  6  8 13
```

---

### Categorical Independent Variables

#### Dummy Coding

One group is designated as the reference group and is assigned a value of 0 for every code variable.  The reference group should serve as a useful comparison (e.g., a control group; the group expected to score the highest or lowest on Y; a standard treatment).  The reference group should be well defined and not a “catch all” category (e.g., Other).  The reference group should not have a very small sample size compared to the other groups.  

&lt;table class=" lightable-classic" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto;border-bottom: 0;'&gt;
&lt;caption&gt;Protestant as Reference Group:&lt;/caption&gt;
 &lt;thead&gt;
&lt;tr&gt;
&lt;th style="empty-cells: hide;" colspan="1"&gt;&lt;/th&gt;
&lt;th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3"&gt;&lt;div style="border-bottom: 1px solid #111111; margin-bottom: -1px; "&gt;Code Variable&lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Religion &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Catholic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Jewish &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Other &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Protestant &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Catholic &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Jewish &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Other &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;&lt;tr&gt;&lt;td style="padding: 0; " colspan="100%"&gt;
&lt;sup&gt;&lt;/sup&gt; Catholic represents Catholic versus non-Catholic; Jewish represnts Jewish versus non-Jewish; and Other represtns other versus non-other (Catholic, Protestant, and Jewish combined)&lt;/td&gt;&lt;/tr&gt;&lt;/tfoot&gt;
&lt;/table&gt;

---

### Categorical Independent Variables

`\(\hat Y = a + {b_1}Catholic + {b_2}Jewish + {b_3}Other\)`

In R, dummy coding is called "treatment constrasts". It's the default coding with the first group "C" as the reference group.


```r
religion$group &lt;- factor(religion$religion)
contrasts(religion$group) # Catholic is the reference group
##   J O P
## C 0 0 0
## J 1 0 0
## O 0 1 0
## P 0 0 1
*mymodel &lt;- lm(ata ~ group, data=religion)
summary(mymodel)$coefficients
##              Estimate Std. Error  t value     Pr(&gt;|t|)
## (Intercept) 60.666667   7.806057 7.771743 7.301349e-09
## groupJ      42.833333  12.342460 3.470405 1.508602e-03
## groupO       9.458333  11.379186 0.831196 4.120223e-01
## groupP      32.641026  10.154800 3.214344 2.982232e-03
# coef(mymodel)
```

---

### Categorical Independent Variables

Manually setting the contrasts


```r
Jewish &lt;- c(0, 1, 0, 0)
Other &lt;- c(0, 0, 1, 0)
Protestant &lt;- c(0, 0, 0, 1)
*contrasts(religion$group) &lt;- cbind(Jewish, Other, Protestant)
                          # add a `contrasts` attribute to `religion$group`
religion$group
##  [1] C O P C C O P C O O C O P C P C C J O C P J P P J O P O J P P P J P J P
## attr(,"contrasts")
##   Jewish Other Protestant
## C      0     0          0
## J      1     0          0
## O      0     1          0
## P      0     0          1
## Levels: C J O P
```

---

### Categorical Independent Variables


```r
mymodel2 &lt;- lm(ata ~ group, data=religion)
summary(mymodel2)$coefficients
```

```
##                  Estimate Std. Error  t value     Pr(&gt;|t|)
## (Intercept)     60.666667   7.806057 7.771743 7.301349e-09
## groupJewish     42.833333  12.342460 3.470405 1.508602e-03
## groupOther       9.458333  11.379186 0.831196 4.120223e-01
## groupProtestant 32.641026  10.154800 3.214344 2.982232e-03
```

---

### Categorical Independent Variables

Change the reference group for dummy coding.


```r
contrasts(religion$group) &lt;-contr.treatment(n=4, base=4) # "religion" has four groups: C, J, O, P
contrasts(religion$group) # Protestant is the reference group
##   1 2 3
## C 1 0 0
## J 0 1 0
## O 0 0 1
## P 0 0 0
*mymodel2 &lt;- lm(ata ~ group, data=religion)
summary(mymodel2)$coefficients
##              Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept)  93.30769   6.495032 14.3660093 1.664683e-15
## group1      -32.64103  10.154800 -3.2143444 2.982232e-03
## group2       10.19231  11.557994  0.8818405 3.844393e-01
## group3      -23.18269  10.523155 -2.2030173 3.491209e-02
```
---

### Categorical Independent Variables

Change the reference group for dummy coding.

Manually setting the contrasts.


```r
Catholic &lt;- c(1, 0, 0, 0)
Jewish &lt;- c(0, 1, 0, 0)
Other &lt;- c(0, 0, 1, 0)
*contrasts(religion$group) &lt;- cbind(Catholic, Jewish, Other)
religion$group
```

```
##  [1] C O P C C O P C O O C O P C P C C J O C P J P P J O P O J P P P J P J P
## attr(,"contrasts")
##   Catholic Jewish Other
## C        1      0     0
## J        0      1     0
## O        0      0     1
## P        0      0     0
## Levels: C J O P
```

---

### Categorical Independent Variables


```r
mymodel3 &lt;- lm(ata ~ group, data=religion)
summary(mymodel3)$coefficients
```

```
##                Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept)    93.30769   6.495032 14.3660093 1.664683e-15
## groupCatholic -32.64103  10.154800 -3.2143444 2.982232e-03
## groupJewish    10.19231  11.557994  0.8818405 3.844393e-01
## groupOther    -23.18269  10.523155 -2.2030173 3.491209e-02
```

---

### Categorical Independent Variables

Regression Equation: `\(\hat Y = 93.31 - 32.64Catholic + 10.19Jewish - 23.18Other\)`

Protestant: `\(\hat Y = 93.31 - 32.64(0) + 10.19(0) - 23.18(0) = 93.31\)`

Catholic: `\(\hat Y = 93.31 - 32.64(1) + 10.19(0) - 23.18(0) = 60.67\)`

Jewish: `\(\hat Y = 93.31 - 32.64(0) + 10.19(1) - 23.18(0) = 103.50\)`

Other: `\(\hat Y = 93.31 - 32.64(0) + 10.19(0) - 23.18(1) = 70.13\)`

`\(\hat Y = a + {b_1}Catholic + {b_2}Jewish + {b_3}Other\)`, What do `\(a\)`, `\(b_1\)`, `\(b_2\)`, `\(b_3\)`, and `\(b_4\)` represent?


---

### Categorical Independent Variables

**Sum this up**: Multiple regression analysis was used to determine how well religion explains attitudes toward abortion.  Religion was categorized as Protestant, Catholic, Jewish, or Other.  Dummy-coding was used, creating three code variables (viz., Catholic, Jewish, and Other).  Protestant was coded as the reference group.  The analysis indicated that religion accounted for approximately 29% of the variance in attitudes toward abortion `\((R_{adj}^{2}=.29)\)`, *F*(3, 32) = 5.87, *p* &lt; .01.  There was a significant difference between Catholic and Protestant attitudes toward abortion, *t*(32) = -3.21, *p* &lt; .01.  On average, Catholic attitudes toward abortion were about 33 points less favorable than Protestant attitudes toward abortion (95% CI:  -53.33; -11.96).  There was no significant difference between Jewish and Protestant attitudes toward abortion, *t*(32) = .882, *p* &gt; .05.  There was a significant difference between Other and Protestant attitudes toward abortion, *t*(32) = -2.20, *p* &lt; .05.  On average, Other attitudes toward abortion were about 23 points less favorable than Protestant attitudes toward abortion (95% CI:  -44.62; -1.75; `\(\beta = -.351\)`).  


---

### Categorical Independent Variables

**Adding Other Variables to the Dummy Coded Model**:


`\(\hat Y = a + {b_1}Catholic + {b_2}Jewish + {b_3}Other + {b_4}Gender + {b_5}Income\)`

The dummy variables Catholic, Jewish, and Other are the same as before. Gender is 0 for male and 1 for female. Income is a continuous variable. 

.red[What does each of the regression coefficients represent?]


---

### Categorical Independent Variables

**Effect Coding**

The reference point is the unweighted mean of all of the group means, which is the intercept.  The unstandardized regression coefficients represent the difference between each group’s mean and the unweighted mean.  You must select a base group (a group for which comparisons with the mean are of least interest) and assign that group a value of -1.  The results of multiple regression do not directly inform us about this base group (e.g., control) but may be obtained through subtraction.  Useful with experimental designs.  


&lt;table class=" lightable-classic" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto;'&gt;
&lt;caption&gt;Effect Coding&lt;/caption&gt;
 &lt;thead&gt;
&lt;tr&gt;
&lt;th style="empty-cells: hide;" colspan="1"&gt;&lt;/th&gt;
&lt;th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3"&gt;&lt;div style="border-bottom: 1px solid #111111; margin-bottom: -1px; "&gt;Code Variable&lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Religion &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Catholic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Jewish &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Other &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Protestant &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Catholic &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Jewish &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Other &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

### Categorical Independent Variables

**Effect Coding**

Regression Equation: `\(\hat Y = 81.90 - 21.23Catholic + 21.6Jewish - 11.78Other\)`

- The intercept is equal to the grand mean of the group means. 

- Each regression coefficient is equal to the difference between the respective group mean and the grand mean of group means

  - Catholic:		60.67 – 81.90 = -21.23

  - Jewish:		103.50 – 81.90 = 21.6

  - Other:		70.12 – 81.90 = -11.78


```r
# Obtain the mean of `ata` by religion group
*aggregate(religion$ata, by = list(religion$group), FUN = mean)
##   Group.1         x
## 1       C  60.66667
## 2       J 103.50000
## 3       O  70.12500
## 4       P  93.30769
```

---

### Categorical Independent Variables

In R, effect coding is called "sum contrasts" (i.e., contrasts sum up to zero). Use the `contr.sum` fucnction in the `memisc` package.


```r
# library(memisc)
contrasts(religion$group) &lt;-contr.sum(n=4, base=4) # "religion" has four groups: C, J, O, P
contrasts(religion$group) # Protestant is the reference group
##    1  2  3
## C  1  0  0
## J  0  1  0
## O  0  0  1
## P -1 -1 -1
*mymodel4 &lt;- lm(ata ~ group, data=religion)
summary(mymodel4)$coefficients
##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)  81.89984   4.054882 20.197835 8.897761e-20
## group1      -21.23317   6.849039 -3.100168 4.015602e-03
## group2       21.60016   7.883081  2.740066 9.961199e-03
## group3      -11.77484   7.121639 -1.653389 1.080306e-01
```


---

### Categorical Independent Variables

Manually setting the sum constrasts


```r
Catholic &lt;- c(1, 0, 0, -1)
Jewish &lt;- c(0, 1, 0, -1)
Other &lt;- c(0, 0, 1, -1)
*contrasts(religion$group) &lt;- cbind(Catholic, Jewish, Other)
religion$group
##  [1] C O P C C O P C O O C O P C P C C J O C P J P P J O P O J P P P J P J P
## attr(,"contrasts")
##   Catholic Jewish Other
## C        1      0     0
## J        0      1     0
## O        0      0     1
## P       -1     -1    -1
## Levels: C J O P
```

---

### Categorical Independent Variables


```r
mymodel5 &lt;- lm(ata ~ group, data=religion)
summary(mymodel5)$coefficients
```

```
##                Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)    81.89984   4.054882 20.197835 8.897761e-20
## groupCatholic -21.23317   6.849039 -3.100168 4.015602e-03
## groupJewish    21.60016   7.883081  2.740066 9.961199e-03
## groupOther    -11.77484   7.121639 -1.653389 1.080306e-01
```

---

### Interactions Between Categorical and Continuous Variables

- Center the continuous predictor variables and multiply the centered continuous variable by the categorical variables.

- For example, if we wanted to regress Endurance on Age, Gender (male = 0 vs. female =1), and Age*Gender, we would center the age variable and multiply that centered age variable by the gender variable: 

 `\(\hat Y = a + {b_1}Age + {b_2}Gender + {b_3}Age*Gender\)` 


- If we wanted to regress Endurance on Age, Marital Status (Single, Married, Divorced/Separated, Widowed), and Age*Marital Status, we would first create 3 (g-1) dummy coded variables, center the age variable, and multiply the age variable by each of the three dummy coded variables:

`$$\begin{array}{l}
\hat Y = a + {b_1}Age + {b_2}Single + {b_3}Divorced/Separated + {b_4}Widowed + \\
{b_5}Age*Single + {b_6}Age*Divorced/Separated + {b_7}Age*Widowed
\end{array}$$`


---

### Useful Functions and Symbols Commonly Used in R When Fitting Linear Models
&lt;table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;Useful Functions in R When Fitting Linear Models&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Function &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Action &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; summary() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Displays detailed results for the fitted model &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; coefficients() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Lists the model parameters (intercept and slopes) for the fitted model &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; confint() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Provides confidence intervals for teh model parameters (95% by default) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; fitted() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Lists the predicted values in a fitted model &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; residuals() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Lists the residual values in a fitted model &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; anova() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Generates an ANOVA table for a fitted model, or an ANOVA table comparing two or more fitted models &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; vcov() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Lists the covariance matrix for model parameters &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; AIC() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Prints Akaike's Information Criterion &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; plot() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Generates diagnostic plots for evaluating the fit of a model &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; predict() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Uses a fitted model to predict response values for a new dataset &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


---

&lt;table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;Symbols Commonly Use in R Formulas&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Symbol &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Usage &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; ~ &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Separates response variables on the left from the explanatory varialbes on the right. For example, a prediction of y from x, z, and w would be coded y ~ x + z + w. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Separates predictor variables. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; : &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Denotes an interaction between predictor variables. A prediction of y from x, z, and the interaction between x and z would be coded y ~ x + z + x:z. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; * &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; A shortcut for denoting all possibel interactions. The code y ~ x*z*w expands to y ~ x + z + w + x:z + x:w + z:w + x:z:w. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; ^ &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Denotes interactions up to a specified degree. The code y ~ (x + z + w)^2 expands to y ~ x + z + w + x:z + x:w + z:w. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; . &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; A placeholder for all other variables in the data frame except the dependent variable. For example, if a data frame contained the varibels x, y, z, and w, then the code y ~ . would expands to y ~ x + z + w. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; - &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; A minus sign removes a variable from teh equation. For example, y ~ (x + z + w)^2 - x:w expands to y ~ x + z + w + x:z + z:w. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; -1 &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Supresses the intercept. For example, the formula y ~ x -1 fits a regression of y on x, and forces the line through the origin at x=0. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; I() &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Elements within the parentheses are interpreted arithmetically. For example, y ~ x + (z + w)^2 would expand to y ~ x + z + w + z:w. In contrast, y ~ x + I(z + w)^2 would expand to y ~ x + h, where h is a new variable created by squaring the sum of z and w. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;border-right:1px solid;"&gt; function &lt;/td&gt;
   &lt;td style="text-align:left;width: 40em; "&gt; Mathematical functions cna be used in formulas. For example, log(y) ~ x + z + w would predict log(y) from x, z, and w &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
